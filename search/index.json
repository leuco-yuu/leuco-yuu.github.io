[{"content":" 目录 Logistic回归（逻辑回归） —— 二分类 将多分类问题转化为双分类问题 Softmax回归（归一化函数回归） —— 多分类 实践——音乐分类器 0、初始化 1、数据处理与特征存储 2、模型训练与存储 3、测试与评估 支持向量机（SVM，Support Vector Machine）与感知机（Perceptron） SMO算法 SVM概率化输出 SVM合页损失 线性分类（Linear Classification） 逻辑回归算法：在多元线性回归算法的基础上把结果缩放到0-1之间 线性分类器（GLM与分布的连接器，可以通过内部核函数升维变成非线性算法）：$\\eta = \\theta^Tx = \\theta_0+\\theta_1x_1+……+\\theta_nx_n$ 广义线性模型（GLM）：$\\displaystyle{p(y;\\eta)=b(y)e^{(\\eta^TT(y)-a(\\eta))}}$，其中$\\eta$是自然参数，$T(y)$是充分统计量，$a(\\eta)$是对数部分函数 Logistic回归（逻辑回归）：伯努利-GLM Softmax回归（归一化函数回归）：多项式分布-GLM Logistic回归（逻辑回归） —— 二分类 伯努利分布： $$ \\displaystyle{P(y;p) = p^y (1-p)^{1-y}, \\quad y=0,1 \\Rightarrow P(y;p)=e^{(ln(\\frac{p}{1-p})y+ln(1-p))}}\\displaystyle{\\Rightarrow \\eta=ln(\\frac{p}{1-p})\\Rightarrow p=\\frac{1}{1+e^{-\\eta}}\\Rightarrow p(x)=\\frac{1}{1+e^{-\\theta^Tx}} } $$ 逻辑回归函数（Sigmoid）： $$ \\displaystyle{\\hat y = h_\\theta(x)=g(\\theta^Tx)=\\frac{1}{1+e^{-\\theta^Tx}}} \\Rightarrow 分界:\\theta^Tx=0 $$。当 $\\hat y\u0026gt;0.5$ 时，$y$ 取1；当 $\\hat y\u0026lt;0.5$ 时，$y$ 取0 损失函数（由极大似然推导而来）： $$ \\displaystyle{J(\\theta)=-[\\sum^m_{i=1}y^{(i)}lnh(x^{(i)})+(1-y^{(i)})ln(1-h(x^{(i)})]} $$ 梯度下降： $$ \\displaystyle{\\theta_j^{(k+1)} = \\theta_j^{(k)}-\\alpha\\frac{\\partial}{\\partial_{\\theta_j^{(k)}}}J(\\theta^{(k)})} $$ $$ \\displaystyle{\\frac{\\partial}{\\partial_{\\theta_j^{(k)}}}J(\\theta^{(k)}) = \\frac{1}{n}\\sum^n_{i=1}(h_\\theta(x_i)-y_i)x^{(j)}_i} \\Rightarrow \\nabla_{\\theta^{(k+1)}}J(\\theta^{(k+1)}) = \\frac{1}{n}X^T(X\\theta^{(k)}-y) $$ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 # Sigmoid Function import numpy as np import math import matplotlib.pyplot as plt def sigmoid_function(x): return 1/(1+np.exp(-x)) x = np.linspace(-10,10,1000) y = sigmoid_function(x) plt.xlim(-10,10) plt.ylim(0,1) plt.plot(x,y) plt.plot([0,0],[0,sigmoid_function(0)],\u0026#39;k--\u0026#39;) plt.plot([-10,0],[0.5,sigmoid_function(0)],\u0026#39;k--\u0026#39;) plt.plot(0,sigmoid_function(0)) plt.plot(0,0.5,\u0026#39;ko\u0026#39;) plt.text(0.1, 0.5 + 0.001, \u0026#39;(0 , 0.5)\u0026#39;, fontsize=14, ha=\u0026#39;left\u0026#39;, va=\u0026#39;bottom\u0026#39;) plt.show() ​ ​\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 # 二分类实例：乳腺癌 from sklearn.datasets import load_breast_cancer from sklearn.linear_model import LogisticRegression from sklearn.preprocessing import StandardScaler from sklearn.model_selection import train_test_split from sklearn.pipeline import Pipeline from sklearn.metrics import accuracy_score import numpy as np import matplotlib.pyplot as plt # 载入参数 X,y = load_breast_cancer(return_X_y=True) # 划分测试集与训练集 X_train,X_test,y_train,y_test = train_test_split( X,y, test_size=0.3, random_state=20230428, stratify=y) # # 标准归一化 # scaler = StandardScaler() # X_train_scaled = scaler.fit_transform(X_train) # X_test_scaled = scaler.fit_transform(X_test) # # 模型训练 # lg_reg = LogisticRegression() # lg_reg.fit(X_train_scaled,y_train) # # 预测 # y_train_pre = lg_reg.predict(X_train_scaled) # y_test_pre = lg_reg.predict(X_test_scaled) # # 评估 # print(\u0026#34;Train:\u0026#34;, accuracy_score(y_train,y_train_pre)*100,\u0026#34;%\u0026#34;) # print(\u0026#34;Test:\u0026#34;,accuracy_score(y_test,y_test_pre)*100,\u0026#34;%\u0026#34;) # 归一化 —— 评估可用Pipeline实现 log_reg = Pipeline([ (\u0026#39;scaler\u0026#39;,StandardScaler()), (\u0026#39;clf\u0026#39;,LogisticRegression()) ]) log_reg.fit(X_train,y_train) print(\u0026#34;Test:\u0026#34;,log_reg.score(X_train,y_train)*100,\u0026#34;%\u0026#34;) print(\u0026#34;Train:\u0026#34;,log_reg.score(X_test,y_test)*100,\u0026#34;%\u0026#34;) Test: 99.49748743718592 % Train: 96.49122807017544 % 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 # 二分类实例：鸢尾花三分类 =\u0026gt; 二分类 from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression from sklearn.preprocessing import StandardScaler from sklearn.metrics import accuracy_score from sklearn.pipeline import Pipeline # 0:\u0026#39;setosa\u0026#39;, 1:\u0026#39;versicolor\u0026#39;, 2:\u0026#39;virginica\u0026#39; X,y = load_iris(return_X_y=True) y = (y==2).astype(int) # 二分化 X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3, random_state=200304) log_reg = Pipeline([ (\u0026#39;scaler\u0026#39;,StandardScaler()), (\u0026#39;clf\u0026#39;,LogisticRegression()) ]) log_reg.fit(X_train,y_train) print(\u0026#39;Train accuracy: {:.2f} %\u0026#39;.format(log_reg.score(X_train, y_train) * 100)) print(\u0026#39;Test accuracy: {:.2f} %\u0026#39;.format(log_reg.score(X_test, y_test) * 100)) Train accuracy: 97.14 % Test accuracy: 93.33 % 将多分类问题转化为双分类问题 One-vs-all（One-vs-rest）：以鸢尾花分类问题为例。将三分类问题转换为三个二分类问题（分别为是否为\u0026rsquo;setosa\u0026rsquo;，是否为\u0026rsquo;versicolor\u0026rsquo;以及是否为\u0026rsquo;virginica\u0026rsquo;）。得到三个预测值，取最大 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 # OneVsRestClassifier from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression from sklearn.multiclass import OneVsRestClassifier from sklearn.preprocessing import StandardScaler from sklearn.metrics import accuracy_score from sklearn.pipeline import Pipeline # 0:\u0026#39;setosa\u0026#39;, 1:\u0026#39;versicolor\u0026#39;, 2:\u0026#39;virginica\u0026#39; X,y = load_iris(return_X_y=True) X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3, random_state=200304) log_reg = Pipeline([ # (\u0026#39;scaler\u0026#39;,StandardScaler()), (\u0026#39;ovr\u0026#39;,OneVsRestClassifier( LogisticRegression() ) ) ]) log_reg.fit(X_train,y_train) print(\u0026#39;Train accuracy: {:.2f} %\u0026#39;.format(log_reg.score(X_train, y_train) * 100)) print(\u0026#39;Test accuracy: {:.2f} %\u0026#39;.format(log_reg.score(X_test, y_test) * 100)) Train accuracy: 93.33 % Test accuracy: 97.78 % 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 # 转化为三个二分类问题 from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression from sklearn.preprocessing import StandardScaler from sklearn.metrics import accuracy_score from sklearn.pipeline import Pipeline import numpy as np # 0:\u0026#39;setosa\u0026#39;, 1:\u0026#39;versicolor\u0026#39;, 2:\u0026#39;virginica\u0026#39; X,y = load_iris(return_X_y=True) temp = [] X_train,X_test,y_train_init,y_test = train_test_split(X,y,test_size=0.3, random_state=200304) for category in [0,1,2]: y_train = (y_train_init==category) log_reg = LogisticRegression() log_reg.fit(X_train,y_train) temp.append(log_reg.predict_proba(X_test)[:,1]) result = np.empty(len(temp[0])).reshape(-1,1) for i in range(len(temp)): result = np.c_[result,temp[i].reshape(-1,1)] result = result[:,1:] y_test_pre = np.argmax(result,axis=1) acc = (y_test_pre==y_test) print(\u0026#39;Test accuracy: {:.2f} %\u0026#39;.format( (acc.sum()/len(acc))* 100)) Test accuracy: 97.78 % Softmax回归（归一化函数回归） —— 多分类 多项式分布： $$\\displaystyle{P(X_1=x_1,\\dots,X_k=x_k;p_1,p_2,\\dots,p_k) = \\frac{n!}{x_1! \\cdots x_k!} p_1^{x_1} \\cdots p_k^{x_k}, \\quad \\sum_{i=1}^k x_i = n}\\displaystyle{\\Rightarrow P(y;\\varphi)=e^{\\displaystyle{\\sum^{k-1}_{i=1}T(y)_iln(\\frac{\\varphi_i}{\\varphi_k})+ln\\varphi_k}} = e^{\\displaystyle{\\eta^TT(y)-a(\\eta)}}}\\Rightarrow\\displaystyle{P_i = P({y^{(i)};\\Theta})=P_k e^{\\displaystyle{\\Theta^{(i)}x}}},\\displaystyle{P_k=\\frac{1}{{\\displaystyle{\\sum^{k}_{j=1}e^{\\displaystyle{\\Theta^{(j)}x}}}}}}$$， $\\displaystyle{\\Theta^{(i)}}$ 为 $\\Theta$ 的第 $i$ 行 Softmax回归函数：$$ \\displaystyle{P(y^{(i)}=k|x^{(i)};\\theta) = \\frac{\\displaystyle{e^{\\theta_k^Tx^{(i)}}}}{\\displaystyle{\\sum^k_{j=1}e^{\\theta_j^Tx^{(i)}}}}} $$，此时 $\\theta$ 是一个矩阵 损失函数——交叉熵损失函数（Cross-Entropy Loss，负对数似然损失）：$$ \\displaystyle{J(\\theta)=-\\frac{1}{N}\\sum^{N}_{n=1}\\sum^{k}_{i=1}y_i^{(n)}ln(\\hat y_i^{(n)})} $$，其中 $N$ 是样本数， $k$ 是分类数， $y_i^{(n)}$ 是真实标签，服从one-hot编码， $\\hat y_i^{(n)}$ 是预测概率，取值为 $(0,1) $ 当y的分类数为2时，Softmax回归退化为Logistc回归 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression from sklearn.preprocessing import StandardScaler from sklearn.metrics import accuracy_score from sklearn.pipeline import Pipeline X,y = load_iris(return_X_y=True) X_train,X_test,y_train,y_test = train_test_split( X,y, test_size=0.3, random_state=200304 ) pipe = Pipeline([ (\u0026#39;scaler\u0026#39;,StandardScaler()), (\u0026#39;clf\u0026#39;,LogisticRegression(# 默认Softmax solver=\u0026#39;sag\u0026#39;, max_iter=1000, )) ]) pipe.fit(X_train,y_train) pipe.score(X_test,y_test) 0.9777777777777777 实践——音乐分类器 数据集路径： \u0026ldquo;.\\Dataset\\2_Music_Classifier\u0026rdquo; 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 from scipy.fft import fft,fftfreq # 快速傅里叶转换 from scipy.io import wavfile # 读取wav文件格式 from matplotlib.pyplot import specgram as spg # 绘制频谱图 import matplotlib.pyplot as plt from itertools import product path = \u0026#34;./Dataset/2_Music_Classifier/\u0026#34; # wavfile返回样本率（每秒采样个数，单通道采样总数） # sample_rate,X = wavfile.read(\u0026#34;./Dataset/2_Music_Classifier/blues/converted/blues.00000.au.wav\u0026#34;) # 绘图查看 def frequence_time(mtype,mserial): sample_rate,X = wavfile.read(path+mtype+\u0026#34;/converted/\u0026#34;+mtype+\u0026#34;.\u0026#34;+mserial+\u0026#34;.au.wav\u0026#34;) plt.specgram(X,Fs=sample_rate,xextent=(0,30)) # plt.plot(fft(X,sample_rate)) plt.title(\u0026#34;frequence_time:\u0026#34;+ mtype + \u0026#34;-\u0026#34; + mserial[-1]) plt.ylabel(\u0026#39;frequence\u0026#39;) plt.xlabel(\u0026#39;time\u0026#39;) def magnitude_time(mtype,mserial): sample_rate,X = wavfile.read(path+mtype+\u0026#34;/converted/\u0026#34;+mtype+\u0026#34;.\u0026#34;+mserial+\u0026#34;.au.wav\u0026#34;) time = np.linspace(0,30,len(X)) plt.plot(time,X) plt.title(\u0026#34;magnitude_time:\u0026#34;+ mtype + \u0026#34;-\u0026#34; + mserial[-1]) plt.ylabel(\u0026#39;magnitude\u0026#39;) plt.xlabel(\u0026#39;time\u0026#39;) def magnitude_frequence(mtype,mserial): sample_rate,X = wavfile.read(path+mtype+\u0026#34;/converted/\u0026#34;+mtype+\u0026#34;.\u0026#34;+mserial+\u0026#34;.au.wav\u0026#34;) N = len(X) freqs = fftfreq(N, 1/sample_rate)[:N//2] mag = np.abs(fft(X))[:N//2] plt.plot(freqs,mag) plt.title(\u0026#34;magnitude_frequence:\u0026#34;+ mtype + \u0026#34;-\u0026#34; + mserial[-1]) plt.ylabel(\u0026#39;magnitude\u0026#39;) plt.xlabel(\u0026#39;frequence\u0026#39;) types = [\u0026#39;blues\u0026#39;,\u0026#39;classical\u0026#39;,\u0026#39;country\u0026#39;,\u0026#39;disco\u0026#39;,\u0026#39;hiphop\u0026#39;,\u0026#39;jazz\u0026#39;,\u0026#39;metal\u0026#39;,\u0026#39;pop\u0026#39;,\u0026#39;reggae\u0026#39;,\u0026#39;rock\u0026#39;] serials = [\u0026#39;00000\u0026#39;,\u0026#39;00001\u0026#39;,\u0026#39;00002\u0026#39;] types = types[:] serials = serials[:] plt.figure(num=None,figsize=(8*len(serials),5*len(types)),dpi=80) for index,(t,s) in enumerate(product(types,serials)): plt.subplot(len(types),len(serials),index+1) frequence_time(t,s) plt.show() plt.figure(num=None,figsize=(8*len(serials),5*len(types)),dpi=80) for index,(t,s) in enumerate(product(types,serials)): plt.subplot(len(types),len(serials),index+1) magnitude_time(t,s) plt.show() # plt.figure(num=None,figsize=(8*len(serials),5*len(types)),dpi=80) # for index,(t,s) in enumerate(product(types,serials)): # plt.subplot(len(types),len(serials),index+1) # magnitude_frequence(t,s) types = [types[i] for i in list(np.sort(np.random.choice(range(len(types)),size=5,replace=False)))] serials = serials[:1] plt.figure(num=None,figsize=(8*len(types),5*3),dpi=80) for index,(t,s) in enumerate(product(types,serials)): plt.subplot(3,len(types),0*len(types)+index+1);frequence_time(t,s) plt.subplot(3,len(types),1*len(types)+index+1);magnitude_time(t,s) plt.subplot(3,len(types),2*len(types)+index+1);magnitude_frequence(t,s) plt.show() ​ ​\n0、初始化 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 from scipy.fft import fft # 快速傅里叶转换 from scipy.io import wavfile # 读取wav文件格式 from matplotlib.pyplot import specgram as spg # 绘制频谱图 from itertools import product import matplotlib.pyplot as plt import numpy as np import os from sklearn.pipeline import Pipeline from sklearn.preprocessing import StandardScaler from sklearn.linear_model import LogisticRegression from sklearn.model_selection import train_test_split path = \u0026#34;./Dataset/2_Music_Classifier/\u0026#34; model_path = \u0026#34;D:/MLab/project/StuML/ML/Model/2_Music_Classifier/\u0026#34; max_len = 660000 labels = dict([(label_name,label_num) for label_num,label_name in enumerate(os.listdir(path))]) 1、数据处理与特征存储 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 y = [];X=[] for label_name in labels.keys(): sub_path = path+label_name+\u0026#34;/converted/\u0026#34; for wave_file_name in os.listdir(sub_path): y.append(labels[label_name]) X.append( abs( # 取幅值（实部和虚部的平方和开根号） fft( # 傅里叶变换 wavfile.read( # 读取 sub_path+wave_file_name )[1][:max_len] # 切片 ) ) ) X_train,X_test,y_train,y_test = train_test_split( X,y, test_size=0.1, random_state=200304, stratify=y ) # 特征存储 np.save(model_path+\u0026#34;train_feature\u0026#34;,X_train) np.save(model_path+\u0026#34;train_label\u0026#34;,y_train) np.save(model_path+\u0026#34;test_feature\u0026#34;,X_test) np.save(model_path+\u0026#34;test_label\u0026#34;,y_test) 2、模型训练与存储 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 X = np.load(model_path+\u0026#34;train_feature.npy\u0026#34;) y = np.load(model_path+\u0026#34;train_label.npy\u0026#34;) log_reg = Pipeline([ (\u0026#39;scaler\u0026#39;,StandardScaler()), (\u0026#39;clf\u0026#39;,LogisticRegression( max_iter=10000, # solver=\u0026#39;sag\u0026#39; )) ]) log_reg.fit(X,y) with open(model_path+\u0026#34;model_object\u0026#34;,\u0026#34;wb\u0026#34;) as file: pickle.dump(log_reg,file) 3、测试与评估 1 2 3 4 5 6 7 8 X_test = np.load(model_path+\u0026#34;test_feature.npy\u0026#34;) y_test = np.load(model_path+\u0026#34;test_label.npy\u0026#34;) with open(model_path+\u0026#34;model_object\u0026#34;,\u0026#34;rb\u0026#34;) as file: log_reg = pickle.load(file) log_reg.score(X_test,y_test) # log_reg.score(X_train,y_train) 0.6 支持向量机（SVM，Support Vector Machine）与感知机（Perceptron） SVM是一种二分类算法，是对Perceptron模型的扩展,具有更强的泛化能力 SVM/Perceptron模型： $$ y=\\operatorname{sign}(\\theta x)= \\begin{cases} +1, \u0026 \\theta x\u003e0 \\\\ -1, \u0026 \\ \\theta x\u003c0 \\end{cases} $$ 间隔：样本到超平面的距离 几何距离：$\\displaystyle{\\gamma = \\frac{|w^tx_0+b|}{||w||}}$，其中超平面为 $\\displaystyle{w^tx+b=0}$ 对于正确分类的样本点，有 $$\\displaystyle{\\gamma = \\frac{y_0(w^tx_0+b)}{\\sqrt{w^tw}} = \\frac{y_0(\\hat y_0+b)}{\\sqrt{w^tw}}}$$ 函数距离：$\\displaystyle{\\delta=|w^tx_0+b|}$ 对于正确分类的样本点，有 $$\\displaystyle{\\delta=|w^tx_0+b|=y_0(w^tx_0+b)}$$ 硬间隔：要求所有样本都必须正确分类，不允许任何错误 软间隔：允许部分样本分类错误，容忍一定程度的误差，以获得更好的泛化能力 支持向量：离分割超平面最近的那些样本点 Perceptron思想：期望使分类错误的所有样本（m条）到超平面的距离之和最小 $$\\displaystyle{J(\\theta)=\\sum_{i\\in M_\\theta}\\frac{-y^i(\\theta^T x^i+b)}{||\\theta||}}\\Rightarrow \\displaystyle{J(\\theta)=-\\sum_{i\\in M_\\theta}{y^i(\\theta^T x^i+b)}}$$，其中 $M_\\theta$ 为分类错误集 优化：随机梯度下降法（SGD）或小批量梯度下降法（MBGD） $$\\displaystyle{\\nabla_\\theta J(\\theta) = -\\sum_{i\\in M_\\theta}y^{(i)}x^{(i)}}$$$，注意，这里的梯度函数表达式内部虽然与 $\\theta$ 无关，但错误集 $M_\\theta$ 却与 $\\theta$ 相关 SVM思想：期望使离超平面比较近的点尽可能地远离超平面 —— 二次优化 线性可分支持向量机：硬间隔最大化（完美分类正负例且距离超平面最近的点离超平面越远越好） 损失函数：求解（拉格朗日条件约束）： $$ \\begin{aligned} \u0026 \\displaystyle{\\max\\limits_{w,b}\\ \\gamma_{min}=\\frac{y_{min}(w^Tx_{min}+b)}{||w||}}\\\\ s.t. \\quad \u0026\\displaystyle{{\\gamma}_{i} = \\frac{y_i(w^Tx_i+b)}{||w||}\\ge \\gamma_{min} ,i \\in \\{1,2,\\ldots m\\}} \\\\ \\end{aligned} $$ 令 $y_{min}(wx_{min}+b) = 1$，则原式等价于 （损失函数）： $$ \\begin{aligned} \u0026 \\displaystyle{\\min\\limits_{w,b}\\ \\frac{1}{2}{||w||^2}}\\\\ s.t. \\quad \u0026\\displaystyle{y_i(w^Tx_i+b)\\ge 1 ,i \\in \\{1,2,\\ldots m\\}} \\\\ \\end{aligned} $$ 二次规划： 构建拉格朗日函数： $$\\displaystyle{ L(w,b,\\alpha)=\\frac{1}{2}||w||^2-\\sum^m_{i=1}\\alpha_i[y_i(w^Tx_i+b)-1], \\alpha_i \\ge 0}$$ 求解 $$\\displaystyle{\\min\\limits_{w,b}\\max\\limits_{\\alpha_i\\ge 0}L(w,b,\\alpha) \\overset{\\text{对偶函数}}{\\Leftrightarrow} \\max\\limits_{\\alpha_i\\ge 0}\\min\\limits_{w,b}L(w,b,\\alpha)}$$ 原问题等价于（SMO算法推导）： $$ \\begin{aligned} \u0026 \\displaystyle{\\min\\limits_\\alpha \\frac{1}{2}\\sum^m_{i=1}\\sum^m_{j=1}\\alpha_i\\alpha_jy_iy_j(x_i·x_j)-\\sum^m_{i=1}\\alpha_i}\\\\ s.t. \\quad \u0026\\displaystyle{\\sum^m_{i=1}\\alpha_iy_i=0}\u0026,i\\in \\{1,2,\\ldots m\\} \\\\ \u0026\\displaystyle{ \\alpha_i \\ge 0} \u0026,i\\in \\{1,2,\\ldots m\\} \\end{aligned} $$ 求 $w^* \\text{与} b^*$ ： $$ \\begin{cases} \\displaystyle{w^* = \\sum^m_{i=1}{\\alpha^*}_iy_ix_i}\\\\ \\displaystyle{y_s({w^*}^Tx_s+b^*)=1,\\quad x_s为任意支持向量}\\\\ \\displaystyle{\\alpha^*(y_s({w^*}^Tx_s+b^*)-1)=0,\\quad 可获得支持向量} \\end{cases} $$ 预测函数：$$\\hat y = ({w^*}^T x+b^*)$$ 线性支持向量机（线性不可分）：软间隔最大化（在硬间隔的基础上引入松弛变量 $\\xi_i \\ge 0$ ） 损失函数：求解（拉格朗日条件约束）： $$ \\begin{aligned} \u0026 \\displaystyle{\\min\\limits_{w,b} \\frac{1}{2}{||w||^2} + C{\\sum^m_{i=1}\\xi_i}}\\\\ s.t. \\quad \u0026\\displaystyle{y_i(w^Tx_i+b)\\ge 1-\\xi_i} \u0026, i \\in \\{1,2,\\ldots m\\} \\\\ \u0026\\displaystyle{ \\xi_i \\ge 0}\u0026, i \\in \\{1,2,\\ldots m\\} \\end{aligned} $$ 二次规划： 构建拉格朗日函数： $$ \\displaystyle{ L(w,b,\\xi,\\alpha,\\mu)=\\frac{1}{2}||w||^2+C\\sum^m_{i=1}\\xi_i-\\sum^m_{i=1}\\alpha_i[y_i(w^Tx_i+b)-1+\\xi_i]-\\sum^m_i\\mu_i\\xi_i, \\alpha_i,\\xi_i \\ge 0} $$ 求解 $\\displaystyle{\\min\\limits_{w,b,\\xi}\\max\\limits_{\\alpha_i\\ge 0,\\xi_i\\ge 0}L(w,b,\\xi,\\alpha,\\mu) \\overset{\\text{对偶函数}}{\\Leftrightarrow} \\max\\limits_{\\alpha_i\\ge 0,\\xi_i\\ge 0}\\min\\limits_{w,b,\\xi}L(w,b,\\xi,\\alpha,\\mu)}$ 原问题等价于（SMO算法推导）： $$ \\begin{aligned} \u0026 \\displaystyle{\\min\\limits_\\alpha \\frac{1}{2}\\sum^m_{i=1}\\sum^m_{j=1}\\alpha_i\\alpha_jy_iy_j(x_i·x_j)-\\sum^m_{i=1}\\alpha_i}\\\\ s.t. \\quad \u0026\\displaystyle{\\sum^m_{i=1}\\alpha_iy_i=0}\u0026,i\\in \\{1,2,\\ldots m\\}\\\\ \u0026 0 \\le \\alpha_i \\le C \u0026 , i\\in \\{1,2,\\ldots m\\}\\\\ % \u0026 \\xi_i \\ge 0 \u0026 , i\\in \\{1,2,\\ldots m\\} \\end{aligned} $$ 对于支持样本 $x_i$: $\\alpha_i = 0$ : $x_i$ 被正确分类 $0 \u0026lt; \\alpha_i \u0026lt; C$ : $x_i$ 在软边界上 $\\alpha_i = C$ : $x_i$ 嵌入软边界内 $\\xi \u0026lt; 1$ : $x_i$ 被正确分类 $\\xi = 1$ : $x_i$ 在超平面上 $\\xi \u0026gt; 0$ : $x_i$ 被错误分类 求 $w^* \\text{与}b^*$ ： $$ \\begin{cases} \\displaystyle{w^* = \\sum^m_{i=1}{\\alpha}_i^*y_ix_i}\\\\ \\displaystyle{y_s({w^*}^Tx_s+b^*)=1,\\quad x_s为任意支持向量}\\\\ \\displaystyle{\\alpha^*(y_s({w^*}^Tx_s+b^*)-1)=0,\\quad 可获得支持向量} \\end{cases} $$ 非线性支持向量机：升维（核函数Kernel） 定义Kernel函数：$K(x,z) = \\phi(x)\\phi(z)$ 此时对偶处理后的目标函数为： $$ \\begin{aligned} \u0026 \\displaystyle{\\min\\limits_\\alpha \\frac{1}{2}\\sum^m_{i=1}\\sum^m_{j=1}\\alpha_i\\alpha_jy_iy_jK(x_i,x_j)-\\sum^m_{i=1}\\alpha_i}\\\\ s.t. \\quad \u0026\\displaystyle{\\sum^m_{i=1}\\alpha_iy_i=0}\u0026,i\\in \\{1,2,\\ldots m\\}\\\\ \u0026 0 \\le \\alpha_i \\le C \u0026 , i\\in \\{1,2,\\ldots m\\}\\\\ % \u0026 \\xi_i \\ge 0 \u0026 , i\\in \\{1,2,\\ldots m\\} \\end{aligned} $$ 常用Kernel函数： 线性Kernel函数： $K(x,z)=x \\cdot z$ 多项式Kernel函数： $K(x,z)=(\\gamma x \\cdot z + r)$ 高斯Kernel函数： $K(x,z)=\\exp(-\\gamma||x-z||^2)$ Sigmoid Kernel函数：$K(x,z)=\\tanh(\\gamma x \\cdot z + r)$ SMO算法 SVM算法关键步骤： 支持向量机算法最优化问题（指定惩罚系数C）： $$ \\begin{aligned} \u0026 \\displaystyle{J(\\alpha)=\\min\\limits_\\alpha \\frac{1}{2}\\sum^m_{i=1}\\sum^m_{j=1}\\alpha_i\\alpha_jy_iy_j(x_i·x_j)-\\sum^m_{i=1}\\alpha_i}\\\\ s.t. \\quad \u0026\\displaystyle{\\sum^m_{i=1}\\alpha_iy_i=0}\u0026,i\\in \\{1,2,\\ldots m\\}\\\\ \u0026 0 \\le \\alpha_i \\le C \u0026 , i\\in \\{1,2,\\ldots m\\}\\\\ % \u0026 \\xi_i \\ge 0 \u0026 , i\\in \\{1,2,\\ldots m\\} \\end{aligned} $$ 等价于求解 ： $$ \\begin{cases} \\displaystyle{w^* = \\sum^m_{i=1}{\\alpha}_i^*y_ix_i}\\\\ \\displaystyle{y_s({w^*}^Tx_s+b^*)=1,\\quad x_s为任意支持向量}\\\\ \\displaystyle{\\alpha^*(y_s({w^*}^Tx_s+b^*)-1)=0,\\quad 可获得支持向量} \\end{cases} $$ 预测函数： $\\displaystyle{f(x)=sign(\\sum^m_{i=1}\\alpha_i^y_iK(x,x_i)+b^)}$ SMO算法求解最优化问题 求解 $\\alpha_i, i\\in {1,2,\\dots m}$ 算法思想： 由于 $\\displaystyle{\\sum^m_{i=1}\\alpha_iy_i=0}$ ，故不能只固定一个参数 将原始求解m个参数二次规划问题分解成多个子二次规划问题分别求解，每个子问题只求解2个参数 采用启发式方法选择需要更新的两个参数 $\\alpha_i $ 和 $\\alpha_j$ 例如，选择 $\\alpha_1$ 和 $\\alpha_2$ 优化 $\\alpha_i $ 和 $\\alpha_j$ 使目标函数最大程度接近全局最优解（$\\nabla_{\\alpha_i,\\alpha_j}J(\\alpha)=0$），其余参数不变 $$\\displaystyle{\\min\\limits_{\\alpha_1,\\alpha_2}J(\\alpha_1,\\alpha_2)=\\min\\limits_{\\alpha_1,\\alpha_2}\\frac{1}{2}K_{11}\\alpha_1^2+\\frac{1}{2}K_{22}\\alpha_2^2+y_1y_2K_{12}\\alpha_1\\alpha_2-(\\alpha_1+\\alpha_2)+y_1v_1\\alpha_1+y_2v_2\\alpha_2+Constant}$$ ，其中 $$v_i = \\displaystyle{\\sum^m_{j=3}\\alpha_jy_jK(x_i,x_j), i = 1,2}$$ 将二元函数视为一元函数 另 $$\\alpha_1 = (\\zeta-y_2\\alpha_2)y_1$$ ，带回上式 $$\\displaystyle{\\min\\limits_{\\alpha_2}J(\\alpha_2)=\\min\\limits_{\\alpha_2}\\frac{1}{2}K_{11}(\\zeta-y_2\\alpha_2)y_1^2+\\frac{1}{2}K_{22}\\alpha_2^2+y_1y_2K_{12}(\\zeta-y_2\\alpha_2)y_1\\alpha_2-((\\zeta-y_2\\alpha_2)y_1+\\alpha_2)+y_1v_1(\\zeta-y_2\\alpha_2)y_1+y_2v_2\\alpha_2+Constant}$$ 对 $J(\\alpha_2)$ 求导，导数为0 $$ \\begin{aligned} \\displaystyle{v_1 = f(x_1)-\\sum^2_{j=1}y_j\\alpha_jK_{1j}-b} \\\\ \\displaystyle{v_2 = f(x_2)-\\sum^2_{j=1}y_j\\alpha_jK_{2j}-b} \\end{aligned} $$ $$\\displaystyle{\\alpha^{new,unclipped}_2 = \\alpha^{old}_2 + \\frac{y_2(E_1-E_2)}{\\eta}}$$，其中 $$\\alpha_2^{new,unclipped}$$ 为未考虑约束的新值， $$E_i = f(x_i)-y_i, \\ \\eta = K_{11}+K_{22}-2K_{12}$$ 对新值进行约束修建 $$ \\begin{aligned} \u0026 0 \\le \\alpha_1,\\alpha_2 \\le C \\\\ \u0026 \\alpha_1 + \\alpha_2 = \\zeta \\end{aligned} $$ 当 $y_1 \\neq y_2$ 时， $$ L=\\max(0,\\alpha^{old}_2-\\alpha^{old}_1),H=\\min(C,C+\\alpha^{old}_2-\\alpha^{old}_1) $$ 当 $y_1 \\neq y_2$ 时，$$L=\\max(0,\\alpha_1^{old}+\\alpha_2^{old}-C);H=\\min(C,\\alpha_2^{old}+\\alpha_1^{old})$$ $$ \\alpha_2^{\\text{new}} = \\begin{cases} H \u0026, \\alpha_2^{\\text{new,unclipped}} \u003e H \\\\ \\alpha_2^{\\text{new,unclipped}} \u0026, L \\le \\alpha_2^{\\text{new,unclipped}} \\le H \\\\ L \u0026, \\alpha_2^{\\text{new,unclipped}} \u003c L \\end{cases} $$ 启发式选择变量 第一个变量的选择称为外循环，首先遍历整个样本集，选择违反KKT条件的 $\\alpha_i$ 作为第一个变量。依据相关规则选择第二个变量，对这两个变量采用上述方法进行优化。直到遍历整个样本集后，没有违反KKT条件 $\\alpha_i$ ，然后退出 KKT条件： $$ \\begin{aligned} \u0026 \\alpha_i = 0 \u0026 \\Rightarrow \u0026 \\qquad y^{(i)}(w^Tx^{(i)}+b) \\ge 1 \\\\ \u0026 \\alpha_i = C \u0026 \\Rightarrow \u0026 \\qquad y^{(i)}(w^Tx^{(i)}+b) \\le 1 \\\\ \u0026 0 \u003c \\alpha_i \u003c C \u0026 \\Rightarrow \u0026 \\qquad y^{(i)}(w^Tx^{(i)}+b) = 1 \\\\ \\end{aligned} $$ 第二个变量的选择成为内循环，假设在外循环中找到第一个变量记为 $\\alpha_1$ ，第二个变量的选择希望能使 $\\alpha_2$ 有较大变化。由于 $\\alpha_2$ 是依赖于 $|E_1-E_2|$ ，当 $E_1$ 为正时，那么选择最小的 $E_i$ 作为 $E_2$ ；如果 $E_1$ 为负时，选择最大的 $E_i$ 作为 $E_2$ ，通常为每个样本的 $E_i$ 保存在一个列表中，选择最大的 $|E_1-E_2|$ 来近似最大化步长。 SVM概率化输出 目的：将决策值映射为概率（如 $P(y=1∣x)\\in[0,1]$），使用户能判断样本属于某类的可能性大小 标准的SVM的无阈值输出为：$f(x)=h(x)+b$ ，其中 $h(x)=\\displaystyle{\\sum_iy_i\\alpha_iK(x_i,x)}$ Platt利用 sigmoid-fitting 方法，将标准的SVM的输出结果进行后处理，转换为后验概率 $P(y=1|f)=\\displaystyle{\\frac{1}{1+\\exp(Af+B)}}$ ，其中 $A$ 和 $B$ 为待拟合参数，$f$ 为样本 $x$ 的无阈值输出。 SVM合页损失 0-1损失是二分类问题的真正损失函数，合页损失与logistic损失是对0-1损失函数的近似 合页损失函数(Hinge Loss Function)：$L(y(wx+b))=[1-y(wx+b)]+$ ， $[]+$ 表示： $$ [z]_+ = \\begin{cases} z\u0026,z\u003e0 \\\\ 0\u0026,z\\le 0 \\end{cases} $$ 也就是说，数据点如果被正确分类，损失为0，如果没有被正确分类，损失为z SVM的损失函数就是合页损失函数+正则化项: $$\\displaystyle{\\min\\limits_{w,b}\\sum^m_{i=1}L(y^{(i)}(w^Tx^{(i)}+b)) + \\lambda ||w||^2}$$ 上述目标函数中，当 $\\displaystyle{\\lambda = \\frac{1}{2C}}$ 时。等价于原目标函数 $$\\displaystyle{\\frac{1}{2}||w||^2+C\\sum^m_{i=1}\\xi_i}$$ 实践-人脸分类器 基于 sklearn 内置 LFW 数据集 + SVM 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 import numpy as np import matplotlib.pyplot as plt from sklearn.datasets import fetch_lfw_people from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.decomposition import PCA from sklearn.svm import SVC from sklearn.pipeline import Pipeline from sklearn.metrics import classification_report,accuracy_score # 1、加载人脸数据 lfw_people = fetch_lfw_people(min_faces_per_person=70,resize=0.4) X,y,names = (lfw_people.data,lfw_people.target,lfw_people.target_names) n_classes = names.shape[0] print(\u0026#34;数据集规模：\u0026#34;,X.shape) print(\u0026#34;类别数：\u0026#34;,n_classes) print(\u0026#34;类别名：\u0026#34;,names) print() # 2、划分数据集与训练集 X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.25,random_state=428,stratify=y) # 3、构建机器学习通道 pipe = Pipeline([ # 先降维，再归一，再SVM ( \u0026#39;pca\u0026#39;, PCA( n_components=150, # 只保留150个主成分（把原始11750维降到150维） whiten=True, # 白化：150维特征缩放到同尺度，去相关 random_state=428 ) ), ( \u0026#39;scaler\u0026#39;, StandardScaler() # 对PCA后的150维特征进行归一化 ), ( \u0026#39;svm\u0026#39;, SVC( kernel=\u0026#39;rbf\u0026#39;, # 使用高斯核函数进行非线性升维 class_weight=\u0026#39;balanced\u0026#39;, # 自动根据各类样本量给出权重，防止某些人照片多导致偏向性 C=5.0, # 正则强度，C越大越不允许误分，可能导致过拟合；C约小margin越大可能导致欠拟合 gamma=\u0026#39;scale\u0026#39; # 高斯核宽度gamma的取值策略 ) ) ]) # 4、训练模型 print(\u0026#34;开始训练 SVM ...\u0026#34;) pipe.fit(X_train,y_train) print(\u0026#34;训练完成\u0026#34;) print() # 5、评估模型 y_pred = pipe.predict(X_test) print(f\u0026#34;\\n测试集整体准确率：{pipe.score(X_test,y_test):.3f}\u0026#34;) print(\u0026#34;\\n详细分类报告：\u0026#34;) print(classification_report(y_test, y_pred, target_names=names)) 数据集规模： (1288, 1850) 类别数： 7 类别名： ['Ariel Sharon' 'Colin Powell' 'Donald Rumsfeld' 'George W Bush' 'Gerhard Schroeder' 'Hugo Chavez' 'Tony Blair'] 开始训练 SVM ... 训练完成 ​ 测试集整体准确率：0.851\n详细分类报告： precision recall f1-score support Ariel Sharon 1.00 0.47 0.64 19 Colin Powell 0.84 0.90 0.87 59 Donald Rumsfeld 0.95 0.70 0.81 30 George W Bush 0.80 0.98 0.88 133 Gerhard Schroeder 0.95 0.78 0.86 27 Hugo Chavez 1.00 0.56 0.71 18 Tony Blair 0.91 0.83 0.87 36 accuracy 0.85 322 macro avg 0.92 0.75 0.81 322 weighted avg 0.87 0.85 0.84 322 ​\n","date":"2025-12-02T18:51:05+08:00","permalink":"http://localhost:1313/p/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB/","title":"机器学习-线性分类"},{"content":" 目录 一、正规方程（Normal Equation） 1、一维线性回归（代码实现） 2、高维线性回归（代码实现） 3、高维线性回归（sklearn库） 二、梯度下降（Gradient Descent） 1、批量梯度下降（GD） 代码实现 2、随机梯度下降（SGD） (1)、代码实现 (2)、小幅优化：打乱数据索引，顺序选取向量 3、小批量梯度下降（Mini-batch SGD） (1)、代码实现 (2)、小幅优化：打乱数据索引，顺序批量选取向量 4、SGD的sklearn库实现 5、梯度下降法的问题与解决思路 （1）、确定速率调整函数 （2）、更改学习速率 三、坐标下降（Coordinate Descent） 代码实现 四、归一化（Normalization） 五、正则化（Regularization） 1、Lasso回归（sklearn库） 2、Ridge回归（sklearn库） 3、ElasticNet回归（sklearn库） 4、随机梯度下降实现elasticnet正则化（sklearn库） 六、升维方法 — 多项式回归（Polynomial Regression） 七、实践 — 保险花销预测 1、数据提取 2、EDA（Exploratory Data Analysis，探索性数据分析） 3、特征工程 4、模型训练 5、模型评估 6、进阶 线性回归（Linear Regression） 任务类型分类：有监督学习 定义：拟合因变量(向量) y 与 x 之间呈线性关系，即 $\\mathbf{y} = \\mathbf{w}^T\\mathbf{x} + \\mathbf{b}$，其中 $\\mathbf{x,y,w,b \\in \\mathbb{R}^n}$ 目标：寻找参数最优解使得$\\operatorname{Loss}$函数最小。$(\\hat{\\mathbf{w}}, \\hat{\\mathbf{b}}) = \\operatorname{Loss}(\\mathbf{y}, \\hat{\\mathbf{y}}) \\Leftrightarrow\\operatorname{Argmin}(\\operatorname{Loss}(\\mathbf{w}, \\mathbf{b}))$ 多元线性回归常用损失函数 —— MSE(均方误差，由似然函数推导而来): $$ \\displaystyle{\\operatorname{\\mathit{J}(\\theta)} = \\frac{1}{2} \\displaystyle \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)})^2 = \\frac{1}{2}(X\\theta - y)^T(X\\theta - y) = \\frac{1}{2}(\\theta^TX^TX\\theta - \\theta^TX^Ty - y^TX\\theta + y^Ty)},系数可为\\displaystyle{\\frac{1}{2}、\\frac{1}{n}、\\frac{1}{2n}} $$ MSE对 $\\theta$ 的梯度: $$ \\displaystyle{\\frac{\\partial{J}(\\theta)}{\\partial\\theta} = X^TX\\theta - X^Ty = 0 \\Rightarrow \\theta = (X^TX)^{-1}X^Ty} $$ 一、正规方程（Normal Equation） 参数解析解： $\\hat{\\theta} = (X^TX)^{-1}X^Ty$ 可能存在的问题：$(X^TX)^{-1}$的求解可能比较困难 1、一维线性回归（代码实现） 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 import numpy as np import matplotlib.pyplot as plt np.random.seed(20030428) # 数据量 m = 100 # 随机生成 x 序列 x = np.random.rand(m,1) * 2 # 模拟 x 对应的 y 标签，误差呈正态分布 y = 5 + x*4 + np.random.randn(m,1) # 补充 x0 列 X = np.c_[np.ones((m,1)),x] # 参数的求解公式 w_hat = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y) # 预测 x_pre = [[0.5],[1.5]] X_pre = np.c_[np.ones((2,1)),x_pre] y_pre = X_pre.dot(w_hat) plt.scatter(x,y,label=\u0026#39;Original Data\u0026#39;) plt.scatter(x_pre,y_pre,label=\u0026#39;Predicted Data\u0026#39;) plt.plot(x,X.dot(w_hat),label=\u0026#39;Regression Model\u0026#39;) plt.legend() plt.show() 2、高维线性回归（代码实现） 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 import numpy as np np.random.seed(20030428) # 维度 n = 4 # 数据量 m = 1000 # 设置预测值 x_pre = np.random.random(size = (3,n))*10 # 随机生成 x 序列 x = np.random.rand(m,n) * 2 # 补充 x0 列 X = np.c_[np.ones((m,1)),x] # 模拟 x 对应的 y 标签，误差呈正态分布 w = np.random.randint(0,6,size=(n+1,1)) y = X.dot(w) + np.random.randn(m,1) # 参数的求解公式 w_hat = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y) print(\u0026#39;W_hat:\u0026#39;) print(w_hat.reshape(1,n+1)) # 预测 X_pre = np.c_[np.ones((3,1)),x_pre] y_pre = X_pre.dot(w_hat) print(\u0026#39;X_pre: \u0026#39;) print(x_pre) print(\u0026#39;Y_pre: \u0026#39;) print(y_pre) W_hat: [[5.02671056 3.98404888 4.00452019 4.9783491 1.02490084]] X_pre: [[0.03016482 4.79096225 5.74410225 4.2038564 ] [0.0650456 9.53496505 3.37288732 7.77475153] [2.68648726 5.44484467 6.07293252 8.00564199]] Y_pre: [[57.23707594] [68.2285754 ] [75.97196488]] 3、高维线性回归（sklearn库） 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 import numpy as np from sklearn.linear_model import LinearRegression np.random.seed(20030428) # 维度 n = 4 # 数据量 m = 1000 # 设置预测值 x_pre = np.random.random(size = (3,n))*10 # 随机生成 x 序列 x = np.random.rand(m,n) * 2 # 补充 x0 列 X = np.c_[np.ones((m,1)),x] # 模拟 x 对应的 y 标签，误差呈正态分布 w = np.random.randint(0,6,size=(n+1,1)) y = X.dot(w) + np.random.randn(m,1) reg = LinearRegression(fit_intercept=True) reg.fit(x,y) print(\u0026#39;W_hat:\u0026#39;) print(np.c_[reg.intercept_,reg.coef_]) # 预测 y_pre = reg.predict(x_pre) print(\u0026#39;X_pre: \u0026#39;) print(x_pre) print(\u0026#39;Y_pre: \u0026#39;) print(y_pre) W_hat: [[5.02671056 3.98404888 4.00452019 4.9783491 1.02490084]] X_pre: [[0.03016482 4.79096225 5.74410225 4.2038564 ] [0.0650456 9.53496505 3.37288732 7.77475153] [2.68648726 5.44484467 6.07293252 8.00564199]] Y_pre: [[57.23707594] [68.2285754 ] [75.97196488]] 二、梯度下降（Gradient Descent） 梯度下降法迭代公式：$\\hat\\theta_{k+1} = \\hat\\theta_k - \\alpha \\nabla_{\\theta}J(\\hat\\theta)$, $\\alpha$为学习率 学习率 $\\alpha$ ：$\\alpha$ 设置过大容易造成震荡，$\\alpha$ 设置太小容易造成迭代次数增加，也可能落到局部最优解。一般设置为0.1、0.01、0.001、0.0001 1、批量梯度下降（GD） $$\\displaystyle{\\nabla_{\\theta}J(\\theta) = \\frac{\\partial}{\\partial{\\theta}}J(\\theta) = \\displaystyle \\sum^{m}_{i=1}(h_\\theta(x^{(i)}) - y^{(i)})x^{(i)} = X^T(X\\theta-y)}$$ 代码实现 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 import numpy as np np.random.seed(20030428) # 维度 n = 4 # 数据量 m = 1000 # 设置预测值 x_pre = np.random.random(size = (3,n))*10 # 超参数 learning_rate = 0.0001 # 学习率 α n_iterations = 1000 # 迭代次数 # 随机生成 x 序列 x = np.random.rand(m,n) * 2 # 补充 x0 列 X = np.c_[np.ones((m,1)),x] # 模拟 x 对应的 y 标签，误差呈正态分布 w = np.random.randint(0,6,size=(n+1,1)) y = X.dot(w) + np.random.randn(m,1) # 初始化参数向量 w_init = np.random.randn(n+1,1) # 求梯度 for n_iteration in range(n_iterations): gradient = X.T.dot(X.dot(w_init)-y) w_init = w_init - learning_rate*gradient print(\u0026#39;W_hat:\u0026#39;) print(w_init.reshape(1,n+1)) # 预测 y_pre = np.c_[np.ones((3,1)),x_pre].dot(w_init) print(\u0026#39;X_pre: \u0026#39;) print(x_pre) print(\u0026#39;Y_pre: \u0026#39;) print(y_pre) W_hat: [[5.02631227 3.98414324 4.00461388 4.97843832 1.02499615]] X_pre: [[0.03016482 4.79096225 5.74410225 4.2038564 ] [0.0650456 9.53496505 3.37288732 7.77475153] [2.68648726 5.44484467 6.07293252 8.00564199]] Y_pre: [[57.23804251] [68.23011847] [75.97363505]] 2、随机梯度下降（SGD） $$ \\displaystyle {\\nabla_{\\theta}J(\\theta) = \\frac{\\partial}{\\partial\\theta}J(\\theta) = (h_\\theta(x^{(i)})-y^{(i)})x^{(i)} = ({x^{(i)}})^T(x^{(i)}\\theta-y^{(i)}) , i \\sim \\text{Uniform}\\bigl(\\{1,2,\\dots,m\\}\\bigr)} $$ (1)、代码实现 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 import numpy as np np.random.seed(20030428) # 维度 n = 4 # 数据量 m = 1000 # 设置预测值 x_pre = np.random.random(size = (3,n))*10 # 超参数 learning_rate = 0.0001 # 学习率 α n_iterations = 1000 # 迭代次数 # 随机生成 x 序列 x = np.random.rand(m,n) * 2 # 补充 x0 列 X = np.c_[np.ones((m,1)),x] # 模拟 x 对应的 y 标签，误差呈正态分布 w = np.random.randint(0,6,size=(n+1,1)) y = X.dot(w) + np.random.randn(m,1) # 初始化参数向量 w_init = np.random.randn(n+1,1) # 求梯度 for n_iteration in range(n_iterations): for _ in range(m): random_index = np.random.randint(m) x_i = X[random_index:random_index+1,:] y_i = y[random_index:random_index+1,:] gradient = x_i.T.dot(x_i.dot(w_init)-y_i) w_init = w_init - learning_rate*gradient print(\u0026#39;W_hat:\u0026#39;) print(w_init.reshape(1,n+1)) # 预测 y_pre = np.c_[np.ones((3,1)),x_pre].dot(w_init) print(\u0026#39;X_pre: \u0026#39;) print(x_pre) print(\u0026#39;Y_pre: \u0026#39;) print(y_pre) W_hat: [[5.01922912 3.97831998 4.00264292 4.96386168 1.01662312]] X_pre: [[0.03016482 4.79096225 5.74410225 4.2038564 ] [0.0650456 9.53496505 3.37288732 7.77475153] [2.68648726 5.44484467 6.07293252 8.00564199]] Y_pre: [[57.1024122 ] [68.08959999] [75.78462184]] (2)、小幅优化：打乱数据索引，顺序选取向量 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 import numpy as np np.random.seed(20030428) # 维度 n = 4 # 数据量 m = 1000 # 设置预测值 x_pre = np.random.random(size = (3,n))*10 # 超参数 learning_rate = 0.0001 # 学习率 α n_iterations = 1000 # 迭代次数 # 随机生成 x 序列 x = np.random.rand(m,n) * 2 # 补充 x0 列 X = np.c_[np.ones((m,1)),x] # 模拟 x 对应的 y 标签，误差呈正态分布 w = np.random.randint(0,6,size=(n+1,1)) y = X.dot(w) + np.random.randn(m,1) # 初始化参数向量 w_init = np.random.randn(n+1,1) # 求梯度 for n_iteration in range(n_iterations): # 打乱顺序，顺序读取 index = np.arange(m) np.random.shuffle(index) X = X[index] y = y[index] for i in range(m): x_i = X[i:i+1,:] y_i = y[i:i+1,:] gradient = x_i.T.dot(x_i.dot(w_init)-y_i) w_init = w_init - learning_rate*gradient print(\u0026#39;W_hat:\u0026#39;) print(w_init.reshape(1,n+1)) # 预测 y_pre = np.c_[np.ones((3,1)),x_pre].dot(w_init) print(\u0026#39;X_pre: \u0026#39;) print(x_pre) print(\u0026#39;Y_pre: \u0026#39;) print(y_pre) W_hat: [[5.02675441 3.98450041 4.00505391 4.9789357 1.02541193]] X_pre: [[0.03016482 4.79096225 5.74410225 4.2038564 ] [0.0650456 9.53496505 3.37288732 7.77475153] [2.68648726 5.44484467 6.07293252 8.00564199]] Y_pre: [[57.24520848] [68.23968977] [75.98378179]] 3、小批量梯度下降（Mini-batch SGD） $$ \\displaystyle{\\nabla_{\\theta}J(\\theta) = \\frac{\\partial}{\\partial{\\theta}}J(\\theta) = \\displaystyle \\sum_{i\\in S}(h_\\theta(x^{(i)}) - y^{(i)})x^{(i)} = ({x^{(S)}})^T(x^{(S)}\\theta-y^{(S)}) , |S|=\\text{batch\\_size} \u003c m,\\; S\\subseteq \\{1,2,\\dots ,m\\},\\; S\\text{均匀随机选取}} $$ (1)、代码实现 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 import numpy as np np.random.seed(20030428) x_pre = np.random.random(size = (3,n))*10 # 维度 n = 4 # 数据量 m = 1000 # 分批大小 batch_size = 10 # 批次数 num_batches = int(m/batch_size) # 超参数 learning_rate = 0.0001 # 学习率 α n_iterations = 1000 # 迭代次数 # 随机生成 x 序列 x = np.random.rand(m,n) * 2 # 补充 x0 列 X = np.c_[np.ones((m,1)),x] # 模拟 x 对应的 y 标签，误差呈正态分布 w = np.random.randint(0,6,size=(n+1,1)) y = X.dot(w) + np.random.randn(m,1) # 初始化参数向量 w_init = np.random.randn(n+1,1) # 求梯度 for n_iteration in range(n_iterations): for _ in range(num_batches): random_index = np.random.randint(m) x_s = X[random_index:random_index+batch_size,:] y_s = y[random_index:random_index+batch_size,:] gradient = x_s.T.dot(x_s.dot(w_init)-y_s) w_init = w_init - learning_rate*gradient print(\u0026#39;W_hat:\u0026#39;) print(w_init.reshape(1,n+1)) # 预测 y_pre = np.c_[np.ones((3,1)),x_pre].dot(w_init) print(\u0026#39;X_pre: \u0026#39;) print(x_pre) print(\u0026#39;Y_pre: \u0026#39;) print(y_pre) W_hat: [[5.04016211 3.9927612 4.01168955 4.96684374 1.00728303]] X_pre: [[0.03016482 4.79096225 5.74410225 4.2038564 ] [0.0650456 9.53496505 3.37288732 7.77475153] [2.68648726 5.44484467 6.07293252 8.00564199]] Y_pre: [[57.14498768] [68.13517282] [75.83694482]] (2)、小幅优化：打乱数据索引，顺序批量选取向量 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 import numpy as np np.random.seed(20030428) x_pre = np.random.random(size = (3,n))*10 # 维度 n = 4 # 数据量 m = 1000 # 分批大小 batch_size = 10 # 批次数 num_batches = int(m/batch_size) # 超参数 learning_rate = 0.0001 # 学习率 α n_iterations = 1000 # 迭代次数 # 随机生成 x 序列 x = np.random.rand(m,n) * 2 # 补充 x0 列 X = np.c_[np.ones((m,1)),x] # 模拟 x 对应的 y 标签，误差呈正态分布 w = np.random.randint(0,6,size=(n+1,1)) y = X.dot(w) + np.random.randn(m,1) # 初始化参数向量 w_init = np.random.randn(n+1,1) # 求梯度 for n_iteration in range(n_iterations): # 打乱顺序，顺序读取 index = np.arange(m) np.random.shuffle(index) X = X[index] y = y[index] for i in range(num_batches): x_s = X[i*batch_size:i*batch_size+batch_size,:] y_s = y[i*batch_size:i*batch_size+batch_size,:] gradient = x_s.T.dot(x_s.dot(w_init)-y_s) w_init = w_init - learning_rate*gradient print(\u0026#39;W_hat:\u0026#39;) print(w_init.reshape(1,n+1)) # 预测 y_pre = np.c_[np.ones((3,1)),x_pre].dot(w_init) print(\u0026#39;X_pre: \u0026#39;) print(x_pre) print(\u0026#39;Y_pre: \u0026#39;) print(y_pre) W_hat: [[5.0267601 3.98450244 4.00504673 4.978934 1.02541437]] X_pre: [[0.03016482 4.79096225 5.74410225 4.2038564 ] [0.0650456 9.53496505 3.37288732 7.77475153] [2.68648726 5.44484467 6.07293252 8.00564199]] Y_pre: [[57.24518034] [68.23964038] [75.98376306]] 4、SGD的sklearn库实现 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 import numpy as np from sklearn.linear_model import SGDRegressor np.random.seed(20030428) x_pre = np.random.random(size = (3,n))*10 # 维度 n = 4 # 数据量 m = 1000 # 超参数 learning_rate = 0.001 # 学习率 α n_iterations = 10000 # 迭代轮次 # 随机生成 x 序列 x = np.random.rand(m,n) * 2 # 补充 x0 列 X = np.c_[np.ones((m,1)),x] # 模拟 x 对应的 y 标签，误差呈正态分布 w = np.random.randint(0,6,size=(n+1,1)) y = X.dot(w) + np.random.randn(m,1) sgd_reg = SGDRegressor( loss=\u0026#39;squared_error\u0026#39;, # MSE penalty = None, # 无正则项 alpha = 0, # 无正则强度 learning_rate = \u0026#39;constant\u0026#39;, # 学习率 eta0 = learning_rate, # 固定学习率 fit_intercept = True, # 增加偏置列 max_iter = n_iterations * m, # 迭代次数 warm_start = False, # 不进行手动迭代，直接迭代完成 random_state = 20030428, # 打乱样本的随机种子 ) sgd_reg.fit(x,y.flatten()) print(\u0026#39;W_hat:\u0026#39;) print(np.c_[sgd_reg.intercept_,sgd_reg.coef_.reshape(1,-1)]) # 预测 y_pre = sgd_reg.predict(x_pre).reshape(-1,1) print(\u0026#39;X_pre: \u0026#39;) print(x_pre) print(\u0026#39;Y_pre: \u0026#39;) print(y_pre) W_hat: [[4.78413833 4.04362443 4.06423645 5.03733553 1.08352435]] X_pre: [[0.03016482 4.79096225 5.74410225 4.2038564 ] [0.0650456 9.53496505 3.37288732 7.77475153] [2.68648726 5.44484467 6.07293252 8.00564199]] Y_pre: [[57.86766804] [69.21400854] [77.04212681]] 5、梯度下降法的问题与解决思路 学习速率调整（学习速率调度，Learning rate schedules）：该方法试图在每次更新参数的过程中，改变学习速率。一般使用某种事先设定的策略或者在每次迭代中衰减一个较小的阈值 在稀疏特征数据中：很少出现的特征应该使用一个相对较大的学习速率 对于非凸目标函数：可能落入鞍点或平滑点 （1）、确定速率调整函数 1 2 3 4 5 6 7 8 9 10 import numpy as np import matplotlib.pyplot as plt t0,t1 = 5,50000 def learning_rate_schedule(alpha): return t0/(alpha * 1000 + t1) x_show = np.linspace(1,1000,1000) y_show = np.array([learning_rate_schedule(x_show[i]) for i in range(len(x_show))]) plt.plot(x_show,y_show) [\u0026lt;matplotlib.lines.Line2D at 0x1a768afe390\u0026gt;] （2）、更改学习速率 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 import numpy as np np.random.seed(20030428) # 维度 n = 4 # 数据量 m = 1000 # 设置预测值 x_pre = np.random.random(size = (3,n))*10 # 超参数 # learning_rate = 0.0001 # 学习率 α n_iterations = 1000 # 迭代次数 # 随机生成 x 序列 x = np.random.rand(m,n) * 2 # 补充 x0 列 X = np.c_[np.ones((m,1)),x] # 模拟 x 对应的 y 标签，误差呈正态分布 w = np.random.randint(0,6,size=(n+1,1)) y = X.dot(w) + np.random.randn(m,1) # 初始化参数向量 w_init = np.random.randn(n+1,1) # 求梯度 for n_iteration in range(n_iterations): gradient = X.T.dot(X.dot(w_init)-y) w_init = w_init - learning_rate_schedule(n_iteration)*gradient print(\u0026#39;W_hat:\u0026#39;) print(w_init.reshape(1,n+1)) # 预测 y_pre = np.c_[np.ones((3,1)),x_pre].dot(w_init) print(\u0026#39;X_pre: \u0026#39;) print(x_pre) print(\u0026#39;Y_pre: \u0026#39;) print(y_pre) W_hat: [[4.97242598 3.99644548 4.01333635 4.98719143 1.04554164]] X_pre: [[0.03016482 4.79096225 5.74410225 4.2038564 ] [0.0650456 9.53496505 3.37288732 7.77475153] [2.68648726 5.44484467 6.07293252 8.00564199]] Y_pre: [[57.36296538] [68.4494602 ] [76.21792798]] 三、坐标下降（Coordinate Descent） 坐标下降：一次走一步，每次只动一个变量。将高维问题拆成一系列一维问题，逐个坐标迭代更新，直到收敛 核心思想：固定其他所有变量，只沿第i个坐标方向做一维最优化，循环往复 迭代公式：$\\displaystyle{\\theta_i^{(k+1)} = \\arg\\min_{\\theta_i}Loss(\\theta_1^{(k+1)}, \\dots, \\theta_{i-1}^{(k+1)}, \\theta_i, \\theta_{i+1}^{(k)}, \\dots, \\theta_n^{(k)})}$，其中 $Loss$ 为目标损失函数，$\\theta^{(k)}$ 为第 $k$ 次迭代时的完整参数向量，$\\theta^{(k)}_i$ 为第 $k$ 次迭代时向量 $\\theta$ 的第 $i$ 个分量 对于MES的参数迭代公式（由MSE推导而来）： $$ \\displaystyle{\\theta_j = \\frac{\\displaystyle{ \\sum^m_{i=1}x^{(i)}_j(y^{(i)}-\\sum_{k\\neq j}\\theta_kx^{(i)}_k)}}{\\displaystyle{\\sum^m_{i=1}(x_j^{(i)})^2}}}\\Rightarrow \\theta^{(k+1)}_j = \\theta^{(k)}_j - \\displaystyle{\\frac{1}{X_{·j}^TX_{·j}}X_{·j}^T(X\\theta-y)} $$，其中 $m$ 为样本数量，$\\theta_j$ 为 $\\theta$ 的第 $j$ 个分量，$x^{(i)}_j$ 为第 $i$ 条样本的第 $j$ 个分量 代码实现 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 import numpy as np np.random.seed(20030428) # 维度 n = 4 # 数据量 m = 1000 # 设置预测值 x_pre = np.random.random(size = (3,n))*10 # 超参数 n_iterations = 1000 # 迭代次数 # 随机生成 x 序列 x = np.random.rand(m,n) * 2 # 补充 x0 列 X = np.c_[np.ones((m,1)),x] # 模拟 x 对应的 y 标签，误差呈正态分布 w = np.random.randint(0,6,size=(n+1,1)) y = X.dot(w) + np.random.randn(m,1) # 初始化参数向量 theta = np.zeros(shape=(n+1,1)) for _ in range(n_iterations): for j in range(theta.shape[0]): theta[j] = theta[j] - (1/(X[:,j]@X[:,j]))*X[:,j]@(X@theta - y) print(\u0026#39;W_hat:\u0026#39;) print(theta) # 预测 y_pre = np.c_[np.ones((3,1)),x_pre].dot(theta) print(\u0026#39;X_pre: \u0026#39;) print(x_pre) print(\u0026#39;Y_pre: \u0026#39;) print(y_pre) W_hat: [[5.02671056] [3.98404888] [4.00452019] [4.9783491 ] [1.02490084]] X_pre: [[0.03016482 4.79096225 5.74410225 4.2038564 ] [0.0650456 9.53496505 3.37288732 7.77475153] [2.68648726 5.44484467 6.07293252 8.00564199]] Y_pre: [[57.23707594] [68.2285754 ] [75.97196488]] 四、归一化（Normalization） 归一化是将不同尺度、纲量和分布的数据缩放到同一标准区间的预处理技术 核心目的：消除纲量影响、加速模型收敛、提升数值稳定性，同时保留原始信息的相对关系 特征级归一化方法 最大值最小值归一化（Min-Max）：受离心值影响较大 对第j个特征值做归一化：$$\\displaystyle{x^{*}_{i,j} = \\frac{x_{i,j}-x_j^{\\text{min}}}{x_j^{\\text{max}}-x_j^{\\text{min}}}}$$ 对整个数据集做归一化：$\\displaystyle{x^* = \\frac{x-min(x)}{max(x)-min(x)}}$ 标准归一化（Z-score）： 对第j个特征值做归一化： $$ \\displaystyle{x^*_{i,j} = \\frac{x_{i,j} - \\mu_j}{\\sigma_j}} $$ ，其中 $$ \\displaystyle{\\mu_j=\\frac{1}{n}\\sum_{i=1}^n x_{i,j},\\quad \\sigma_j=\\sqrt{\\frac{1}{n} \\sum_{i=1}^n (x_{i,j}-\\mu_j)^2}} $$ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 import numpy as np from sklearn.preprocessing import MinMaxScaler from sklearn.preprocessing import StandardScaler np.random.seed(20030428) # 数据维度 n = 4 # 数据规模 m = 100 # 初始化数据 x = np.empty((m, 2)) x[:, 0] = np.random.normal(1, 2, m) x[:, 1] = np.random.random(m) * 10-3 plt.scatter(x[:,0],x[:,1],label=\u0026#39;original\u0026#39;) # Min-Max归一化 minmax_scaler = MinMaxScaler() x_min_max_normalization = minmax_scaler.fit_transform(x) plt.scatter(x_min_max_normalization[:,0],x_min_max_normalization[:,1],label=\u0026#39;min-max\u0026#39;) minmax_scaler.fit(x) # Z-score归一化 standar_scaler = StandardScaler() x_standar_scaler = standar_scaler.fit_transform(x) plt.scatter(x_standar_scaler[:,0],x_standar_scaler[:,1],label=\u0026#39;z-score\u0026#39;) standar_scaler.fit(x) print(\u0026#39;μ =\u0026#39;, standar_scaler.mean_) print(\u0026#39;σ² =\u0026#39;, standar_scaler.var_) plt.legend() plt.show() μ = [1.07725633 2.44897092] σ² = [3.86482916 7.93336293] 五、正则化（Regularization） 过拟合与欠拟合 欠拟合（underfit）：未拟合到位，训练集和测试集准确率未达到最高 过拟合（overfit）：拟合过度，训练集准确率升高的同时，测试集的准确率反而降低 适度拟合（just right）：过拟合前，训练集和测试集准确率都达到最高时刻 正则化：防止过拟合，增加模型鲁棒性（Robust） 鲁棒性调优：使模型具有更好的鲁棒性，让模型的的泛化能力和推广能力更加强大 正则化本质：牺牲模型在训练集上的正确率以提高模型的推广能力，参数 w 在数值上越小越好，进而抵抗数值扰动。但 w 的数值不能极小，故而将原来的损失函数加上一个惩罚项 惩罚项（正则项）： L1 正则项： $$ \\displaystyle{L_1(w) = \\sum^m_{i=0}|w_i|} $$（曼哈顿距离）， $$ \\displaystyle{\\frac{\\partial}{\\partial w_i}L_1(w_i) = sign(w_i) = \\pm 1} $$ L2 正则项： $$ \\displaystyle{L_2(w) = \\sum^m_{i=0}|w_i|^2} $$（欧式距离的平方）， $$ \\displaystyle{\\frac{\\partial}{\\partial w_i}L_2(w_i) = 2w_i} $$ 正则化后的多元线性回归的损失函数 Lasso 回归（套索回归，稀疏性）： 损失函数 $$ J_{lasso}(\\theta)=MSE(\\theta)+L_1(\\theta) $$ 梯度 $$ \\displaystyle{\\nabla_{\\theta}J_{Lasso}(\\theta)=\\frac{\\partial}{\\partial \\theta}J(\\theta)+\\frac{\\partial}{\\partial \\theta}L_1(\\theta)=X^T(X\\theta-y)+\\lambda sign(\\theta)} $$ Ridge 回归（岭回归，平滑性）： 损失函数 $$ J_{ridge}(\\theta)=MSE(\\theta)+L_2(\\theta) $$ 梯度 $$ \\displaystyle{\\nabla_{\\theta}J_{Ridge}(\\theta)=\\frac{\\partial}{\\partial \\theta}J(\\theta)+\\frac{\\partial}{\\partial \\theta}L_2(\\theta)=X^T(X\\theta-y)+2\\lambda \\theta} $$ ElasticNet回归（弹性网络回归）： 损失函数 $$ J_{lasso}(\\theta)=MSE(\\theta)+L_1(\\theta)+L_2(\\theta) $$ 梯度 $$\\displaystyle{\\nabla_{\\theta}J_{Lasso}(\\theta)=\\frac{\\partial}{\\partial \\theta}J(\\theta)+\\frac{\\partial}{\\partial \\theta}L_1(\\theta)+\\frac{\\partial}{\\partial \\theta}L_2(\\theta)=X^T(X\\theta-y)+\\lambda[(1-r)sign(\\theta)+2r\\theta)]} $$ 注：Lasso 回归与 Ridge 回归只定义了 Loss 函数模型，可以用梯度下降法、坐标下降法、最小角回归（LARS）、正规方程等方法进行求解。在 sklearn 库中 Lasso 和 ElasticNet 使用坐标下降；Ridge 使用正规方程；SVGRegressor 使用随机梯度下降，可自定义正则化类型、正则化强度、学习率、阈值、迭代次数等等 1、Lasso回归（sklearn库） 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 import numpy as np from sklearn.linear_model import Lasso np.random.seed(20030428) # 维度 n = 4 # 数据量 m = 1000 # 设置预测值 x_pre = np.random.random(size = (3,n))*10 # 随机生成 x 序列 x = np.random.rand(m,n) * 2 # 补充 x0 列 X = np.c_[np.ones((m,1)),x] # 模拟 x 对应的 y 标签，误差呈正态分布 w = np.random.randint(0,6,size=(n+1,1)) y = X.dot(w) + np.random.randn(m,1) lasso_reg = Lasso(alpha=0.0015,max_iter=30000000) lasso_reg.fit(x,y) print(\u0026#39;W_hat:\u0026#39;) print(np.c_[lasso_reg.intercept_,lasso_reg.coef_.reshape(1,-1)]) # 预测 y_pre = lasso_reg.predict(x_pre) print(\u0026#39;X_pre: \u0026#39;) print(x_pre) print(\u0026#39;Y_pre: \u0026#39;) print(y_pre.reshape(-1,1)) 1 2 3 4 5 6 7 8 9 10 W_hat: [[5.0464025 3.97908982 3.99958499 4.97364032 1.01995191]] X_pre: [[0.03016482 4.79096225 5.74410225 4.2038564 ] [0.0650456 9.53496505 3.37288732 7.77475153] [2.68648726 5.44484467 6.07293252 8.00564199]] Y_pre: [[57.18512159] [68.14652889] [75.88324745]] 2、Ridge回归（sklearn库） 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 import numpy as np from sklearn.linear_model import Ridge np.random.seed(20030428) # 维度 n = 4 # 数据量 m = 1000 # 设置预测值 x_pre = np.random.random(size = (3,n))*10 # 随机生成 x 序列 x = np.random.rand(m,n) * 2 # 补充 x0 列 X = np.c_[np.ones((m,1)),x] # 模拟 x 对应的 y 标签，误差呈正态分布 w = np.random.randint(0,6,size=(n+1,1)) y = X.dot(w) + np.random.randn(m,1) ridge_reg = Ridge(alpha=0.4,solver=\u0026#39;sag\u0026#39;) ridge_reg.fit(x,y) print(\u0026#39;W_hat:\u0026#39;) print(np.c_[ridge_reg.intercept_,ridge_reg.coef_.reshape(1,-1)]) # 预测 y_pre = ridge_reg.predict(x_pre) print(\u0026#39;X_pre: \u0026#39;) print(x_pre) print(\u0026#39;Y_pre: \u0026#39;) print(y_pre.reshape(-1,1)) W_hat: [[5.04469365 3.97896047 3.99922338 4.97248436 1.02326193]] X_pre: [[0.03016482 4.79096225 5.74410225 4.2038564 ] [0.0650456 9.53496505 3.37288732 7.77475153] [2.68648726 5.44484467 6.07293252 8.00564199]] Y_pre: [[57.18895127] [68.16319935] [75.89870099]] 3、ElasticNet回归（sklearn库） 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 import numpy as np from sklearn.linear_model import ElasticNet np.random.seed(20030428) # 维度 n = 4 # 数据量 m = 1000 # 设置预测值 x_pre = np.random.random(size = (3,n))*10 # 随机生成 x 序列 x = np.random.rand(m,n) * 2 # 补充 x0 列 X = np.c_[np.ones((m,1)),x] # 模拟 x 对应的 y 标签，误差呈正态分布 w = np.random.randint(0,6,size=(n+1,1)) y = X.dot(w) + np.random.randn(m,1) elasticnet_reg = ElasticNet(alpha=0.01,l1_ratio=0.2) elasticnet_reg.fit(x,y) print(\u0026#39;W_hat:\u0026#39;) print(np.c_[elasticnet_reg.intercept_,elasticnet_reg.coef_.reshape(1,-1)]) # 预测 y_pre = elasticnet_reg.predict(x_pre) print(\u0026#39;X_pre: \u0026#39;) print(x_pre) print(\u0026#39;Y_pre: \u0026#39;) print(y_pre.reshape(-1,1)) W_hat: [[5.40784751 3.87770911 3.89554975 4.85588961 0.98362001]] X_pre: [[0.03016482 4.79096225 5.74410225 4.2038564 ] [0.0650456 9.53496505 3.37288732 7.77475153] [2.68648726 5.44484467 6.07293252 8.00564199]] Y_pre: [[56.21597338] [66.82977581] [74.39992651]] 4、随机梯度下降实现elasticnet正则化（sklearn库） 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 import numpy as np from sklearn.linear_model import SGDRegressor np.random.seed(20030428) # 维度 n = 4 # 数据量 m = 1000 # 设置预测值 x_pre = np.random.random(size = (3,n))*10 # 随机生成 x 序列 x = np.random.rand(m,n) * 2 # 补充 x0 列 X = np.c_[np.ones((m,1)),x] # 模拟 x 对应的 y 标签，误差呈正态分布 w = np.random.randint(0,6,size=(n+1,1)) y = X.dot(w) + np.random.randn(m,1) sgd_reg = SGDRegressor( penalty=\u0026#39;elasticnet\u0026#39;, # 正则化类型 max_iter = 100000, # 迭代次数 alpha = 0.01, # 正则化强度 λ l1_ratio=0.2, # l1正则化比例 tol = 1e-10, # 收敛阈值 # …… ) sgd_reg.fit(x,y.ravel()) print(\u0026#39;W_hat:\u0026#39;) print(np.c_[sgd_reg.intercept_,sgd_reg.coef_.reshape(1,-1)]) # 预测 y_pre = sgd_reg.predict(x_pre) print(\u0026#39;X_pre: \u0026#39;) print(x_pre) print(\u0026#39;Y_pre: \u0026#39;) print(y_pre.reshape(-1,1)) W_hat: [[5.1320958 3.93664843 3.95394899 4.913876 1.04281603]] X_pre: [[0.03016482 4.79096225 5.74410225 4.2038564 ] [0.0650456 9.53496505 3.37288732 7.77475153] [2.68648726 5.44484467 6.07293252 8.00564199]] Y_pre: [[56.80371943] [67.77050846] [75.42653888]] 六、升维方法 — 多项式回归（Polynomial Regression） 目的：解决欠拟合问题 常见手段：将已知维度进行相乘来构建新的维度，将非线性Data转换为线性Data 以二阶多项式升维为例： $$ y=w_0+w_1x_1+w_2x_2\\Rightarrow y^*=w_0+w_1x_1+w_2x_2+w_3x_1^2+w_4x_2^2+w_5x_1x_2 $$ 以下面拟合过程为例：$D_3$ 达到最佳拟合。随着维度的增加，$D_3$ 以下训练集和测试集误差均减小，属于欠拟合；$D_3$ 以上训练集误差减小，测试集误差增大，属于过拟合 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 import numpy as np import matplotlib.pyplot as plt from sklearn.preprocessing import PolynomialFeatures from sklearn.linear_model import LinearRegression from sklearn.metrics import mean_squared_error np.random.seed(20030428) plt.xlim(-21, 51) # 横轴 plt.ylim(-8000, 4000) # 纵轴 # 预设函数 def mapping(x): return 0.2*x**3 - 10*x**2 + 5*x - 1 # 维度 n = 1 x_show = np.linspace(-20,50,10000) plt.plot(x_show,mapping(x_show),color=\u0026#39;C0\u0026#39;,label=\u0026#39;scheduled model\u0026#39;) # 训练集数据量 train_size= 10000 # 测试集数据量 test_size = 10000 # 随机生成训练集 x_train = np.random.rand(m,n) * 60 - 20 y_train = mapping(x_train) + np.random.randn(m,1)*1111 plt.scatter(x_train,y_train,color=\u0026#39;C1\u0026#39;,s=5,label=\u0026#39;train set data\u0026#39;) # 随机生成测试集 x_test = np.random.rand(m,n) * 400 - 100 y_test = mapping(x_test) + np.random.randn(m,1)*1111 plt.scatter(x_test,y_test,color=\u0026#39;C2\u0026#39;,s=5,label=\u0026#39;train set data\u0026#39;) ### 升维预测 ### # dimensions = {2:\u0026#39;C3\u0026#39;} dimensions = {1:\u0026#39;C8\u0026#39;,2:\u0026#39;C3\u0026#39;,3:\u0026#39;C4\u0026#39;,4:\u0026#39;C5\u0026#39;,5:\u0026#39;C9\u0026#39;,6:\u0026#39;C7\u0026#39;,7:\u0026#39;C6\u0026#39;,8:\u0026#39;C10\u0026#39;} for dim,color in dimensions.items(): # 多项式升维 poly_features = PolynomialFeatures( degree=dim, # 设置维度 include_bias=True # 设置截距 ) # 训练集和测试集升维 x_train_poly = poly_features.fit_transform(x_train) x_test_poly = poly_features.fit_transform(x_test) # 线性回归 lin_reg = LinearRegression( fit_intercept=False # 不加偏置 ) lin_reg.fit(x_train_poly,y_train) y_train_pre = lin_reg.predict(x_train_poly) y_test_pre = lin_reg.predict(x_test_poly) train_norm = mean_squared_error(y_train,y_train_pre) test_norm = mean_squared_error(y_test,y_test_pre) lab = f\u0026#34;D:{dim}/Train:{train_norm/1e6:.5f}/Test:{test_norm/1e6:.5f}\u0026#34; x_show_poly = poly_features.fit_transform(x_show.reshape(-1,1)) plt.plot(x_show,lin_reg.predict(x_show_poly),color=dimensions[dim],label=lab) plt.legend( loc=\u0026#39;upper left\u0026#39;, # 以legend左上角为基准点 bbox_to_anchor=(1.02, 1), # 距离原点的相对位置，图像为0-1 borderaxespad=0, # 间距值 ncol=1, # 分几列 ) \u0026lt;matplotlib.legend.Legend at 0x1a768d8d150\u0026gt; 七、实践 — 保险花销预测 数据集路径： \u0026ldquo;.\\Dataset\\1_Insurance_Expense_Forecast\\insurance.csv\u0026rdquo; 1、数据提取 1 2 3 4 5 6 7 import pandas as pd import numpy as np data = pd.read_csv( \u0026#39;./Dataset/1_Insurance_Expense_Forecast/insurance.csv\u0026#39;, sep=\u0026#39;,\u0026#39;) data.head(6) age sex bmi children smoker region charges 0 19 female 27.900 0 yes southwest 16884.92400 1 18 male 33.770 1 no southeast 1725.55230 2 28 male 33.000 3 no southeast 4449.46200 3 33 male 22.705 0 no northwest 21984.47061 4 32 male 28.880 0 no northwest 3866.85520 5 31 female 25.740 0 no southeast 3756.62160 2、EDA（Exploratory Data Analysis，探索性数据分析） 本质：在把数据喂给算法之前，先用人眼和统计工具观察和清洗数据 目的：发现数据长什么样、哪里脏、哪里怪、哪里藏着有用的信号，从而为后续的特征工程、模型选择、甚至业务决策提供直觉和依据 发现charges出现右偏问题，通过特征工程中数值变换的取对数手段进行处理 1 2 3 4 # 观察数据结构信息 print(data.info()) # 结果显示数据质量较好，无缺失值 print(data.describe()) \u0026lt;class 'pandas.core.frame.DataFrame'\u0026gt; RangeIndex: 1338 entries, 0 to 1337 Data columns (total 7 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 age 1338 non-null int64 1 sex 1338 non-null object 2 bmi 1338 non-null float64 3 children 1338 non-null int64 4 smoker 1338 non-null object 5 region 1338 non-null object 6 charges 1338 non-null float64 dtypes: float64(2), int64(2), object(3) memory usage: 73.3+ KB None age bmi children charges count 1338.000000 1338.000000 1338.000000 1338.000000 mean 39.207025 30.663397 1.094918 13270.422265 std 14.049960 6.098187 1.205493 12110.011237 min 18.000000 15.960000 0.000000 1121.873900 25% 27.000000 26.296250 0.000000 4740.287150 50% 39.000000 30.400000 1.000000 9382.033000 75% 51.000000 34.693750 2.000000 16639.912515 max 64.000000 53.130000 5.000000 63770.428010 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 # 观察数据分布 import matplotlib.pyplot as plt # %matplotlib inline plt.rcParams[\u0026#39;font.family\u0026#39;] = \u0026#39;sans-serif\u0026#39; # 1. 启用 sans-serif 列表 plt.rcParams[\u0026#39;font.sans-serif\u0026#39;] = [\u0026#39;SimHei\u0026#39;] # 2. 把 SimHei 放在最前 plt.rcParams[\u0026#39;axes.unicode_minus\u0026#39;] = False # 3. 让负号正常显示（可选） plt.figure(figsize=(10, 8)) # 设置画布大小 plt.subplot(2, 2, 1) # 第一个子图 plt.hist(data[\u0026#39;age\u0026#39;], bins=20, color=\u0026#39;lightgreen\u0026#39;, edgecolor=\u0026#39;k\u0026#39;) plt.xlabel(\u0026#39;Ages\u0026#39;) plt.ylabel(\u0026#39;Count\u0026#39;) plt.title(\u0026#39;Distribution of Age\u0026#39;) plt.subplot(2, 2, 2) # 第二个子图 plt.hist(data[\u0026#39;bmi\u0026#39;], bins=25, color=\u0026#39;salmon\u0026#39;, edgecolor=\u0026#39;k\u0026#39;) plt.xlabel(\u0026#39;BMI\u0026#39;) plt.ylabel(\u0026#39;Count\u0026#39;) plt.title(\u0026#39;Distribution of BMI\u0026#39;) plt.subplot(2, 2, 3) # 第三个子图 plt.hist(data[\u0026#39;children\u0026#39;], bins=6, color=\u0026#39;gold\u0026#39;, edgecolor=\u0026#39;k\u0026#39;) plt.xlabel(\u0026#39;Children\u0026#39;) plt.ylabel(\u0026#39;Count\u0026#39;) plt.title(\u0026#39;Distribution of Children\u0026#39;) ax_charges=plt.subplot(2, 2, 4) # 第四个子图 plt.hist(data[\u0026#39;charges\u0026#39;], bins=30, color=\u0026#39;skyblue\u0026#39;, edgecolor=\u0026#39;k\u0026#39;) plt.xlabel(\u0026#39;Charges\u0026#39;) plt.ylabel(\u0026#39;Count\u0026#39;) plt.title(\u0026#39;Distribution of Charges\u0026#39;) ax_charges.text( 0.95,0.95, # 百分比坐标 \u0026#39;右偏（positively skewed / right-skewed）\\n指分布的尾部向右（值大的一侧）拖得很长\\n均值 \u0026gt; 中位数\u0026#39;, transform=ax_charges.transAxes, # 用子图的坐标系 va=\u0026#39;top\u0026#39;,ha=\u0026#39;right\u0026#39;, # 文本框基准点选取 fontsize = 10, bbox=dict( boxstyle=\u0026#39;round,pad=0.3\u0026#39;, facecolor=(0.9,0.9,0.9), alpha=0.5, ) ) plt.tight_layout() # 自动调整间距 plt.show() 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 # 压缩优化异常标签值 ax = plt.subplot(1, 1, 1) ax.text( 0.55,0.95, # 百分比坐标 \u0026#39;通过ln函数将原右偏拖尾压缩，\\n使结果更接近正态\u0026#39;, transform=ax.transAxes, # 用子图的坐标系 va=\u0026#39;top\u0026#39;,ha=\u0026#39;left\u0026#39;, # 文本框基准点选取 fontsize = 10, bbox=dict( boxstyle=\u0026#39;round,pad=0.3\u0026#39;, facecolor=(0.9,0.9,0.9), alpha=0.5, ) ) plt.hist(np.log1p(data[\u0026#39;charges\u0026#39;]),bins=20, color=\u0026#39;skyblue\u0026#39;, edgecolor=\u0026#39;k\u0026#39;) plt.title(\u0026#39;log of \\\u0026#39;charges\\\u0026#39;\u0026#39;) plt.show() # 增加标签值压缩列 data[\u0026#39;log_charges\u0026#39;]=np.log1p(data[\u0026#39;charges\u0026#39;]) data.head(6) age sex bmi children smoker region charges log_charges 0 19 female 27.900 0 yes southwest 16884.92400 9.734236 1 18 male 33.770 1 no southeast 1725.55230 7.453882 2 28 male 33.000 3 no southeast 4449.46200 8.400763 3 33 male 22.705 0 no northwest 21984.47061 9.998137 4 32 male 28.880 0 no northwest 3866.85520 8.260455 5 31 female 25.740 0 no southeast 3756.62160 8.231541 3、特征工程 类别：缺失值处理/数值变换/类别编码/高维稀疏/非线性交叉/时序\u0026amp;序列/业务先验 类别编码 — One-Hot编码：对某个特征从“分类”到“向量”的变化过程，例如： 若简单地将“男”/“女”，编码为0/1，模型会误以为“女”\u0026gt;“男”，把颜色当成连续量，学到错误的序关系 将“男”/“女”编码为二维向量 $(0,1)$ 和 $(1,0)$ 将本项目中的region特征中的southeast、southwest、northeast、northwest编码为$(1,0,0,0)，(0,1,0,0)，(0,0,1,0)，(0,0,0,1)$ 1 2 3 # 进行 One-Hot编码 data = pd.get_dummies(data,dtype=int) data.head(6) age bmi children charges log_charges sex_female sex_male smoker_no smoker_yes region_northeast region_northwest region_southeast region_southwest 0 19 27.900 0 16884.92400 9.734236 1 0 0 1 0 0 0 1 1 18 33.770 1 1725.55230 7.453882 0 1 1 0 0 0 1 0 2 28 33.000 3 4449.46200 8.400763 0 1 1 0 0 0 1 0 3 33 22.705 0 21984.47061 9.998137 0 1 1 0 0 1 0 0 4 32 28.880 0 3866.85520 8.260455 0 1 1 0 0 1 0 0 5 31 25.740 0 3756.62160 8.231541 1 0 1 0 0 0 1 0 1 2 3 4 5 6 7 8 9 # 取出样本及标签值 x = data.drop(columns=[\u0026#39;charges\u0026#39;,\u0026#39;log_charges\u0026#39;]) y = data[\u0026#39;log_charges\u0026#39;] # 填充空值 x.fillna(0,inplace=True) y.fillna(0,inplace=True) x.head() age bmi children sex_female sex_male smoker_no smoker_yes region_northeast region_northwest region_southeast region_southwest 0 19 27.900 0 1 0 0 1 0 0 0 1 1 18 33.770 1 0 1 1 0 0 0 1 0 2 28 33.000 3 0 1 1 0 0 0 1 0 3 33 22.705 0 0 1 1 0 0 1 0 0 4 32 28.880 0 0 1 1 0 0 1 0 0 1 2 3 4 5 6 # 划分训练集与测试集 from sklearn.model_selection import train_test_split x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3) print(x.shape,y.shape) print(x_train.shape,y_train.shape) print(x_test.shape,y_test.shape) (1338, 11) (1338,) (936, 11) (936,) (402, 11) (402,) 1 2 3 4 5 6 7 8 9 # 归一化 from sklearn.preprocessing import StandardScaler x_scaler = StandardScaler( copy=True, # 非原地更改 with_mean=True, # 先减去平均值 with_std=True, # 除以标准差 ).fit(x_train) # 用测试集数据训练出均值和标准差 x_train_scaled = x_scaler.transform(x_train) x_test_scaled = x_scaler.transform(x_test) 1 2 3 4 5 6 7 8 # 多项式升维以拟合非线性特征 from sklearn.preprocessing import PolynomialFeatures poly_features = PolynomialFeatures( degree = 1, include_bias=False ) x_train_scaled = poly_features.fit_transform(x_train_scaled) x_test_scaled = poly_features.fit_transform(x_test_scaled) 4、模型训练 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 from sklearn.linear_model import LinearRegression from sklearn.linear_model import Ridge from sklearn.linear_model import SGDRegressor # 线性回归 lin_reg = LinearRegression() lin_reg.fit(x_train_scaled,y_train) y_train_predict_lin = lin_reg.predict(x_train_scaled) y_test_predict_lin = lin_reg.predict(x_test_scaled) # 岭回归 rid_reg = Ridge() rid_reg.fit(x_train_scaled,y_train) y_train_predict_rid = rid_reg.predict(x_train_scaled) y_test_predict_rid = rid_reg.predict(x_test_scaled) # 随机梯度下降 sgd_reg = SGDRegressor() sgd_reg.fit(x_train_scaled,y_train) y_train_predict_sgd = sgd_reg.predict(x_train_scaled) y_test_predict_sgd = sgd_reg.predict(x_test_scaled) 5、模型评估 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 from sklearn.metrics import mean_squared_error print( \u0026#34;LinearRegression(Train / Test): \u0026#34;, np.sqrt(mean_squared_error(y_train,y_train_predict_lin)), np.sqrt(mean_squared_error(y_test,y_test_predict_lin)) ) print( \u0026#34;RidgeRegression(Train / Test): \u0026#34;, np.sqrt(mean_squared_error(y_train,y_train_predict_rid)), np.sqrt(mean_squared_error(y_test,y_test_predict_rid)) ) print( \u0026#34;SGDRegression(Train / Test): \u0026#34;, np.sqrt(mean_squared_error(y_train,y_train_predict_sgd)), np.sqrt(mean_squared_error(y_test,y_test_predict_sgd)) ) LinearRegression(Train / Test): 0.44905273756975034 0.43211955383967054 RidgeRegression(Train / Test): 0.44905315842354915 0.4321996372029468 SGDRegression(Train / Test): 0.449286159412961 0.43308751362633735 6、进阶 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns from sklearn.preprocessing import PolynomialFeatures from sklearn.model_selection import train_test_split from sklearn.linear_model import LinearRegression from sklearn.linear_model import Ridge from sklearn.ensemble import GradientBoostingRegressor from sklearn.metrics import mean_squared_error from sklearn.preprocessing import StandardScaler # 数据读取 data = pd.read_csv(\u0026#39;./Dataset/1_Insurance_Expense_Forecast/insurance.csv\u0026#39;) # EDA # 如果对于某个特征对预测值几乎无影响，则可忽略（降噪） data[\u0026#39;charges\u0026#39;] = np.log1p(data[\u0026#39;charges\u0026#39;]) print(data.head()) print(data.info()) print(data.describe()) ax = plt.subplot(2,2,1) plt.title(\u0026#39;sex\u0026#39;) sns.kdeplot(data.loc[data.sex==\u0026#39;male\u0026#39;,\u0026#39;charges\u0026#39;], fill=True, label=\u0026#39;male\u0026#39;, ax=ax) sns.kdeplot(data.loc[data.sex==\u0026#39;female\u0026#39;,\u0026#39;charges\u0026#39;], fill=True, label=\u0026#39;female\u0026#39;, ax=ax) plt.legend() ax = plt.subplot(2,2,2) plt.title(\u0026#39;smoke\u0026#39;) sns.kdeplot(data.loc[data.smoker==\u0026#39;yes\u0026#39;,\u0026#39;charges\u0026#39;], fill=True, label=\u0026#39;smoker_yes\u0026#39;, ax=ax) sns.kdeplot(data.loc[data.smoker==\u0026#39;no\u0026#39;,\u0026#39;charges\u0026#39;], fill=True, label=\u0026#39;smoker_no\u0026#39;, ax=ax) plt.legend() ax = plt.subplot(2,2,3) plt.title(\u0026#39;region\u0026#39;) sns.kdeplot(data.loc[data.region==\u0026#39;southeast\u0026#39;,\u0026#39;charges\u0026#39;], fill=True,label=\u0026#39;southeast\u0026#39;,ax=ax) sns.kdeplot(data.loc[data.region==\u0026#39;southwest\u0026#39;,\u0026#39;charges\u0026#39;], fill=True,label=\u0026#39;southwest\u0026#39;,ax=ax) sns.kdeplot(data.loc[data.region==\u0026#39;northeast\u0026#39;,\u0026#39;charges\u0026#39;], fill=True,label=\u0026#39;northeast\u0026#39;,ax=ax) sns.kdeplot(data.loc[data.region==\u0026#39;northwest\u0026#39;,\u0026#39;charges\u0026#39;], fill=True,label=\u0026#39;northwest\u0026#39;,ax=ax) plt.legend() ax = plt.subplot(2,2,4) plt.title(\u0026#39;children\u0026#39;) sns.kdeplot(data.loc[data.children==0,\u0026#39;charges\u0026#39;], fill=True, label=\u0026#39;children_0\u0026#39;, ax=ax) sns.kdeplot(data.loc[data.children==1,\u0026#39;charges\u0026#39;], fill=True, label=\u0026#39;children_1\u0026#39;, ax=ax) sns.kdeplot(data.loc[data.children==2,\u0026#39;charges\u0026#39;], fill=True, label=\u0026#39;children_2\u0026#39;, ax=ax) sns.kdeplot(data.loc[data.children==3,\u0026#39;charges\u0026#39;], fill=True, label=\u0026#39;children_3\u0026#39;, ax=ax) sns.kdeplot(data.loc[data.children==4,\u0026#39;charges\u0026#39;], fill=True, label=\u0026#39;children_4\u0026#39;, ax=ax) sns.kdeplot(data.loc[data.children==5,\u0026#39;charges\u0026#39;], fill=True, label=\u0026#39;children_5\u0026#39;, ax=ax) plt.legend() plt.tight_layout() plt.show() # 特征工程 data = data.drop([\u0026#39;region\u0026#39;,\u0026#39;sex\u0026#39;],axis=1) # 删除无用特征 # 特征离散化 def discretization(df,bmi=30,child=0): df[\u0026#39;bmi\u0026#39;]=\u0026#39;over\u0026#39; if df[\u0026#39;bmi\u0026#39;]\u0026gt;=bmi else \u0026#39;under\u0026#39; df[\u0026#39;children\u0026#39;] = \u0026#39;no\u0026#39; if df[\u0026#39;children\u0026#39;] == child else \u0026#39;yes\u0026#39; return df data = data.apply(discretization,axis=1,args=(30,0)) print(data.head()) # one-hot 编码 data = pd.get_dummies(data) # 解决右偏问题 print(data.head()) # 样本获取与空值填充 x = data.drop(\u0026#39;charges\u0026#39;,axis=1) y = data[\u0026#39;charges\u0026#39;] x.fillna(0,inplace=True) y.fillna(0,inplace=True) print(x.head()) print(y.head()) # 模型训练 # 划分 x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3) # # 归一化 # x_scaler = StandardScaler().fit(x_train) # x_train = x_scaler.transform(x_train) # 升维 poly_featrues = PolynomialFeatures(degree=2,include_bias=False) x_train_poly = poly_features.fit_transform(x_train) # 训练 reg_lin = LinearRegression() reg_lin.fit(x_train_poly,y_train) reg_rid = Ridge() reg_rid.fit(x_train_poly,y_train) reg_gra = GradientBoostingRegressor() reg_gra.fit(x_train_poly,y_train) # 测试集预测 # # 归一化 # x_test = x_scaler.transform(x_test) # 升维 x_test_poly = poly_features.fit_transform(x_test) # 预测 y_test_pre_lin = reg_lin.predict(x_test_poly) y_test_pre_rid = reg_rid.predict(x_test_poly) y_test_pre_gra = reg_gra.predict(x_test_poly) # 模型评估 print( \u0026#34;LinearRegression: \u0026#34;, np.sqrt(mean_squared_error(y_test,y_test_pre_lin)) ) print( \u0026#34;RidgeRegression: \u0026#34;, np.sqrt(mean_squared_error(y_test,y_test_pre_rid)) ) print( \u0026#34;GradientBoostingRegressor: \u0026#34;, np.sqrt(mean_squared_error(y_test,y_test_pre_gra)) ) age sex bmi children smoker region charges 0 19 female 27.900 0 yes southwest 9.734236 1 18 male 33.770 1 no southeast 7.453882 2 28 male 33.000 3 no southeast 8.400763 3 33 male 22.705 0 no northwest 9.998137 4 32 male 28.880 0 no northwest 8.260455 \u0026lt;class 'pandas.core.frame.DataFrame'\u0026gt; RangeIndex: 1338 entries, 0 to 1337 Data columns (total 7 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 age 1338 non-null int64 1 sex 1338 non-null object 2 bmi 1338 non-null float64 3 children 1338 non-null int64 4 smoker 1338 non-null object 5 region 1338 non-null object 6 charges 1338 non-null float64 dtypes: float64(2), int64(2), object(3) memory usage: 73.3+ KB None age bmi children charges count 1338.000000 1338.000000 1338.000000 1338.000000 mean 39.207025 30.663397 1.094918 9.098828 std 14.049960 6.098187 1.205493 0.919379 min 18.000000 15.960000 0.000000 7.023647 25% 27.000000 26.296250 0.000000 8.464064 50% 39.000000 30.400000 1.000000 9.146658 75% 51.000000 34.693750 2.000000 9.719618 max 64.000000 53.130000 5.000000 11.063061 age bmi children smoker charges 0 19 under no yes 9.734236 1 18 over yes no 7.453882 2 28 over yes no 8.400763 3 33 under no no 9.998137 4 32 under no no 8.260455 age charges bmi_over bmi_under children_no children_yes smoker_no \\ 0 19 9.734236 False True True False False 1 18 7.453882 True False False True True 2 28 8.400763 True False False True True 3 33 9.998137 False True True False True 4 32 8.260455 False True True False True smoker_yes 0 True 1 False 2 False 3 False 4 False age bmi_over bmi_under children_no children_yes smoker_no smoker_yes 0 19 False True True False False True 1 18 True False False True True False 2 28 True False False True True False 3 33 False True True False True False 4 32 False True True False True False 0 9.734236 1 7.453882 2 8.400763 3 9.998137 4 8.260455 Name: charges, dtype: float64 LinearRegression: 0.41171540130927264 RidgeRegression: 0.4119283802418486 GradientBoostingRegressor: 0.3163566171772987 ","date":"2025-12-02T13:16:53+08:00","permalink":"http://localhost:1313/p/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/","title":"机器学习-线性回归"}]