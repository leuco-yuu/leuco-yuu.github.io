[{"content":" 目录 K均值聚类（K-Means Clustering） 层次聚类（Hierarchical Clustering） 密度聚类 高斯混合模型（GMM模型） 单个高斯分布（GM） 混合高斯模型（GMM） EM算法 GMM流程 降维 降维方法： PCA降维 无监督学习（Unsupervised Learning） 聚类是一种无监督的机器学习任务，它可以自动将数据划分成类（Cluster） K均值聚类（K-Means Clustering） 对于没有标签的数据样本 $X$ ，根据 $X$ 的相似度划分为 $k$ 类 K-Means一般过程： 初始化：指定中心数（簇数） $k$ 以及 $k$ 个簇中心（ $ \\mu_1,\\mu_2\\dots \\mu_k $ ，经典K-Means中随机选择中心） 迭代： 分配步：对每个样本 $$ x_i,i \\in \\{ 1,2\\dots m \\},$$ 计算 $ x_i $与 $ \\mu_1,\\mu_2\\dots \\mu_k $ 的距离，并将 $ x_i $ 分配给簇中心距离其最近的簇：$$\\displaystyle{x_i \\rightarrow Cluster_{ \\underset{j}{argmin}||x_i-\\mu_j||^2}}$$ 更新步：更新簇中心：$$\\displaystyle{\\mu_j=\\frac{1}{|N_j|}\\sum_{x_i\\in Cluster_j}x_i}$$ 迭代至： 簇中心不变或变化幅度小于阈值 样本点的归属不再发生变化 迭代次数达到最大 K-Means损失函数MSE：每个点到中心点的距离 $$\\displaystyle{J(\\mu_1,\\mu_2 \\dots \\mu_k) = \\frac{1}{2} \\sum^k_{j=1}\\sum^{N_j}_{i=1}(x_i-\\mu_j)^2}$$ $k$ 的选取——肘部法 依次选取 $k\\in{1,2\\dots m}$，计算整体的损失 最终选取 $k_m$ 使得收益： $$Gain=|Loss(k_m-1)-Loss(k_m)|-|Loss(k_m)-Loss(k_m+1)|$$ 取得最大值。即$$\\displaystyle{k_m = \\underset{k}{\\operatorname{argmax}} |Loss(k-1)-Loss(k)|-|Loss(k)-Loss(k+1)|}$$ 相似度——以距离函数衡量 闵可夫斯基距离：$$\\displaystyle{d_p(x,y)=\\sqrt[p]{\\sum_{i=1}^n|x_n-y_n|^p}}$$ 曼哈顿距离(p=1)：$$\\displaystyle{d_1(x,y)=\\sum_{i=1}^n|x_n-y_n|}$$ 欧式距离(p=2)：$$\\displaystyle{d_2(x,y)=\\sqrt{(x-y)^T(x-y)}}$$ 切比雪夫距离(p=$\\infty$)：$$\\displaystyle{d_\\infty(x,y)=\\max\\limits_{i\\in\\{1,2\\dots n\\}}|x_i-y_i|}$$ 余弦距离：$$\\displaystyle{d_{\\cos}(x,y)=\\frac{x^Ty}{||x||·||y||}} \\in [-1,1]$$ Jaccard相似系数：$$\\displaystyle{J(A,B) = \\frac{|A\\cap B|}{|A \\cup B|} = \\frac{|A\\cap B|}{|A|+|B|-|A\\cap B|}}$$ K-Means变种 K-Mediods：计算新的簇中心的时候不再选择均值，而选择中位数。抗噪能力得到加强 二分K-Means：合并簇中心点比较近，MSE很小的簇；切分簇中心点比较远，MSE比较大的簇。重新进行K-Means聚类 K-Means++：使初始化簇中心稍微远一点。随机选择第一个中心点，计算MSE，将MSE转化为概率进行概率化选择初始簇中心点 Canopy聚类： 一次迭代，给出k的值以及k个初始中心点（然后进行K-Means算法） 层次聚类（Hierarchical Clustering） 分裂法 思想：将所有样本归为一个簇。每次迭代在一个簇中找到距离最远的两个样本点，将该簇划分为两个子簇。依次类推直到划分为k个簇 凝聚法 思想：将所有样本点看作一个独立的簇。每次迭代找到距离最小的两个簇进行合并。 密度聚类 DB-SCAN聚类（Density_Based Spatial Clustering of Applications with Noise） 与层次聚类不同，它将簇定义为密度相连的点的最大集合，能够把具有高密度的区域划分为簇，并可有效地对抗噪声 密度相连 直接密度可达（若对象q的e邻域内至少有m个对象，m指定。则q为核心对象） 若给定一个对象集合D，如果p在q的e邻域内，而q是一个核心对象，则p从q出发是直接密度可达的 若q直接密度可达r，r直接密度可达p，则q直接密度可达p 密度可达：如果存在一个对象链使得相邻两对象直接密度可达，则称对象链首尾密度可达 密度相连：若o密度可达p，o密度可达q，则p，q密度相连 DB-SCAN通过检查数据集中每个对象的e邻域来寻找聚类 如果一个点p是核心对象则以p为中心创建新簇。依据p来反复寻找密度相连的集合（可能合并原有簇），当没有新点时寻找结束 密度最大值聚类 局部密度： $$ \\displaystyle{\\rho_i = \\sum_j\\chi(d_{ij}-d_c), 其中 \\chi(x) = } \\begin{cases} 1 \u0026, x \u003c 0 \\\\ 0 \u0026, otherwise \\end{cases} $$ $D_c$ 是一个截断距离，$\\rho_i$ 即到对象 $i$ 的距离小于 $D_c$ 的对象的个数，即： $\\rho_i$ = 任何一个点以 $D_c$ 为半径的圆内的样本点的数量， $D_c$ 的设定经验是使每个点的邻居数目是所有点的 1% ~ 2% 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 import time import numpy as np import matplotlib.pyplot as plt from sklearn.datasets import make_blobs # 生成高斯blob数据 from sklearn.cluster import KMeans,AgglomerativeClustering,DBSCAN # KMeans、层次、密度聚类 from sklearn.metrics import adjusted_rand_score,silhouette_score from sklearn.preprocessing import StandardScaler titles = [\u0026#39;Ground Truth\u0026#39;,\u0026#39;K-Means(random)\u0026#39;,\u0026#39;K-Means++\u0026#39;,\u0026#39;K-Means(Canopy)\u0026#39;,\u0026#39;Hierarchical\u0026#39;,\u0026#39;DBSCAN\u0026#39;] # colors = [ # \u0026#39;#e41a1c\u0026#39;, \u0026#39;#377eb8\u0026#39;, \u0026#39;#4daf4a\u0026#39;, \u0026#39;#984ea3\u0026#39;, # 红 蓝 绿 紫 # \u0026#39;#ff7f00\u0026#39;, \u0026#39;#ffff33\u0026#39;, # 橙 黄 # \u0026#39;#1b9e77\u0026#39;, \u0026#39;#d95f02\u0026#39;, \u0026#39;#7570b3\u0026#39;, \u0026#39;#e7298a\u0026#39;, # 青 棕 靛 洋红 # \u0026#39;#66a61e\u0026#39;, \u0026#39;#666666\u0026#39; # 草绿 深灰 # ] # markers = [ # \u0026#39;o\u0026#39;, # 圆 # \u0026#39;s\u0026#39;, # 方 # \u0026#39;^\u0026#39;, # 上三角 # \u0026#39;d\u0026#39;, # 钻石 # \u0026#39;v\u0026#39;, # 下三角 # \u0026#39;p\u0026#39;, # 五边形 # \u0026#39;*\u0026#39;, # 星号 # \u0026#39;h\u0026#39;, # 六边形1 # \u0026#39;H\u0026#39;, # 六边形2 # \u0026#39;8\u0026#39;, # 八边形 # \u0026#39;\u0026gt;\u0026#39;, # 右三角 # \u0026#39;\u0026lt;\u0026#39; # 左三角 # ] # 数据可视化 def data_visualization_only(x,y): plt.scatter(x[:,0],x[:,1],s=12,c=y) plt.show() def data_visualization_all(labels,times): fix,axes = plt.subplots(2,3,figsize=(20,12)) axes = axes.flatten() for index,(ax,lab,tit,t) in enumerate(zip(axes,labels,titles,times)): ax.scatter(X[:,0],X[:,1],c=lab,s=12,cmap=\u0026#39;tab10\u0026#39;,) ax.set_title(f\u0026#34;{tit}({t:.4f}s)\u0026#34;) # ax.set_title(tit+\u0026#34;(\u0026#34;++\u0026#34;s)\u0026#34;) ax.set_xticks([]) ax.set_yticks([]) plt.tight_layout() plt.show() # 一 构造模拟数据 centers = [(0,0),(8,8),(-5,10),(10,-4)] # 样本中心 cluster_std = [1.2, 0.8, 2.0, 0.5] # 标准差 n_samples = [800,600,400,200] # 样本数量 X,y_true = make_blobs( n_samples=n_samples, centers=centers, cluster_std=cluster_std, random_state=42 ) # data_visualization_only(X,y_true) # 二 算法实现 def kmeans_random(X, k): \u0026#39;\u0026#39;\u0026#39; 标准K-Means算法，随机选择初始中心 \u0026#39;\u0026#39;\u0026#39; start = time.time() model = KMeans( n_clusters=k, init=\u0026#39;random\u0026#39;, n_init=10, random_state=42 ) labels = model.fit_predict(X) return labels,time.time()-start def kmeans_pp(X,k): \u0026#39;\u0026#39;\u0026#39; K-Means++:智能初始化，远离地选择中心 \u0026#39;\u0026#39;\u0026#39; start=time.time() model = KMeans( n_clusters=k, init=\u0026#39;k-means++\u0026#39;, n_init=10, random_state=42 ) labels = model.fit_predict(X) return labels,time.time()-start def kmeans_canopy(X,t1=4.0,t2=2.0): \u0026#39;\u0026#39;\u0026#39; K-Means+Canopy。智能选择k和初始化中心 \u0026#39;\u0026#39;\u0026#39; start = time.time() canopies=[] X_copy = X.copy() np.random.shuffle(X_copy) for pt in X_copy: if any(np.linalg.norm(pt-c) \u0026lt; t2 for c in canopies): continue canopies.append(pt) k = len(canopies) model = KMeans( n_clusters=k, init=np.vstack(canopies), n_init=1, random_state=42 ) labels = model.fit_predict(X) return labels,time.time()-start def hierarchical(X,k): \u0026#39;\u0026#39;\u0026#39; 凝聚层次聚类，使用Ward方差最小化准则 \u0026#39;\u0026#39;\u0026#39; start = time.time() model = AgglomerativeClustering( n_clusters=k, linkage=\u0026#39;ward\u0026#39; ) labels = model.fit_predict(X) return labels,time.time()-start def dbscan(X): \u0026#39;\u0026#39;\u0026#39; 密度聚类DBSCAN \u0026#39;\u0026#39;\u0026#39; start = time.time() X_stand = StandardScaler().fit_transform(X) model = DBSCAN( eps = 0.35, # 指定e邻域 min_samples=5 # 指定m ) labels = model.fit_predict(X_stand) return labels,time.time()-start # 三 数据训练 k_true = len(set(y_true)) labels_random ,t_random = kmeans_random(X,k_true) labels_pp ,t_pp = kmeans_pp(X,k_true) labels_canopy ,t_canopy = kmeans_canopy(X,30,15) labels_hier ,t_hier = hierarchical(X,k_true) labels_dbscan ,t_dbscan = dbscan(X) labels = [ y_true, labels_random, labels_pp, labels_canopy, labels_hier, labels_dbscan ] times = [ 0, t_random, t_pp, t_canopy, t_hier, t_dbscan ] # 四 结果评估 def evaluate(name,labels,time): ari = adjusted_rand_score(y_true,labels) # 任意一对样本，看“预测是否同簇”与“真实是否同类”一致的比例 # 轮廓系数：a为样本与同簇其他点的平均距离；b是样本与最近邻簇所有点的平均距离 # 轮廓系数 si_i = (b-a)/max(a,b)。轮廓系数约接近1，簇间距离越远，簇内距离约近，聚类越理想 sil = silhouette_score(X,labels) if len(set(labels))\u0026gt;1 else np.nan print(f\u0026#34;{name:15s} | ARI={ari:5.3f} | Sil={sil:5.3f} | Time={time:.3f}s\u0026#34;) for index,(label,time) in enumerate(zip(labels,times)): evaluate(titles[index],label,time) # 五 可视化 data_visualization_all(labels,times) Ground Truth | ARI=1.000 | Sil=0.801 | Time=0.000s K-Means(random) | ARI=0.999 | Sil=0.801 | Time=0.012s K-Means++ | ARI=0.999 | Sil=0.801 | Time=0.014s K-Means(Canopy) | ARI=0.823 | Sil=0.687 | Time=0.013s Hierarchical | ARI=1.000 | Sil=0.801 | Time=0.068s DBSCAN | ARI=0.999 | Sil=0.769 | Time=0.025s 高斯混合模型（GMM模型） 单个高斯分布（GM） 高斯分布：$N(\\mu,\\sigma)$ 通过MLE思想估计 $\\mu$ 和 $\\sigma$ : $$\\displaystyle{\\hat \\mu=\\frac{1}{n}\\sum_{i \\in \\{1,2 \\dots n\\}} x_i=\\bar x}$$ $$\\displaystyle{\\hat \\sigma^2=\\frac{1}{n}\\sum_{i \\in \\{1,2 \\dots n\\}}(x_i-\\hat\\mu)^2$$ 混合高斯模型（GMM） GMM：假设随机变量 $X$ 是由 $k$ 个高斯分布混合而来，取到各个高斯分布的概率为 $\\pi_1,\\pi_2 \\dots \\pi_k$ ，第 $i$ 个高斯分布的均值为 $\\mu_i$ ，标准差为 $\\sigma_i$ 。当前观测到一系列样本 $X_1,X_2\\dots X_n$ ，估计参数向量： $\\pi，\\mu，\\sigma$ $$\\displaystyle{L_{\\pi,\\mu,\\sigma}(X) = \\sum^N_{i=1}\\log P(X_i)\\overset{\\text{全概率公式}}{=}\\sum^N_{i=1}\\log(\\sum^K_{k=1}\\pi_kN(x_i|\\mu_k,\\sigma_k))}$$ EM算法求解估计 $\\pi，\\mu，\\sigma$ EM算法 Jensen不等式：若f是凸函数，x是随机变量。有： $$f(\\mathcal{E}x) \\le \\mathcal{E}f(x),\\displaystyle{\\sum_i \\mathcal{E}_i=1}$$ E-Step（求责任度）与M-Step（更新参数） GMM流程 变量介绍： $ N $ 为数据集中样本总数量 $ N_k $ 为第 $ k $ 个高斯分布的有效数据点数量 $ K $ 为聚类数（即预设的高斯分布数量） $ X $ 为整个数据集（ $ X_i $ 为第 $ i $ 个数据向量） $ \\pi_k $ 为第 $ k $ 个高斯分布的混合系数/权重 $ \\mu_k $ 为第 $ k $ 个高斯分布的均值向量 $ \\sigma_k $ 为第 $ k $ 个高斯分布的协方差 $ \\gamma(i,k) $ 为责任度或响应度（即E步计算核心） 第零步：随机找 $m$ 个数据（中心点） 确定初始 $\\pi,\\mu,\\sigma$ ，例如随机等 第一步：估计数据来源于哪个分布(E步——责任度) 代入：$$\\displaystyle{\\gamma(i,k) = P(X \\in Z_k | X=X_i) \\overset{\\text{贝叶斯公式}}{=} \\frac{\\pi_kN(X_i|\\mu_k,\\sigma_k)}{\\displaystyle{\\sum^K_{j=1}\\pi_jN(X_i|\\mu_j,\\sigma_j)}}}$$ 根据 $\\gamma$ 划分类别 第二步：更新参数（$\\pi，\\mu，\\sigma$）（M步——参数更新） $$\\displaystyle{N_k=\\sum^N_{i=1}\\gamma(i,k)}$$ $$\\displaystyle{\\mu_k = \\frac{1}{N_k}\\displaystyle{\\sum_{i}^N\\gamma(i,k)X_i}}$$ $$\\displaystyle{\\sigma_k = \\frac{1}{N_k}\\sum^N_{i=1}\\gamma(i,k)(x_i-\\mu_k)(x_i-\\mu_k)^T}$$ $$\\displaystyle{\\pi_k = \\frac{N_k}{N}=\\frac{1}{N}\\sum_{i=1}^N\\gamma(i,k)}$$ 第三步：迭代，直到 $\\pi,\\mu,\\sigma$ 不再变化 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 import time import numpy as np import matplotlib.pyplot as plt from sklearn.datasets import make_blobs # 生成高斯blob数据 from sklearn.mixture import GaussianMixture from sklearn.metrics import adjusted_rand_score,adjusted_mutual_info_score,silhouette_score from sklearn.preprocessing import StandardScaler # 数据可视化 def data_visualization_only(x,y): plt.scatter(x[:,0],x[:,1],s=12,c=y) plt.show() # 一 构造模拟数据 centers = [(0,0),(8,8),(-5,10),(10,-4)] # 样本中心 cluster_std = [1.2, 0.8, 2.0, 0.5] # 标准差 n_samples = [800,600,400,200] # 样本数量 X,y_true = make_blobs( n_samples=n_samples, centers=centers, cluster_std=cluster_std, random_state=42 ) # 二 用BIC在2~8个成分里面选最佳GMM(看看几个候选成分时解释力最好) n_componenrs_range = range(2,9) gmms=[GaussianMixture(n,random_state=42).fit(X) for n in n_componenrs_range] bics = [g.bic(X) for g in gmms] best_gmm = gmms[np.argmin(bics)] print(gmms.index(best_gmm)) # 三 聚类与评估 y_pred = best_gmm.predict(X) ari = adjusted_rand_score(y_true,y_pred) ami = adjusted_mutual_info_score(y_true,y_pred) sil = silhouette_score(X,y_pred) print(f\u0026#34;ARI: {ari:.3f} AMI: {ami:.3f} Silhouette: {sil:.3f}\u0026#34;) # data_visualization_only(X,y_true) data_visualization_only(X,y_pred) 2 ARI: 1.000 AMI: 1.000 Silhouette: 0.801 降维 降维方法： 特征提取：特征映射。把高维空间的数据映射到低维空间。比如PCA和基于神经网络的降维等 特征选择： 过滤式（打分机制）：通过某个阈值进行过滤。比如根据方差、信息增益、互信息过滤经常会看到但可能不会去用的信息 独立于任何机器学习算法。它基于数据本身的统计特征（如相关性、互信息）对特征进行评分和筛选，就像用一个“过滤器”把不好的特征过滤掉。 优点：快，计算开销小，不依赖具体模型。 缺点：可能忽略特征与模型的协同作用，选出的特征单独看很强，但组合起来对特定模型可能不是最优的。 互信息：$$\\displaystyle{I(X;Y)=\\sum_{x\\in X}\\sum_{y \\in Y}P(X=x,Y=y)\\log_2\\frac{P(X=x,Y=y)}{P(X=x)P(Y=y)}}$$ 包裹式：每次迭代产生一个特征子集，评分 将模型性能作为评价标准。它会尝试不同的特征子集，并用一个特定的机器学习模型去评估每个子集的性能（比如准确率）。 优点：针对性强，选出的特征子集对该模型通常性能最优。 缺点：非常慢，计算开销大，容易过拟合。 嵌入式：先通过机器学习模型训练来对每个特征提到一个权值。接下来和过滤式类似，通过设定某个阈值来筛选特征 特征选择的过程嵌入在模型训练过程之中。模型在训练的同时会自动进行特征选择。 优点：平衡了效率和效果，比过滤式更针对模型，比包裹式快很多。 缺点：依赖于具有内置特征选择机制的模型。 PCA降维 算法过程（对于样本矩阵 $X_{m×n}$，从 $n$ 维到 $k$ 维） 1、数据标准化（中心化）：计算每个特征的均值 $ X_{mean} $ ，另 $ X_c=X-X_{mean} $ 2、计算协方差矩阵：$$\\displaystyle{\\Sigma = \\frac{1}{m-1}X_c^TX_c}$$ 3、特征值分解（正交对角化）： $$ \\begin{aligned} \u0026\\Sigma=\\chi\\Lambda \\chi^T = \\begin{bmatrix} \\xi_1,\\xi_2 \\dots \\xi_n \\end{bmatrix} \\begin{bmatrix} \\lambda_1 \\\\ \u0026 \\lambda_2 \\\\ \u0026\u0026 \\ddots \\\\ \u0026\u0026\u0026 \\lambda_n \\end{bmatrix} \\begin{bmatrix} \\xi_1,\\xi_2 \\dots \\xi_n \\end{bmatrix}^{T}\\\\ \u0026 \\lambda_1 \\ge \\lambda_2 \\ge \\dots \\ge \\lambda_n \\end{aligned} $$ 4、选择主成分（$\\lambda_1,\\lambda_2\\dots \\lambda_k,k\u0026lt;n$） 保留信息量（方差贡献量）：$$\\displaystyle{\\eta_{k/n} = \\frac{\\displaystyle{\\sum_{i=1}^k\\lambda_i}}{\\displaystyle{\\sum_{i=1}^n\\lambda_i}}}$$ 截取投影矩阵： $$ \\chi_k = \\begin{bmatrix} \\xi_1,\\xi_2 \\dots \\xi_k \\end{bmatrix} $$ 5、投影（降维）： $Z=X_c·\\chi_k$ Kernel PCA KernelPCA首先将 $X_{m×n}$ 升维映射到 $K_{n×n}$ $K_{i,j} = F_k(X_i,X_j)$ ， $F_k$ 为核函数 特征值特征向量的求解：SVD分解 直接对 $X_c$ 进行SVD分解：$$X_c = U·\\Sigma·V^T$$ $ U $ 是一个 $ m×m $ 的正交矩阵，左奇异向量 $ \\Sigma $ 是一个 $ m×n $ 的对角矩阵，对角线上的值 $ \\sigma_i $ 是奇异值。 $$\\displaystyle{\\lambda_i=\\frac{\\sigma_i^2}{m-1}}$$ $ V $ 是一 $ n×n $ 的正交矩阵，右奇异向量。 $$V = \\chi $$ ","date":"2025-12-02T19:45:17+08:00","image":"http://localhost:1313/p/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/cover_hu_b61f99213a9abae8.png","permalink":"http://localhost:1313/p/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/","title":"机器学习-无监督学习"},{"content":" 目录 Logistic回归（逻辑回归） —— 二分类 将多分类问题转化为双分类问题 Softmax回归（归一化函数回归） —— 多分类 实践——音乐分类器 0、初始化 1、数据处理与特征存储 2、模型训练与存储 3、测试与评估 支持向量机（SVM，Support Vector Machine）与感知机（Perceptron） SMO算法 SVM概率化输出 SVM合页损失 线性分类（Linear Classification） 逻辑回归算法：在多元线性回归算法的基础上把结果缩放到0-1之间 线性分类器（GLM与分布的连接器，可以通过内部核函数升维变成非线性算法）：$$\\eta = \\theta^Tx = \\theta_0+\\theta_1x_1+……+\\theta_nx_n$$ 广义线性模型（GLM）：$\\displaystyle{p(y;\\eta)=b(y)e^{(\\eta^TT(y)-a(\\eta))}}$，其中$\\eta$是自然参数，$T(y)$是充分统计量，$a(\\eta)$是对数部分函数 Logistic回归（逻辑回归）：伯努利-GLM Softmax回归（归一化函数回归）：多项式分布-GLM Logistic回归（逻辑回归） —— 二分类 伯努利分布： $$ \\displaystyle{P(y;p) = p^y (1-p)^{1-y}, \\quad y=0,1 \\Rightarrow P(y;p)=e^{(ln(\\frac{p}{1-p})y+ln(1-p))}}\\displaystyle{\\Rightarrow \\eta=ln(\\frac{p}{1-p})\\Rightarrow p=\\frac{1}{1+e^{-\\eta}}\\Rightarrow p(x)=\\frac{1}{1+e^{-\\theta^Tx}} } $$ 逻辑回归函数（Sigmoid）： $$ \\displaystyle{\\hat y = h_\\theta(x)=g(\\theta^Tx)=\\frac{1}{1+e^{-\\theta^Tx}}} \\Rightarrow 分界:\\theta^Tx=0 $$。当 $\\hat y\u0026gt;0.5$ 时，$y$ 取1；当 $\\hat y\u0026lt;0.5$ 时，$y$ 取0 损失函数（由极大似然推导而来）： $$ \\displaystyle{J(\\theta)=-[\\sum^m_{i=1}y^{(i)}lnh(x^{(i)})+(1-y^{(i)})ln(1-h(x^{(i)})]} $$ 梯度下降： $$ \\displaystyle{\\theta_j^{(k+1)} = \\theta_j^{(k)}-\\alpha\\frac{\\partial}{\\partial_{\\theta_j^{(k)}}}J(\\theta^{(k)})} $$ $$ \\displaystyle{\\frac{\\partial}{\\partial_{\\theta_j^{(k)}}}J(\\theta^{(k)}) = \\frac{1}{n}\\sum^n_{i=1}(h_\\theta(x_i)-y_i)x^{(j)}_i} \\Rightarrow \\nabla_{\\theta^{(k+1)}}J(\\theta^{(k+1)}) = \\frac{1}{n}X^T(X\\theta^{(k)}-y) $$ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 # Sigmoid Function import numpy as np import math import matplotlib.pyplot as plt def sigmoid_function(x): return 1/(1+np.exp(-x)) x = np.linspace(-10,10,1000) y = sigmoid_function(x) plt.xlim(-10,10) plt.ylim(0,1) plt.plot(x,y) plt.plot([0,0],[0,sigmoid_function(0)],\u0026#39;k--\u0026#39;) plt.plot([-10,0],[0.5,sigmoid_function(0)],\u0026#39;k--\u0026#39;) plt.plot(0,sigmoid_function(0)) plt.plot(0,0.5,\u0026#39;ko\u0026#39;) plt.text(0.1, 0.5 + 0.001, \u0026#39;(0 , 0.5)\u0026#39;, fontsize=14, ha=\u0026#39;left\u0026#39;, va=\u0026#39;bottom\u0026#39;) plt.show() ​ ​\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 # 二分类实例：乳腺癌 from sklearn.datasets import load_breast_cancer from sklearn.linear_model import LogisticRegression from sklearn.preprocessing import StandardScaler from sklearn.model_selection import train_test_split from sklearn.pipeline import Pipeline from sklearn.metrics import accuracy_score import numpy as np import matplotlib.pyplot as plt # 载入参数 X,y = load_breast_cancer(return_X_y=True) # 划分测试集与训练集 X_train,X_test,y_train,y_test = train_test_split( X,y, test_size=0.3, random_state=20230428, stratify=y) # # 标准归一化 # scaler = StandardScaler() # X_train_scaled = scaler.fit_transform(X_train) # X_test_scaled = scaler.fit_transform(X_test) # # 模型训练 # lg_reg = LogisticRegression() # lg_reg.fit(X_train_scaled,y_train) # # 预测 # y_train_pre = lg_reg.predict(X_train_scaled) # y_test_pre = lg_reg.predict(X_test_scaled) # # 评估 # print(\u0026#34;Train:\u0026#34;, accuracy_score(y_train,y_train_pre)*100,\u0026#34;%\u0026#34;) # print(\u0026#34;Test:\u0026#34;,accuracy_score(y_test,y_test_pre)*100,\u0026#34;%\u0026#34;) # 归一化 —— 评估可用Pipeline实现 log_reg = Pipeline([ (\u0026#39;scaler\u0026#39;,StandardScaler()), (\u0026#39;clf\u0026#39;,LogisticRegression()) ]) log_reg.fit(X_train,y_train) print(\u0026#34;Test:\u0026#34;,log_reg.score(X_train,y_train)*100,\u0026#34;%\u0026#34;) print(\u0026#34;Train:\u0026#34;,log_reg.score(X_test,y_test)*100,\u0026#34;%\u0026#34;) Test: 99.49748743718592 % Train: 96.49122807017544 % 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 # 二分类实例：鸢尾花三分类 =\u0026gt; 二分类 from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression from sklearn.preprocessing import StandardScaler from sklearn.metrics import accuracy_score from sklearn.pipeline import Pipeline # 0:\u0026#39;setosa\u0026#39;, 1:\u0026#39;versicolor\u0026#39;, 2:\u0026#39;virginica\u0026#39; X,y = load_iris(return_X_y=True) y = (y==2).astype(int) # 二分化 X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3, random_state=200304) log_reg = Pipeline([ (\u0026#39;scaler\u0026#39;,StandardScaler()), (\u0026#39;clf\u0026#39;,LogisticRegression()) ]) log_reg.fit(X_train,y_train) print(\u0026#39;Train accuracy: {:.2f} %\u0026#39;.format(log_reg.score(X_train, y_train) * 100)) print(\u0026#39;Test accuracy: {:.2f} %\u0026#39;.format(log_reg.score(X_test, y_test) * 100)) Train accuracy: 97.14 % Test accuracy: 93.33 % 将多分类问题转化为双分类问题 One-vs-all（One-vs-rest）：以鸢尾花分类问题为例。将三分类问题转换为三个二分类问题（分别为是否为\u0026rsquo;setosa\u0026rsquo;，是否为\u0026rsquo;versicolor\u0026rsquo;以及是否为\u0026rsquo;virginica\u0026rsquo;）。得到三个预测值，取最大 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 # OneVsRestClassifier from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression from sklearn.multiclass import OneVsRestClassifier from sklearn.preprocessing import StandardScaler from sklearn.metrics import accuracy_score from sklearn.pipeline import Pipeline # 0:\u0026#39;setosa\u0026#39;, 1:\u0026#39;versicolor\u0026#39;, 2:\u0026#39;virginica\u0026#39; X,y = load_iris(return_X_y=True) X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3, random_state=200304) log_reg = Pipeline([ # (\u0026#39;scaler\u0026#39;,StandardScaler()), (\u0026#39;ovr\u0026#39;,OneVsRestClassifier( LogisticRegression() ) ) ]) log_reg.fit(X_train,y_train) print(\u0026#39;Train accuracy: {:.2f} %\u0026#39;.format(log_reg.score(X_train, y_train) * 100)) print(\u0026#39;Test accuracy: {:.2f} %\u0026#39;.format(log_reg.score(X_test, y_test) * 100)) Train accuracy: 93.33 % Test accuracy: 97.78 % 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 # 转化为三个二分类问题 from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression from sklearn.preprocessing import StandardScaler from sklearn.metrics import accuracy_score from sklearn.pipeline import Pipeline import numpy as np # 0:\u0026#39;setosa\u0026#39;, 1:\u0026#39;versicolor\u0026#39;, 2:\u0026#39;virginica\u0026#39; X,y = load_iris(return_X_y=True) temp = [] X_train,X_test,y_train_init,y_test = train_test_split(X,y,test_size=0.3, random_state=200304) for category in [0,1,2]: y_train = (y_train_init==category) log_reg = LogisticRegression() log_reg.fit(X_train,y_train) temp.append(log_reg.predict_proba(X_test)[:,1]) result = np.empty(len(temp[0])).reshape(-1,1) for i in range(len(temp)): result = np.c_[result,temp[i].reshape(-1,1)] result = result[:,1:] y_test_pre = np.argmax(result,axis=1) acc = (y_test_pre==y_test) print(\u0026#39;Test accuracy: {:.2f} %\u0026#39;.format( (acc.sum()/len(acc))* 100)) Test accuracy: 97.78 % Softmax回归（归一化函数回归） —— 多分类 多项式分布： $$\\displaystyle{P(X_1=x_1,\\dots,X_k=x_k;p_1,p_2,\\dots,p_k) = \\frac{n!}{x_1! \\cdots x_k!} p_1^{x_1} \\cdots p_k^{x_k}, \\quad \\sum_{i=1}^k x_i = n}\\displaystyle{\\Rightarrow P(y;\\varphi)=e^{\\displaystyle{\\sum^{k-1}_{i=1}T(y)_iln(\\frac{\\varphi_i}{\\varphi_k})+ln\\varphi_k}} = e^{\\displaystyle{\\eta^TT(y)-a(\\eta)}}}\\Rightarrow\\displaystyle{P_i = P({y^{(i)};\\Theta})=P_k e^{\\displaystyle{\\Theta^{(i)}x}}},\\displaystyle{P_k=\\frac{1}{{\\displaystyle{\\sum^{k}_{j=1}e^{\\displaystyle{\\Theta^{(j)}x}}}}}}$$， $\\displaystyle{\\Theta^{(i)}}$ 为 $\\Theta$ 的第 $i$ 行 Softmax回归函数：$$ \\displaystyle{P(y^{(i)}=k|x^{(i)};\\theta) = \\frac{\\displaystyle{e^{\\theta_k^Tx^{(i)}}}}{\\displaystyle{\\sum^k_{j=1}e^{\\theta_j^Tx^{(i)}}}}} $$，此时 $\\theta$ 是一个矩阵 损失函数——交叉熵损失函数（Cross-Entropy Loss，负对数似然损失）：$$ \\displaystyle{J(\\theta)=-\\frac{1}{N}\\sum^{N}_{n=1}\\sum^{k}_{i=1}y_i^{(n)}ln(\\hat y_i^{(n)})} $$，其中 $N$ 是样本数， $k$ 是分类数， $y_i^{(n)}$ 是真实标签，服从one-hot编码， $\\hat y_i^{(n)}$ 是预测概率，取值为 $(0,1) $ 当y的分类数为2时，Softmax回归退化为Logistc回归 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression from sklearn.preprocessing import StandardScaler from sklearn.metrics import accuracy_score from sklearn.pipeline import Pipeline X,y = load_iris(return_X_y=True) X_train,X_test,y_train,y_test = train_test_split( X,y, test_size=0.3, random_state=200304 ) pipe = Pipeline([ (\u0026#39;scaler\u0026#39;,StandardScaler()), (\u0026#39;clf\u0026#39;,LogisticRegression(# 默认Softmax solver=\u0026#39;sag\u0026#39;, max_iter=1000, )) ]) pipe.fit(X_train,y_train) pipe.score(X_test,y_test) 0.9777777777777777 实践——音乐分类器 数据集路径： \u0026ldquo;.\\Dataset\\2_Music_Classifier\u0026rdquo; 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 from scipy.fft import fft,fftfreq # 快速傅里叶转换 from scipy.io import wavfile # 读取wav文件格式 from matplotlib.pyplot import specgram as spg # 绘制频谱图 import matplotlib.pyplot as plt from itertools import product path = \u0026#34;./Dataset/2_Music_Classifier/\u0026#34; # wavfile返回样本率（每秒采样个数，单通道采样总数） # sample_rate,X = wavfile.read(\u0026#34;./Dataset/2_Music_Classifier/blues/converted/blues.00000.au.wav\u0026#34;) # 绘图查看 def frequence_time(mtype,mserial): sample_rate,X = wavfile.read(path+mtype+\u0026#34;/converted/\u0026#34;+mtype+\u0026#34;.\u0026#34;+mserial+\u0026#34;.au.wav\u0026#34;) plt.specgram(X,Fs=sample_rate,xextent=(0,30)) # plt.plot(fft(X,sample_rate)) plt.title(\u0026#34;frequence_time:\u0026#34;+ mtype + \u0026#34;-\u0026#34; + mserial[-1]) plt.ylabel(\u0026#39;frequence\u0026#39;) plt.xlabel(\u0026#39;time\u0026#39;) def magnitude_time(mtype,mserial): sample_rate,X = wavfile.read(path+mtype+\u0026#34;/converted/\u0026#34;+mtype+\u0026#34;.\u0026#34;+mserial+\u0026#34;.au.wav\u0026#34;) time = np.linspace(0,30,len(X)) plt.plot(time,X) plt.title(\u0026#34;magnitude_time:\u0026#34;+ mtype + \u0026#34;-\u0026#34; + mserial[-1]) plt.ylabel(\u0026#39;magnitude\u0026#39;) plt.xlabel(\u0026#39;time\u0026#39;) def magnitude_frequence(mtype,mserial): sample_rate,X = wavfile.read(path+mtype+\u0026#34;/converted/\u0026#34;+mtype+\u0026#34;.\u0026#34;+mserial+\u0026#34;.au.wav\u0026#34;) N = len(X) freqs = fftfreq(N, 1/sample_rate)[:N//2] mag = np.abs(fft(X))[:N//2] plt.plot(freqs,mag) plt.title(\u0026#34;magnitude_frequence:\u0026#34;+ mtype + \u0026#34;-\u0026#34; + mserial[-1]) plt.ylabel(\u0026#39;magnitude\u0026#39;) plt.xlabel(\u0026#39;frequence\u0026#39;) types = [\u0026#39;blues\u0026#39;,\u0026#39;classical\u0026#39;,\u0026#39;country\u0026#39;,\u0026#39;disco\u0026#39;,\u0026#39;hiphop\u0026#39;,\u0026#39;jazz\u0026#39;,\u0026#39;metal\u0026#39;,\u0026#39;pop\u0026#39;,\u0026#39;reggae\u0026#39;,\u0026#39;rock\u0026#39;] serials = [\u0026#39;00000\u0026#39;,\u0026#39;00001\u0026#39;,\u0026#39;00002\u0026#39;] types = types[:] serials = serials[:] plt.figure(num=None,figsize=(8*len(serials),5*len(types)),dpi=80) for index,(t,s) in enumerate(product(types,serials)): plt.subplot(len(types),len(serials),index+1) frequence_time(t,s) plt.show() plt.figure(num=None,figsize=(8*len(serials),5*len(types)),dpi=80) for index,(t,s) in enumerate(product(types,serials)): plt.subplot(len(types),len(serials),index+1) magnitude_time(t,s) plt.show() # plt.figure(num=None,figsize=(8*len(serials),5*len(types)),dpi=80) # for index,(t,s) in enumerate(product(types,serials)): # plt.subplot(len(types),len(serials),index+1) # magnitude_frequence(t,s) types = [types[i] for i in list(np.sort(np.random.choice(range(len(types)),size=5,replace=False)))] serials = serials[:1] plt.figure(num=None,figsize=(8*len(types),5*3),dpi=80) for index,(t,s) in enumerate(product(types,serials)): plt.subplot(3,len(types),0*len(types)+index+1);frequence_time(t,s) plt.subplot(3,len(types),1*len(types)+index+1);magnitude_time(t,s) plt.subplot(3,len(types),2*len(types)+index+1);magnitude_frequence(t,s) plt.show() ​ ​\n0、初始化 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 from scipy.fft import fft # 快速傅里叶转换 from scipy.io import wavfile # 读取wav文件格式 from matplotlib.pyplot import specgram as spg # 绘制频谱图 from itertools import product import matplotlib.pyplot as plt import numpy as np import os from sklearn.pipeline import Pipeline from sklearn.preprocessing import StandardScaler from sklearn.linear_model import LogisticRegression from sklearn.model_selection import train_test_split path = \u0026#34;./Dataset/2_Music_Classifier/\u0026#34; model_path = \u0026#34;D:/MLab/project/StuML/ML/Model/2_Music_Classifier/\u0026#34; max_len = 660000 labels = dict([(label_name,label_num) for label_num,label_name in enumerate(os.listdir(path))]) 1、数据处理与特征存储 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 y = [];X=[] for label_name in labels.keys(): sub_path = path+label_name+\u0026#34;/converted/\u0026#34; for wave_file_name in os.listdir(sub_path): y.append(labels[label_name]) X.append( abs( # 取幅值（实部和虚部的平方和开根号） fft( # 傅里叶变换 wavfile.read( # 读取 sub_path+wave_file_name )[1][:max_len] # 切片 ) ) ) X_train,X_test,y_train,y_test = train_test_split( X,y, test_size=0.1, random_state=200304, stratify=y ) # 特征存储 np.save(model_path+\u0026#34;train_feature\u0026#34;,X_train) np.save(model_path+\u0026#34;train_label\u0026#34;,y_train) np.save(model_path+\u0026#34;test_feature\u0026#34;,X_test) np.save(model_path+\u0026#34;test_label\u0026#34;,y_test) 2、模型训练与存储 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 X = np.load(model_path+\u0026#34;train_feature.npy\u0026#34;) y = np.load(model_path+\u0026#34;train_label.npy\u0026#34;) log_reg = Pipeline([ (\u0026#39;scaler\u0026#39;,StandardScaler()), (\u0026#39;clf\u0026#39;,LogisticRegression( max_iter=10000, # solver=\u0026#39;sag\u0026#39; )) ]) log_reg.fit(X,y) with open(model_path+\u0026#34;model_object\u0026#34;,\u0026#34;wb\u0026#34;) as file: pickle.dump(log_reg,file) 3、测试与评估 1 2 3 4 5 6 7 8 X_test = np.load(model_path+\u0026#34;test_feature.npy\u0026#34;) y_test = np.load(model_path+\u0026#34;test_label.npy\u0026#34;) with open(model_path+\u0026#34;model_object\u0026#34;,\u0026#34;rb\u0026#34;) as file: log_reg = pickle.load(file) log_reg.score(X_test,y_test) # log_reg.score(X_train,y_train) 0.6 支持向量机（SVM，Support Vector Machine）与感知机（Perceptron） SVM是一种二分类算法，是对Perceptron模型的扩展,具有更强的泛化能力 SVM/Perceptron模型： $$ y=\\operatorname{sign}(\\theta x)= \\begin{cases} +1, \u0026 \\theta x\u003e0 \\\\ -1, \u0026 \\ \\theta x\u003c0 \\end{cases} $$ 间隔：样本到超平面的距离 几何距离：$\\displaystyle{\\gamma = \\frac{|w^tx_0+b|}{||w||}}$，其中超平面为 $\\displaystyle{w^tx+b=0}$ 对于正确分类的样本点，有 $$\\displaystyle{\\gamma = \\frac{y_0(w^tx_0+b)}{\\sqrt{w^tw}} = \\frac{y_0(\\hat y_0+b)}{\\sqrt{w^tw}}}$$ 函数距离：$\\displaystyle{\\delta=|w^tx_0+b|}$ 对于正确分类的样本点，有 $$\\displaystyle{\\delta=|w^tx_0+b|=y_0(w^tx_0+b)}$$ 硬间隔：要求所有样本都必须正确分类，不允许任何错误 软间隔：允许部分样本分类错误，容忍一定程度的误差，以获得更好的泛化能力 支持向量：离分割超平面最近的那些样本点 Perceptron思想：期望使分类错误的所有样本（m条）到超平面的距离之和最小 $$\\displaystyle{J(\\theta)=\\sum_{i\\in M_\\theta}\\frac{-y^i(\\theta^T x^i+b)}{||\\theta||}}\\Rightarrow \\displaystyle{J(\\theta)=-\\sum_{i\\in M_\\theta}{y^i(\\theta^T x^i+b)}}$$，其中 $M_\\theta$ 为分类错误集 优化：随机梯度下降法（SGD）或小批量梯度下降法（MBGD） $$\\displaystyle{\\nabla_\\theta J(\\theta) = -\\sum_{i\\in M_\\theta}y^{(i)}x^{(i)}}$$$，注意，这里的梯度函数表达式内部虽然与 $\\theta$ 无关，但错误集 $M_\\theta$ 却与 $\\theta$ 相关 SVM思想：期望使离超平面比较近的点尽可能地远离超平面 —— 二次优化 线性可分支持向量机：硬间隔最大化（完美分类正负例且距离超平面最近的点离超平面越远越好） 损失函数：求解（拉格朗日条件约束）： $$ \\begin{aligned} \u0026 \\displaystyle{\\max\\limits_{w,b}\\ \\gamma_{min}=\\frac{y_{min}(w^Tx_{min}+b)}{||w||}}\\\\ s.t. \\quad \u0026\\displaystyle{{\\gamma}_{i} = \\frac{y_i(w^Tx_i+b)}{||w||}\\ge \\gamma_{min} ,i \\in \\{1,2,\\ldots m\\}} \\\\ \\end{aligned} $$ 令 $y_{min}(wx_{min}+b) = 1$，则原式等价于 （损失函数）： $$ \\begin{aligned} \u0026 \\displaystyle{\\min\\limits_{w,b}\\ \\frac{1}{2}{||w||^2}}\\\\ s.t. \\quad \u0026\\displaystyle{y_i(w^Tx_i+b)\\ge 1 ,i \\in \\{1,2,\\ldots m\\}} \\\\ \\end{aligned} $$ 二次规划： 构建拉格朗日函数： $$\\displaystyle{ L(w,b,\\alpha)=\\frac{1}{2}||w||^2-\\sum^m_{i=1}\\alpha_i[y_i(w^Tx_i+b)-1], \\alpha_i \\ge 0}$$ 求解 $$\\displaystyle{\\min\\limits_{w,b}\\max\\limits_{\\alpha_i\\ge 0}L(w,b,\\alpha) \\overset{\\text{对偶函数}}{\\Leftrightarrow} \\max\\limits_{\\alpha_i\\ge 0}\\min\\limits_{w,b}L(w,b,\\alpha)}$$ 原问题等价于（SMO算法推导）： $$ \\begin{aligned} \u0026 \\displaystyle{\\min\\limits_\\alpha \\frac{1}{2}\\sum^m_{i=1}\\sum^m_{j=1}\\alpha_i\\alpha_jy_iy_j(x_i·x_j)-\\sum^m_{i=1}\\alpha_i}\\\\ s.t. \\quad \u0026\\displaystyle{\\sum^m_{i=1}\\alpha_iy_i=0}\u0026,i\\in \\{1,2,\\ldots m\\} \\\\ \u0026\\displaystyle{ \\alpha_i \\ge 0} \u0026,i\\in \\{1,2,\\ldots m\\} \\end{aligned} $$ 求 $w^* \\text{与} b^*$ ： $$ \\begin{cases} \\displaystyle{w^* = \\sum^m_{i=1}{\\alpha^*}_iy_ix_i}\\\\ \\displaystyle{y_s({w^*}^Tx_s+b^*)=1,\\quad x_s为任意支持向量}\\\\ \\displaystyle{\\alpha^*(y_s({w^*}^Tx_s+b^*)-1)=0,\\quad 可获得支持向量} \\end{cases} $$ 预测函数：$$\\hat y = ({w^*}^T x+b^*)$$ 线性支持向量机（线性不可分）：软间隔最大化（在硬间隔的基础上引入松弛变量 $\\xi_i \\ge 0$ ） 损失函数：求解（拉格朗日条件约束）： $$ \\begin{aligned} \u0026 \\displaystyle{\\min\\limits_{w,b} \\frac{1}{2}{||w||^2} + C{\\sum^m_{i=1}\\xi_i}}\\\\ s.t. \\quad \u0026\\displaystyle{y_i(w^Tx_i+b)\\ge 1-\\xi_i} \u0026, i \\in \\{1,2,\\ldots m\\} \\\\ \u0026\\displaystyle{ \\xi_i \\ge 0}\u0026, i \\in \\{1,2,\\ldots m\\} \\end{aligned} $$ 二次规划： 构建拉格朗日函数： $$ \\displaystyle{ L(w,b,\\xi,\\alpha,\\mu)=\\frac{1}{2}||w||^2+C\\sum^m_{i=1}\\xi_i-\\sum^m_{i=1}\\alpha_i[y_i(w^Tx_i+b)-1+\\xi_i]-\\sum^m_i\\mu_i\\xi_i, \\alpha_i,\\xi_i \\ge 0} $$ 求解 $\\displaystyle{\\min\\limits_{w,b,\\xi}\\max\\limits_{\\alpha_i\\ge 0,\\xi_i\\ge 0}L(w,b,\\xi,\\alpha,\\mu) \\overset{\\text{对偶函数}}{\\Leftrightarrow} \\max\\limits_{\\alpha_i\\ge 0,\\xi_i\\ge 0}\\min\\limits_{w,b,\\xi}L(w,b,\\xi,\\alpha,\\mu)}$ 原问题等价于（SMO算法推导）： $$ \\begin{aligned} \u0026 \\displaystyle{\\min\\limits_\\alpha \\frac{1}{2}\\sum^m_{i=1}\\sum^m_{j=1}\\alpha_i\\alpha_jy_iy_j(x_i·x_j)-\\sum^m_{i=1}\\alpha_i}\\\\ s.t. \\quad \u0026\\displaystyle{\\sum^m_{i=1}\\alpha_iy_i=0}\u0026,i\\in \\{1,2,\\ldots m\\}\\\\ \u0026 0 \\le \\alpha_i \\le C \u0026 , i\\in \\{1,2,\\ldots m\\}\\\\ % \u0026 \\xi_i \\ge 0 \u0026 , i\\in \\{1,2,\\ldots m\\} \\end{aligned} $$ 对于支持样本 $x_i$: $\\alpha_i = 0$ : $x_i$ 被正确分类 $0 \u0026lt; \\alpha_i \u0026lt; C$ : $x_i$ 在软边界上 $\\alpha_i = C$ : $x_i$ 嵌入软边界内 $\\xi \u0026lt; 1$ : $x_i$ 被正确分类 $\\xi = 1$ : $x_i$ 在超平面上 $\\xi \u0026gt; 0$ : $x_i$ 被错误分类 求 $w^* \\text{与}b^*$ ： $$ \\begin{cases} \\displaystyle{w^* = \\sum^m_{i=1}{\\alpha}_i^*y_ix_i}\\\\ \\displaystyle{y_s({w^*}^Tx_s+b^*)=1,\\quad x_s为任意支持向量}\\\\ \\displaystyle{\\alpha^*(y_s({w^*}^Tx_s+b^*)-1)=0,\\quad 可获得支持向量} \\end{cases} $$ 非线性支持向量机：升维（核函数Kernel） 定义Kernel函数：$K(x,z) = \\phi(x)\\phi(z)$ 此时对偶处理后的目标函数为： $$ \\begin{aligned} \u0026 \\displaystyle{\\min\\limits_\\alpha \\frac{1}{2}\\sum^m_{i=1}\\sum^m_{j=1}\\alpha_i\\alpha_jy_iy_jK(x_i,x_j)-\\sum^m_{i=1}\\alpha_i}\\\\ s.t. \\quad \u0026\\displaystyle{\\sum^m_{i=1}\\alpha_iy_i=0}\u0026,i\\in \\{1,2,\\ldots m\\}\\\\ \u0026 0 \\le \\alpha_i \\le C \u0026 , i\\in \\{1,2,\\ldots m\\}\\\\ % \u0026 \\xi_i \\ge 0 \u0026 , i\\in \\{1,2,\\ldots m\\} \\end{aligned} $$ 常用Kernel函数： 线性Kernel函数： $K(x,z)=x \\cdot z$ 多项式Kernel函数： $K(x,z)=(\\gamma x \\cdot z + r)$ 高斯Kernel函数： $K(x,z)=\\exp(-\\gamma||x-z||^2)$ Sigmoid Kernel函数：$K(x,z)=\\tanh(\\gamma x \\cdot z + r)$ SMO算法 SVM算法关键步骤： 支持向量机算法最优化问题（指定惩罚系数C）： $$ \\begin{aligned} \u0026 \\displaystyle{J(\\alpha)=\\min\\limits_\\alpha \\frac{1}{2}\\sum^m_{i=1}\\sum^m_{j=1}\\alpha_i\\alpha_jy_iy_j(x_i·x_j)-\\sum^m_{i=1}\\alpha_i}\\\\ s.t. \\quad \u0026\\displaystyle{\\sum^m_{i=1}\\alpha_iy_i=0}\u0026,i\\in \\{1,2,\\ldots m\\}\\\\ \u0026 0 \\le \\alpha_i \\le C \u0026 , i\\in \\{1,2,\\ldots m\\}\\\\ % \u0026 \\xi_i \\ge 0 \u0026 , i\\in \\{1,2,\\ldots m\\} \\end{aligned} $$ 等价于求解 ： $$ \\begin{cases} \\displaystyle{w^* = \\sum^m_{i=1}{\\alpha}_i^*y_ix_i}\\\\ \\displaystyle{y_s({w^*}^Tx_s+b^*)=1,\\quad x_s为任意支持向量}\\\\ \\displaystyle{\\alpha^*(y_s({w^*}^Tx_s+b^*)-1)=0,\\quad 可获得支持向量} \\end{cases} $$ 预测函数： $\\displaystyle{f(x)=sign(\\sum^m_{i=1}\\alpha_i^y_iK(x,x_i)+b^)}$ SMO算法求解最优化问题 求解 $\\alpha_i, i\\in {1,2,\\dots m}$ 算法思想： 由于 $\\displaystyle{\\sum^m_{i=1}\\alpha_iy_i=0}$ ，故不能只固定一个参数 将原始求解m个参数二次规划问题分解成多个子二次规划问题分别求解，每个子问题只求解2个参数 采用启发式方法选择需要更新的两个参数 $\\alpha_i $ 和 $\\alpha_j$ 例如，选择 $\\alpha_1$ 和 $\\alpha_2$ 优化 $\\alpha_i $ 和 $\\alpha_j$ 使目标函数最大程度接近全局最优解（$\\nabla_{\\alpha_i,\\alpha_j}J(\\alpha)=0$），其余参数不变 $$\\displaystyle{\\min\\limits_{\\alpha_1,\\alpha_2}J(\\alpha_1,\\alpha_2)=\\min\\limits_{\\alpha_1,\\alpha_2}\\frac{1}{2}K_{11}\\alpha_1^2+\\frac{1}{2}K_{22}\\alpha_2^2+y_1y_2K_{12}\\alpha_1\\alpha_2-(\\alpha_1+\\alpha_2)+y_1v_1\\alpha_1+y_2v_2\\alpha_2+Constant}$$ ，其中 $$v_i = \\displaystyle{\\sum^m_{j=3}\\alpha_jy_jK(x_i,x_j), i = 1,2}$$ 将二元函数视为一元函数 另 $$\\alpha_1 = (\\zeta-y_2\\alpha_2)y_1$$ ，带回上式 $$\\displaystyle{\\min\\limits_{\\alpha_2}J(\\alpha_2)=\\min\\limits_{\\alpha_2}\\frac{1}{2}K_{11}(\\zeta-y_2\\alpha_2)y_1^2+\\frac{1}{2}K_{22}\\alpha_2^2+y_1y_2K_{12}(\\zeta-y_2\\alpha_2)y_1\\alpha_2-((\\zeta-y_2\\alpha_2)y_1+\\alpha_2)+y_1v_1(\\zeta-y_2\\alpha_2)y_1+y_2v_2\\alpha_2+Constant}$$ 对 $J(\\alpha_2)$ 求导，导数为0 $$ \\begin{aligned} \\displaystyle{v_1 = f(x_1)-\\sum^2_{j=1}y_j\\alpha_jK_{1j}-b} \\\\ \\displaystyle{v_2 = f(x_2)-\\sum^2_{j=1}y_j\\alpha_jK_{2j}-b} \\end{aligned} $$ $$\\displaystyle{\\alpha^{new,unclipped}_2 = \\alpha^{old}_2 + \\frac{y_2(E_1-E_2)}{\\eta}}$$，其中 $$\\alpha_2^{new,unclipped}$$ 为未考虑约束的新值， $$E_i = f(x_i)-y_i, \\ \\eta = K_{11}+K_{22}-2K_{12}$$ 对新值进行约束修建 $$ \\begin{aligned} \u0026 0 \\le \\alpha_1,\\alpha_2 \\le C \\\\ \u0026 \\alpha_1 + \\alpha_2 = \\zeta \\end{aligned} $$ 当 $y_1 \\neq y_2$ 时， $$ L=\\max(0,\\alpha^{old}_2-\\alpha^{old}_1),H=\\min(C,C+\\alpha^{old}_2-\\alpha^{old}_1) $$ 当 $y_1 \\neq y_2$ 时，$$L=\\max(0,\\alpha_1^{old}+\\alpha_2^{old}-C);H=\\min(C,\\alpha_2^{old}+\\alpha_1^{old})$$ $$ \\alpha_2^{\\text{new}} = \\begin{cases} H \u0026, \\alpha_2^{\\text{new,unclipped}} \u003e H \\\\ \\alpha_2^{\\text{new,unclipped}} \u0026, L \\le \\alpha_2^{\\text{new,unclipped}} \\le H \\\\ L \u0026, \\alpha_2^{\\text{new,unclipped}} \u003c L \\end{cases} $$ 启发式选择变量 第一个变量的选择称为外循环，首先遍历整个样本集，选择违反KKT条件的 $\\alpha_i$ 作为第一个变量。依据相关规则选择第二个变量，对这两个变量采用上述方法进行优化。直到遍历整个样本集后，没有违反KKT条件 $\\alpha_i$ ，然后退出 KKT条件： $$ \\begin{aligned} \u0026 \\alpha_i = 0 \u0026 \\Rightarrow \u0026 \\qquad y^{(i)}(w^Tx^{(i)}+b) \\ge 1 \\\\ \u0026 \\alpha_i = C \u0026 \\Rightarrow \u0026 \\qquad y^{(i)}(w^Tx^{(i)}+b) \\le 1 \\\\ \u0026 0 \u003c \\alpha_i \u003c C \u0026 \\Rightarrow \u0026 \\qquad y^{(i)}(w^Tx^{(i)}+b) = 1 \\\\ \\end{aligned} $$ 第二个变量的选择成为内循环，假设在外循环中找到第一个变量记为 $\\alpha_1$ ，第二个变量的选择希望能使 $\\alpha_2$ 有较大变化。由于 $\\alpha_2$ 是依赖于 $|E_1-E_2|$ ，当 $E_1$ 为正时，那么选择最小的 $E_i$ 作为 $E_2$ ；如果 $E_1$ 为负时，选择最大的 $E_i$ 作为 $E_2$ ，通常为每个样本的 $E_i$ 保存在一个列表中，选择最大的 $|E_1-E_2|$ 来近似最大化步长。 SVM概率化输出 目的：将决策值映射为概率（如 $P(y=1∣x)\\in[0,1]$），使用户能判断样本属于某类的可能性大小 标准的SVM的无阈值输出为：$f(x)=h(x)+b$ ，其中 $h(x)=\\displaystyle{\\sum_iy_i\\alpha_iK(x_i,x)}$ Platt利用 sigmoid-fitting 方法，将标准的SVM的输出结果进行后处理，转换为后验概率 $P(y=1|f)=\\displaystyle{\\frac{1}{1+\\exp(Af+B)}}$ ，其中 $A$ 和 $B$ 为待拟合参数，$f$ 为样本 $x$ 的无阈值输出。 SVM合页损失 0-1损失是二分类问题的真正损失函数，合页损失与logistic损失是对0-1损失函数的近似 合页损失函数(Hinge Loss Function)：$L(y(wx+b))=[1-y(wx+b)]+$ ， $[]+$ 表示： $$ [z]_+ = \\begin{cases} z\u0026,z\u003e0 \\\\ 0\u0026,z\\le 0 \\end{cases} $$ 也就是说，数据点如果被正确分类，损失为0，如果没有被正确分类，损失为z SVM的损失函数就是合页损失函数+正则化项: $$\\displaystyle{\\min\\limits_{w,b}\\sum^m_{i=1}L(y^{(i)}(w^Tx^{(i)}+b)) + \\lambda ||w||^2}$$ 上述目标函数中，当 $\\displaystyle{\\lambda = \\frac{1}{2C}}$ 时。等价于原目标函数 $$\\displaystyle{\\frac{1}{2}||w||^2+C\\sum^m_{i=1}\\xi_i}$$ 实践-人脸分类器 基于 sklearn 内置 LFW 数据集 + SVM 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 import numpy as np import matplotlib.pyplot as plt from sklearn.datasets import fetch_lfw_people from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.decomposition import PCA from sklearn.svm import SVC from sklearn.pipeline import Pipeline from sklearn.metrics import classification_report,accuracy_score # 1、加载人脸数据 lfw_people = fetch_lfw_people(min_faces_per_person=70,resize=0.4) X,y,names = (lfw_people.data,lfw_people.target,lfw_people.target_names) n_classes = names.shape[0] print(\u0026#34;数据集规模：\u0026#34;,X.shape) print(\u0026#34;类别数：\u0026#34;,n_classes) print(\u0026#34;类别名：\u0026#34;,names) print() # 2、划分数据集与训练集 X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.25,random_state=428,stratify=y) # 3、构建机器学习通道 pipe = Pipeline([ # 先降维，再归一，再SVM ( \u0026#39;pca\u0026#39;, PCA( n_components=150, # 只保留150个主成分（把原始11750维降到150维） whiten=True, # 白化：150维特征缩放到同尺度，去相关 random_state=428 ) ), ( \u0026#39;scaler\u0026#39;, StandardScaler() # 对PCA后的150维特征进行归一化 ), ( \u0026#39;svm\u0026#39;, SVC( kernel=\u0026#39;rbf\u0026#39;, # 使用高斯核函数进行非线性升维 class_weight=\u0026#39;balanced\u0026#39;, # 自动根据各类样本量给出权重，防止某些人照片多导致偏向性 C=5.0, # 正则强度，C越大越不允许误分，可能导致过拟合；C约小margin越大可能导致欠拟合 gamma=\u0026#39;scale\u0026#39; # 高斯核宽度gamma的取值策略 ) ) ]) # 4、训练模型 print(\u0026#34;开始训练 SVM ...\u0026#34;) pipe.fit(X_train,y_train) print(\u0026#34;训练完成\u0026#34;) print() # 5、评估模型 y_pred = pipe.predict(X_test) print(f\u0026#34;\\n测试集整体准确率：{pipe.score(X_test,y_test):.3f}\u0026#34;) print(\u0026#34;\\n详细分类报告：\u0026#34;) print(classification_report(y_test, y_pred, target_names=names)) 数据集规模： (1288, 1850) 类别数： 7 类别名： ['Ariel Sharon' 'Colin Powell' 'Donald Rumsfeld' 'George W Bush' 'Gerhard Schroeder' 'Hugo Chavez' 'Tony Blair'] 开始训练 SVM ... 训练完成 ​ ​ 测试集整体准确率：0.851 ​ ​ 详细分类报告： ​ precision recall f1-score support ​ Ariel Sharon 1.00 0.47 0.64 19 Colin Powell 0.84 0.90 0.87 59 Donald Rumsfeld 0.95 0.70 0.81 30 George W Bush 0.80 0.98 0.88 133 Gerhard Schroeder 0.95 0.78 0.86 27 Hugo Chavez 1.00 0.56 0.71 18 Tony Blair 0.91 0.83 0.87 36\naccuracy 0.85 322 macro avg 0.92 0.75 0.81 322 weighted avg 0.87 0.85 0.84 322 ​\n","date":"2025-12-02T18:51:05+08:00","image":"http://localhost:1313/p/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB/SVM_hu_e41ac7d8880d26ed.jpg","permalink":"http://localhost:1313/p/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB/","title":"机器学习-线性分类"},{"content":" 目录 一、正规方程（Normal Equation） 1、一维线性回归（代码实现） 2、高维线性回归（代码实现） 3、高维线性回归（sklearn库） 二、梯度下降（Gradient Descent） 1、批量梯度下降（GD） 代码实现 2、随机梯度下降（SGD） (1)、代码实现 (2)、小幅优化：打乱数据索引，顺序选取向量 3、小批量梯度下降（Mini-batch SGD） (1)、代码实现 (2)、小幅优化：打乱数据索引，顺序批量选取向量 4、SGD的sklearn库实现 5、梯度下降法的问题与解决思路 （1）、确定速率调整函数 （2）、更改学习速率 三、坐标下降（Coordinate Descent） 代码实现 四、归一化（Normalization） 五、正则化（Regularization） 1、Lasso回归（sklearn库） 2、Ridge回归（sklearn库） 3、ElasticNet回归（sklearn库） 4、随机梯度下降实现elasticnet正则化（sklearn库） 六、升维方法 — 多项式回归（Polynomial Regression） 七、实践 — 保险花销预测 1、数据提取 2、EDA（Exploratory Data Analysis，探索性数据分析） 3、特征工程 4、模型训练 5、模型评估 6、进阶 线性回归（Linear Regression） 任务类型分类：有监督学习 定义：拟合因变量(向量) y 与 x 之间呈线性关系，即 $\\mathbf{y} = \\mathbf{w}^T\\mathbf{x} + \\mathbf{b}$，其中 $\\mathbf{x,y,w,b \\in \\mathbb{R}^n}$ 目标：寻找参数最优解使得$\\operatorname{Loss}$函数最小。$(\\hat{\\mathbf{w}}, \\hat{\\mathbf{b}}) = \\operatorname{Loss}(\\mathbf{y}, \\hat{\\mathbf{y}}) \\Leftrightarrow\\operatorname{Argmin}(\\operatorname{Loss}(\\mathbf{w}, \\mathbf{b}))$ 多元线性回归常用损失函数 —— MSE(均方误差，由似然函数推导而来): $$ \\displaystyle{\\operatorname{\\mathit{J}(\\theta)} = \\frac{1}{2} \\displaystyle \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)})^2 = \\frac{1}{2}(X\\theta - y)^T(X\\theta - y) = \\frac{1}{2}(\\theta^TX^TX\\theta - \\theta^TX^Ty - y^TX\\theta + y^Ty)},系数可为\\displaystyle{\\frac{1}{2}、\\frac{1}{n}、\\frac{1}{2n}} $$ MSE对 $\\theta$ 的梯度: $$ \\displaystyle{\\frac{\\partial{J}(\\theta)}{\\partial\\theta} = X^TX\\theta - X^Ty = 0 \\Rightarrow \\theta = (X^TX)^{-1}X^Ty} $$ 一、正规方程（Normal Equation） 参数解析解： $\\hat{\\theta} = (X^TX)^{-1}X^Ty$ 可能存在的问题：$(X^TX)^{-1}$的求解可能比较困难 1、一维线性回归（代码实现） 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 import numpy as np import matplotlib.pyplot as plt np.random.seed(20030428) # 数据量 m = 100 # 随机生成 x 序列 x = np.random.rand(m,1) * 2 # 模拟 x 对应的 y 标签，误差呈正态分布 y = 5 + x*4 + np.random.randn(m,1) # 补充 x0 列 X = np.c_[np.ones((m,1)),x] # 参数的求解公式 w_hat = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y) # 预测 x_pre = [[0.5],[1.5]] X_pre = np.c_[np.ones((2,1)),x_pre] y_pre = X_pre.dot(w_hat) plt.scatter(x,y,label=\u0026#39;Original Data\u0026#39;) plt.scatter(x_pre,y_pre,label=\u0026#39;Predicted Data\u0026#39;) plt.plot(x,X.dot(w_hat),label=\u0026#39;Regression Model\u0026#39;) plt.legend() plt.show() 2、高维线性回归（代码实现） 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 import numpy as np np.random.seed(20030428) # 维度 n = 4 # 数据量 m = 1000 # 设置预测值 x_pre = np.random.random(size = (3,n))*10 # 随机生成 x 序列 x = np.random.rand(m,n) * 2 # 补充 x0 列 X = np.c_[np.ones((m,1)),x] # 模拟 x 对应的 y 标签，误差呈正态分布 w = np.random.randint(0,6,size=(n+1,1)) y = X.dot(w) + np.random.randn(m,1) # 参数的求解公式 w_hat = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y) print(\u0026#39;W_hat:\u0026#39;) print(w_hat.reshape(1,n+1)) # 预测 X_pre = np.c_[np.ones((3,1)),x_pre] y_pre = X_pre.dot(w_hat) print(\u0026#39;X_pre: \u0026#39;) print(x_pre) print(\u0026#39;Y_pre: \u0026#39;) print(y_pre) W_hat: [[5.02671056 3.98404888 4.00452019 4.9783491 1.02490084]] X_pre: [[0.03016482 4.79096225 5.74410225 4.2038564 ] [0.0650456 9.53496505 3.37288732 7.77475153] [2.68648726 5.44484467 6.07293252 8.00564199]] Y_pre: [[57.23707594] [68.2285754 ] [75.97196488]] 3、高维线性回归（sklearn库） 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 import numpy as np from sklearn.linear_model import LinearRegression np.random.seed(20030428) # 维度 n = 4 # 数据量 m = 1000 # 设置预测值 x_pre = np.random.random(size = (3,n))*10 # 随机生成 x 序列 x = np.random.rand(m,n) * 2 # 补充 x0 列 X = np.c_[np.ones((m,1)),x] # 模拟 x 对应的 y 标签，误差呈正态分布 w = np.random.randint(0,6,size=(n+1,1)) y = X.dot(w) + np.random.randn(m,1) reg = LinearRegression(fit_intercept=True) reg.fit(x,y) print(\u0026#39;W_hat:\u0026#39;) print(np.c_[reg.intercept_,reg.coef_]) # 预测 y_pre = reg.predict(x_pre) print(\u0026#39;X_pre: \u0026#39;) print(x_pre) print(\u0026#39;Y_pre: \u0026#39;) print(y_pre) W_hat: [[5.02671056 3.98404888 4.00452019 4.9783491 1.02490084]] X_pre: [[0.03016482 4.79096225 5.74410225 4.2038564 ] [0.0650456 9.53496505 3.37288732 7.77475153] [2.68648726 5.44484467 6.07293252 8.00564199]] Y_pre: [[57.23707594] [68.2285754 ] [75.97196488]] 二、梯度下降（Gradient Descent） 梯度下降法迭代公式：$\\hat\\theta_{k+1} = \\hat\\theta_k - \\alpha \\nabla_{\\theta}J(\\hat\\theta)$, $\\alpha$为学习率 学习率 $\\alpha$ ：$\\alpha$ 设置过大容易造成震荡，$\\alpha$ 设置太小容易造成迭代次数增加，也可能落到局部最优解。一般设置为0.1、0.01、0.001、0.0001 1、批量梯度下降（GD） $$\\displaystyle{\\nabla_{\\theta}J(\\theta) = \\frac{\\partial}{\\partial{\\theta}}J(\\theta) = \\displaystyle \\sum^{m}_{i=1}(h_\\theta(x^{(i)}) - y^{(i)})x^{(i)} = X^T(X\\theta-y)}$$ 代码实现 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 import numpy as np np.random.seed(20030428) # 维度 n = 4 # 数据量 m = 1000 # 设置预测值 x_pre = np.random.random(size = (3,n))*10 # 超参数 learning_rate = 0.0001 # 学习率 α n_iterations = 1000 # 迭代次数 # 随机生成 x 序列 x = np.random.rand(m,n) * 2 # 补充 x0 列 X = np.c_[np.ones((m,1)),x] # 模拟 x 对应的 y 标签，误差呈正态分布 w = np.random.randint(0,6,size=(n+1,1)) y = X.dot(w) + np.random.randn(m,1) # 初始化参数向量 w_init = np.random.randn(n+1,1) # 求梯度 for n_iteration in range(n_iterations): gradient = X.T.dot(X.dot(w_init)-y) w_init = w_init - learning_rate*gradient print(\u0026#39;W_hat:\u0026#39;) print(w_init.reshape(1,n+1)) # 预测 y_pre = np.c_[np.ones((3,1)),x_pre].dot(w_init) print(\u0026#39;X_pre: \u0026#39;) print(x_pre) print(\u0026#39;Y_pre: \u0026#39;) print(y_pre) W_hat: [[5.02631227 3.98414324 4.00461388 4.97843832 1.02499615]] X_pre: [[0.03016482 4.79096225 5.74410225 4.2038564 ] [0.0650456 9.53496505 3.37288732 7.77475153] [2.68648726 5.44484467 6.07293252 8.00564199]] Y_pre: [[57.23804251] [68.23011847] [75.97363505]] 2、随机梯度下降（SGD） $$ \\displaystyle {\\nabla_{\\theta}J(\\theta) = \\frac{\\partial}{\\partial\\theta}J(\\theta) = (h_\\theta(x^{(i)})-y^{(i)})x^{(i)} = ({x^{(i)}})^T(x^{(i)}\\theta-y^{(i)}) , i \\sim \\text{Uniform}\\bigl(\\{1,2,\\dots,m\\}\\bigr)} $$ (1)、代码实现 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 import numpy as np np.random.seed(20030428) # 维度 n = 4 # 数据量 m = 1000 # 设置预测值 x_pre = np.random.random(size = (3,n))*10 # 超参数 learning_rate = 0.0001 # 学习率 α n_iterations = 1000 # 迭代次数 # 随机生成 x 序列 x = np.random.rand(m,n) * 2 # 补充 x0 列 X = np.c_[np.ones((m,1)),x] # 模拟 x 对应的 y 标签，误差呈正态分布 w = np.random.randint(0,6,size=(n+1,1)) y = X.dot(w) + np.random.randn(m,1) # 初始化参数向量 w_init = np.random.randn(n+1,1) # 求梯度 for n_iteration in range(n_iterations): for _ in range(m): random_index = np.random.randint(m) x_i = X[random_index:random_index+1,:] y_i = y[random_index:random_index+1,:] gradient = x_i.T.dot(x_i.dot(w_init)-y_i) w_init = w_init - learning_rate*gradient print(\u0026#39;W_hat:\u0026#39;) print(w_init.reshape(1,n+1)) # 预测 y_pre = np.c_[np.ones((3,1)),x_pre].dot(w_init) print(\u0026#39;X_pre: \u0026#39;) print(x_pre) print(\u0026#39;Y_pre: \u0026#39;) print(y_pre) W_hat: [[5.01922912 3.97831998 4.00264292 4.96386168 1.01662312]] X_pre: [[0.03016482 4.79096225 5.74410225 4.2038564 ] [0.0650456 9.53496505 3.37288732 7.77475153] [2.68648726 5.44484467 6.07293252 8.00564199]] Y_pre: [[57.1024122 ] [68.08959999] [75.78462184]] (2)、小幅优化：打乱数据索引，顺序选取向量 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 import numpy as np np.random.seed(20030428) # 维度 n = 4 # 数据量 m = 1000 # 设置预测值 x_pre = np.random.random(size = (3,n))*10 # 超参数 learning_rate = 0.0001 # 学习率 α n_iterations = 1000 # 迭代次数 # 随机生成 x 序列 x = np.random.rand(m,n) * 2 # 补充 x0 列 X = np.c_[np.ones((m,1)),x] # 模拟 x 对应的 y 标签，误差呈正态分布 w = np.random.randint(0,6,size=(n+1,1)) y = X.dot(w) + np.random.randn(m,1) # 初始化参数向量 w_init = np.random.randn(n+1,1) # 求梯度 for n_iteration in range(n_iterations): # 打乱顺序，顺序读取 index = np.arange(m) np.random.shuffle(index) X = X[index] y = y[index] for i in range(m): x_i = X[i:i+1,:] y_i = y[i:i+1,:] gradient = x_i.T.dot(x_i.dot(w_init)-y_i) w_init = w_init - learning_rate*gradient print(\u0026#39;W_hat:\u0026#39;) print(w_init.reshape(1,n+1)) # 预测 y_pre = np.c_[np.ones((3,1)),x_pre].dot(w_init) print(\u0026#39;X_pre: \u0026#39;) print(x_pre) print(\u0026#39;Y_pre: \u0026#39;) print(y_pre) W_hat: [[5.02675441 3.98450041 4.00505391 4.9789357 1.02541193]] X_pre: [[0.03016482 4.79096225 5.74410225 4.2038564 ] [0.0650456 9.53496505 3.37288732 7.77475153] [2.68648726 5.44484467 6.07293252 8.00564199]] Y_pre: [[57.24520848] [68.23968977] [75.98378179]] 3、小批量梯度下降（Mini-batch SGD） $$ \\displaystyle{\\nabla_{\\theta}J(\\theta) = \\frac{\\partial}{\\partial{\\theta}}J(\\theta) = \\displaystyle \\sum_{i\\in S}(h_\\theta(x^{(i)}) - y^{(i)})x^{(i)} = ({x^{(S)}})^T(x^{(S)}\\theta-y^{(S)}) , |S|=\\text{batch\\_size} \u003c m,\\; S\\subseteq \\{1,2,\\dots ,m\\},\\; S\\text{均匀随机选取}} $$ (1)、代码实现 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 import numpy as np np.random.seed(20030428) x_pre = np.random.random(size = (3,n))*10 # 维度 n = 4 # 数据量 m = 1000 # 分批大小 batch_size = 10 # 批次数 num_batches = int(m/batch_size) # 超参数 learning_rate = 0.0001 # 学习率 α n_iterations = 1000 # 迭代次数 # 随机生成 x 序列 x = np.random.rand(m,n) * 2 # 补充 x0 列 X = np.c_[np.ones((m,1)),x] # 模拟 x 对应的 y 标签，误差呈正态分布 w = np.random.randint(0,6,size=(n+1,1)) y = X.dot(w) + np.random.randn(m,1) # 初始化参数向量 w_init = np.random.randn(n+1,1) # 求梯度 for n_iteration in range(n_iterations): for _ in range(num_batches): random_index = np.random.randint(m) x_s = X[random_index:random_index+batch_size,:] y_s = y[random_index:random_index+batch_size,:] gradient = x_s.T.dot(x_s.dot(w_init)-y_s) w_init = w_init - learning_rate*gradient print(\u0026#39;W_hat:\u0026#39;) print(w_init.reshape(1,n+1)) # 预测 y_pre = np.c_[np.ones((3,1)),x_pre].dot(w_init) print(\u0026#39;X_pre: \u0026#39;) print(x_pre) print(\u0026#39;Y_pre: \u0026#39;) print(y_pre) W_hat: [[5.04016211 3.9927612 4.01168955 4.96684374 1.00728303]] X_pre: [[0.03016482 4.79096225 5.74410225 4.2038564 ] [0.0650456 9.53496505 3.37288732 7.77475153] [2.68648726 5.44484467 6.07293252 8.00564199]] Y_pre: [[57.14498768] [68.13517282] [75.83694482]] (2)、小幅优化：打乱数据索引，顺序批量选取向量 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 import numpy as np np.random.seed(20030428) x_pre = np.random.random(size = (3,n))*10 # 维度 n = 4 # 数据量 m = 1000 # 分批大小 batch_size = 10 # 批次数 num_batches = int(m/batch_size) # 超参数 learning_rate = 0.0001 # 学习率 α n_iterations = 1000 # 迭代次数 # 随机生成 x 序列 x = np.random.rand(m,n) * 2 # 补充 x0 列 X = np.c_[np.ones((m,1)),x] # 模拟 x 对应的 y 标签，误差呈正态分布 w = np.random.randint(0,6,size=(n+1,1)) y = X.dot(w) + np.random.randn(m,1) # 初始化参数向量 w_init = np.random.randn(n+1,1) # 求梯度 for n_iteration in range(n_iterations): # 打乱顺序，顺序读取 index = np.arange(m) np.random.shuffle(index) X = X[index] y = y[index] for i in range(num_batches): x_s = X[i*batch_size:i*batch_size+batch_size,:] y_s = y[i*batch_size:i*batch_size+batch_size,:] gradient = x_s.T.dot(x_s.dot(w_init)-y_s) w_init = w_init - learning_rate*gradient print(\u0026#39;W_hat:\u0026#39;) print(w_init.reshape(1,n+1)) # 预测 y_pre = np.c_[np.ones((3,1)),x_pre].dot(w_init) print(\u0026#39;X_pre: \u0026#39;) print(x_pre) print(\u0026#39;Y_pre: \u0026#39;) print(y_pre) W_hat: [[5.0267601 3.98450244 4.00504673 4.978934 1.02541437]] X_pre: [[0.03016482 4.79096225 5.74410225 4.2038564 ] [0.0650456 9.53496505 3.37288732 7.77475153] [2.68648726 5.44484467 6.07293252 8.00564199]] Y_pre: [[57.24518034] [68.23964038] [75.98376306]] 4、SGD的sklearn库实现 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 import numpy as np from sklearn.linear_model import SGDRegressor np.random.seed(20030428) x_pre = np.random.random(size = (3,n))*10 # 维度 n = 4 # 数据量 m = 1000 # 超参数 learning_rate = 0.001 # 学习率 α n_iterations = 10000 # 迭代轮次 # 随机生成 x 序列 x = np.random.rand(m,n) * 2 # 补充 x0 列 X = np.c_[np.ones((m,1)),x] # 模拟 x 对应的 y 标签，误差呈正态分布 w = np.random.randint(0,6,size=(n+1,1)) y = X.dot(w) + np.random.randn(m,1) sgd_reg = SGDRegressor( loss=\u0026#39;squared_error\u0026#39;, # MSE penalty = None, # 无正则项 alpha = 0, # 无正则强度 learning_rate = \u0026#39;constant\u0026#39;, # 学习率 eta0 = learning_rate, # 固定学习率 fit_intercept = True, # 增加偏置列 max_iter = n_iterations * m, # 迭代次数 warm_start = False, # 不进行手动迭代，直接迭代完成 random_state = 20030428, # 打乱样本的随机种子 ) sgd_reg.fit(x,y.flatten()) print(\u0026#39;W_hat:\u0026#39;) print(np.c_[sgd_reg.intercept_,sgd_reg.coef_.reshape(1,-1)]) # 预测 y_pre = sgd_reg.predict(x_pre).reshape(-1,1) print(\u0026#39;X_pre: \u0026#39;) print(x_pre) print(\u0026#39;Y_pre: \u0026#39;) print(y_pre) W_hat: [[4.78413833 4.04362443 4.06423645 5.03733553 1.08352435]] X_pre: [[0.03016482 4.79096225 5.74410225 4.2038564 ] [0.0650456 9.53496505 3.37288732 7.77475153] [2.68648726 5.44484467 6.07293252 8.00564199]] Y_pre: [[57.86766804] [69.21400854] [77.04212681]] 5、梯度下降法的问题与解决思路 学习速率调整（学习速率调度，Learning rate schedules）：该方法试图在每次更新参数的过程中，改变学习速率。一般使用某种事先设定的策略或者在每次迭代中衰减一个较小的阈值 在稀疏特征数据中：很少出现的特征应该使用一个相对较大的学习速率 对于非凸目标函数：可能落入鞍点或平滑点 （1）、确定速率调整函数 1 2 3 4 5 6 7 8 9 10 import numpy as np import matplotlib.pyplot as plt t0,t1 = 5,50000 def learning_rate_schedule(alpha): return t0/(alpha * 1000 + t1) x_show = np.linspace(1,1000,1000) y_show = np.array([learning_rate_schedule(x_show[i]) for i in range(len(x_show))]) plt.plot(x_show,y_show) [\u0026lt;matplotlib.lines.Line2D at 0x1a768afe390\u0026gt;] （2）、更改学习速率 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 import numpy as np np.random.seed(20030428) # 维度 n = 4 # 数据量 m = 1000 # 设置预测值 x_pre = np.random.random(size = (3,n))*10 # 超参数 # learning_rate = 0.0001 # 学习率 α n_iterations = 1000 # 迭代次数 # 随机生成 x 序列 x = np.random.rand(m,n) * 2 # 补充 x0 列 X = np.c_[np.ones((m,1)),x] # 模拟 x 对应的 y 标签，误差呈正态分布 w = np.random.randint(0,6,size=(n+1,1)) y = X.dot(w) + np.random.randn(m,1) # 初始化参数向量 w_init = np.random.randn(n+1,1) # 求梯度 for n_iteration in range(n_iterations): gradient = X.T.dot(X.dot(w_init)-y) w_init = w_init - learning_rate_schedule(n_iteration)*gradient print(\u0026#39;W_hat:\u0026#39;) print(w_init.reshape(1,n+1)) # 预测 y_pre = np.c_[np.ones((3,1)),x_pre].dot(w_init) print(\u0026#39;X_pre: \u0026#39;) print(x_pre) print(\u0026#39;Y_pre: \u0026#39;) print(y_pre) W_hat: [[4.97242598 3.99644548 4.01333635 4.98719143 1.04554164]] X_pre: [[0.03016482 4.79096225 5.74410225 4.2038564 ] [0.0650456 9.53496505 3.37288732 7.77475153] [2.68648726 5.44484467 6.07293252 8.00564199]] Y_pre: [[57.36296538] [68.4494602 ] [76.21792798]] 三、坐标下降（Coordinate Descent） 坐标下降：一次走一步，每次只动一个变量。将高维问题拆成一系列一维问题，逐个坐标迭代更新，直到收敛 核心思想：固定其他所有变量，只沿第i个坐标方向做一维最优化，循环往复 迭代公式：$\\displaystyle{\\theta_i^{(k+1)} = \\arg\\min_{\\theta_i}Loss(\\theta_1^{(k+1)}, \\dots, \\theta_{i-1}^{(k+1)}, \\theta_i, \\theta_{i+1}^{(k)}, \\dots, \\theta_n^{(k)})}$，其中 $Loss$ 为目标损失函数，$\\theta^{(k)}$ 为第 $k$ 次迭代时的完整参数向量，$\\theta^{(k)}_i$ 为第 $k$ 次迭代时向量 $\\theta$ 的第 $i$ 个分量 对于MES的参数迭代公式（由MSE推导而来）： $$ \\displaystyle{\\theta_j = \\frac{\\displaystyle{ \\sum^m_{i=1}x^{(i)}_j(y^{(i)}-\\sum_{k\\neq j}\\theta_kx^{(i)}_k)}}{\\displaystyle{\\sum^m_{i=1}(x_j^{(i)})^2}}}\\Rightarrow \\theta^{(k+1)}_j = \\theta^{(k)}_j - \\displaystyle{\\frac{1}{X_{·j}^TX_{·j}}X_{·j}^T(X\\theta-y)} $$，其中 $m$ 为样本数量，$\\theta_j$ 为 $\\theta$ 的第 $j$ 个分量，$x^{(i)}_j$ 为第 $i$ 条样本的第 $j$ 个分量 代码实现 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 import numpy as np np.random.seed(20030428) # 维度 n = 4 # 数据量 m = 1000 # 设置预测值 x_pre = np.random.random(size = (3,n))*10 # 超参数 n_iterations = 1000 # 迭代次数 # 随机生成 x 序列 x = np.random.rand(m,n) * 2 # 补充 x0 列 X = np.c_[np.ones((m,1)),x] # 模拟 x 对应的 y 标签，误差呈正态分布 w = np.random.randint(0,6,size=(n+1,1)) y = X.dot(w) + np.random.randn(m,1) # 初始化参数向量 theta = np.zeros(shape=(n+1,1)) for _ in range(n_iterations): for j in range(theta.shape[0]): theta[j] = theta[j] - (1/(X[:,j]@X[:,j]))*X[:,j]@(X@theta - y) print(\u0026#39;W_hat:\u0026#39;) print(theta) # 预测 y_pre = np.c_[np.ones((3,1)),x_pre].dot(theta) print(\u0026#39;X_pre: \u0026#39;) print(x_pre) print(\u0026#39;Y_pre: \u0026#39;) print(y_pre) W_hat: [[5.02671056] [3.98404888] [4.00452019] [4.9783491 ] [1.02490084]] X_pre: [[0.03016482 4.79096225 5.74410225 4.2038564 ] [0.0650456 9.53496505 3.37288732 7.77475153] [2.68648726 5.44484467 6.07293252 8.00564199]] Y_pre: [[57.23707594] [68.2285754 ] [75.97196488]] 四、归一化（Normalization） 归一化是将不同尺度、纲量和分布的数据缩放到同一标准区间的预处理技术 核心目的：消除纲量影响、加速模型收敛、提升数值稳定性，同时保留原始信息的相对关系 特征级归一化方法 最大值最小值归一化（Min-Max）：受离心值影响较大 对第j个特征值做归一化：$$\\displaystyle{x^{*}_{i,j} = \\frac{x_{i,j}-x_j^{\\text{min}}}{x_j^{\\text{max}}-x_j^{\\text{min}}}}$$ 对整个数据集做归一化：$\\displaystyle{x^* = \\frac{x-min(x)}{max(x)-min(x)}}$ 标准归一化（Z-score）： 对第j个特征值做归一化： $$ \\displaystyle{x^*_{i,j} = \\frac{x_{i,j} - \\mu_j}{\\sigma_j}} $$ ，其中 $$ \\displaystyle{\\mu_j=\\frac{1}{n}\\sum_{i=1}^n x_{i,j},\\quad \\sigma_j=\\sqrt{\\frac{1}{n} \\sum_{i=1}^n (x_{i,j}-\\mu_j)^2}} $$ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 import numpy as np from sklearn.preprocessing import MinMaxScaler from sklearn.preprocessing import StandardScaler np.random.seed(20030428) # 数据维度 n = 4 # 数据规模 m = 100 # 初始化数据 x = np.empty((m, 2)) x[:, 0] = np.random.normal(1, 2, m) x[:, 1] = np.random.random(m) * 10-3 plt.scatter(x[:,0],x[:,1],label=\u0026#39;original\u0026#39;) # Min-Max归一化 minmax_scaler = MinMaxScaler() x_min_max_normalization = minmax_scaler.fit_transform(x) plt.scatter(x_min_max_normalization[:,0],x_min_max_normalization[:,1],label=\u0026#39;min-max\u0026#39;) minmax_scaler.fit(x) # Z-score归一化 standar_scaler = StandardScaler() x_standar_scaler = standar_scaler.fit_transform(x) plt.scatter(x_standar_scaler[:,0],x_standar_scaler[:,1],label=\u0026#39;z-score\u0026#39;) standar_scaler.fit(x) print(\u0026#39;μ =\u0026#39;, standar_scaler.mean_) print(\u0026#39;σ² =\u0026#39;, standar_scaler.var_) plt.legend() plt.show() μ = [1.07725633 2.44897092] σ² = [3.86482916 7.93336293] 五、正则化（Regularization） 过拟合与欠拟合 欠拟合（underfit）：未拟合到位，训练集和测试集准确率未达到最高 过拟合（overfit）：拟合过度，训练集准确率升高的同时，测试集的准确率反而降低 适度拟合（just right）：过拟合前，训练集和测试集准确率都达到最高时刻 正则化：防止过拟合，增加模型鲁棒性（Robust） 鲁棒性调优：使模型具有更好的鲁棒性，让模型的的泛化能力和推广能力更加强大 正则化本质：牺牲模型在训练集上的正确率以提高模型的推广能力，参数 w 在数值上越小越好，进而抵抗数值扰动。但 w 的数值不能极小，故而将原来的损失函数加上一个惩罚项 惩罚项（正则项）： L1 正则项： $$ \\displaystyle{L_1(w) = \\sum^m_{i=0}|w_i|} $$（曼哈顿距离）， $$ \\displaystyle{\\frac{\\partial}{\\partial w_i}L_1(w_i) = sign(w_i) = \\pm 1} $$ L2 正则项： $$ \\displaystyle{L_2(w) = \\sum^m_{i=0}|w_i|^2} $$（欧式距离的平方）， $$ \\displaystyle{\\frac{\\partial}{\\partial w_i}L_2(w_i) = 2w_i} $$ 正则化后的多元线性回归的损失函数 Lasso 回归（套索回归，稀疏性）： 损失函数 $$ J_{lasso}(\\theta)=MSE(\\theta)+L_1(\\theta) $$ 梯度 $$ \\displaystyle{\\nabla_{\\theta}J_{Lasso}(\\theta)=\\frac{\\partial}{\\partial \\theta}J(\\theta)+\\frac{\\partial}{\\partial \\theta}L_1(\\theta)=X^T(X\\theta-y)+\\lambda sign(\\theta)} $$ Ridge 回归（岭回归，平滑性）： 损失函数 $$ J_{ridge}(\\theta)=MSE(\\theta)+L_2(\\theta) $$ 梯度 $$ \\displaystyle{\\nabla_{\\theta}J_{Ridge}(\\theta)=\\frac{\\partial}{\\partial \\theta}J(\\theta)+\\frac{\\partial}{\\partial \\theta}L_2(\\theta)=X^T(X\\theta-y)+2\\lambda \\theta} $$ ElasticNet回归（弹性网络回归）： 损失函数 $$ J_{lasso}(\\theta)=MSE(\\theta)+L_1(\\theta)+L_2(\\theta) $$ 梯度 $$\\displaystyle{\\nabla_{\\theta}J_{Lasso}(\\theta)=\\frac{\\partial}{\\partial \\theta}J(\\theta)+\\frac{\\partial}{\\partial \\theta}L_1(\\theta)+\\frac{\\partial}{\\partial \\theta}L_2(\\theta)=X^T(X\\theta-y)+\\lambda[(1-r)sign(\\theta)+2r\\theta)]} $$ 注：Lasso 回归与 Ridge 回归只定义了 Loss 函数模型，可以用梯度下降法、坐标下降法、最小角回归（LARS）、正规方程等方法进行求解。在 sklearn 库中 Lasso 和 ElasticNet 使用坐标下降；Ridge 使用正规方程；SVGRegressor 使用随机梯度下降，可自定义正则化类型、正则化强度、学习率、阈值、迭代次数等等 1、Lasso回归（sklearn库） 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 import numpy as np from sklearn.linear_model import Lasso np.random.seed(20030428) # 维度 n = 4 # 数据量 m = 1000 # 设置预测值 x_pre = np.random.random(size = (3,n))*10 # 随机生成 x 序列 x = np.random.rand(m,n) * 2 # 补充 x0 列 X = np.c_[np.ones((m,1)),x] # 模拟 x 对应的 y 标签，误差呈正态分布 w = np.random.randint(0,6,size=(n+1,1)) y = X.dot(w) + np.random.randn(m,1) lasso_reg = Lasso(alpha=0.0015,max_iter=30000000) lasso_reg.fit(x,y) print(\u0026#39;W_hat:\u0026#39;) print(np.c_[lasso_reg.intercept_,lasso_reg.coef_.reshape(1,-1)]) # 预测 y_pre = lasso_reg.predict(x_pre) print(\u0026#39;X_pre: \u0026#39;) print(x_pre) print(\u0026#39;Y_pre: \u0026#39;) print(y_pre.reshape(-1,1)) 1 2 3 4 5 6 7 8 9 10 W_hat: [[5.0464025 3.97908982 3.99958499 4.97364032 1.01995191]] X_pre: [[0.03016482 4.79096225 5.74410225 4.2038564 ] [0.0650456 9.53496505 3.37288732 7.77475153] [2.68648726 5.44484467 6.07293252 8.00564199]] Y_pre: [[57.18512159] [68.14652889] [75.88324745]] 2、Ridge回归（sklearn库） 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 import numpy as np from sklearn.linear_model import Ridge np.random.seed(20030428) # 维度 n = 4 # 数据量 m = 1000 # 设置预测值 x_pre = np.random.random(size = (3,n))*10 # 随机生成 x 序列 x = np.random.rand(m,n) * 2 # 补充 x0 列 X = np.c_[np.ones((m,1)),x] # 模拟 x 对应的 y 标签，误差呈正态分布 w = np.random.randint(0,6,size=(n+1,1)) y = X.dot(w) + np.random.randn(m,1) ridge_reg = Ridge(alpha=0.4,solver=\u0026#39;sag\u0026#39;) ridge_reg.fit(x,y) print(\u0026#39;W_hat:\u0026#39;) print(np.c_[ridge_reg.intercept_,ridge_reg.coef_.reshape(1,-1)]) # 预测 y_pre = ridge_reg.predict(x_pre) print(\u0026#39;X_pre: \u0026#39;) print(x_pre) print(\u0026#39;Y_pre: \u0026#39;) print(y_pre.reshape(-1,1)) W_hat: [[5.04469365 3.97896047 3.99922338 4.97248436 1.02326193]] X_pre: [[0.03016482 4.79096225 5.74410225 4.2038564 ] [0.0650456 9.53496505 3.37288732 7.77475153] [2.68648726 5.44484467 6.07293252 8.00564199]] Y_pre: [[57.18895127] [68.16319935] [75.89870099]] 3、ElasticNet回归（sklearn库） 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 import numpy as np from sklearn.linear_model import ElasticNet np.random.seed(20030428) # 维度 n = 4 # 数据量 m = 1000 # 设置预测值 x_pre = np.random.random(size = (3,n))*10 # 随机生成 x 序列 x = np.random.rand(m,n) * 2 # 补充 x0 列 X = np.c_[np.ones((m,1)),x] # 模拟 x 对应的 y 标签，误差呈正态分布 w = np.random.randint(0,6,size=(n+1,1)) y = X.dot(w) + np.random.randn(m,1) elasticnet_reg = ElasticNet(alpha=0.01,l1_ratio=0.2) elasticnet_reg.fit(x,y) print(\u0026#39;W_hat:\u0026#39;) print(np.c_[elasticnet_reg.intercept_,elasticnet_reg.coef_.reshape(1,-1)]) # 预测 y_pre = elasticnet_reg.predict(x_pre) print(\u0026#39;X_pre: \u0026#39;) print(x_pre) print(\u0026#39;Y_pre: \u0026#39;) print(y_pre.reshape(-1,1)) W_hat: [[5.40784751 3.87770911 3.89554975 4.85588961 0.98362001]] X_pre: [[0.03016482 4.79096225 5.74410225 4.2038564 ] [0.0650456 9.53496505 3.37288732 7.77475153] [2.68648726 5.44484467 6.07293252 8.00564199]] Y_pre: [[56.21597338] [66.82977581] [74.39992651]] 4、随机梯度下降实现elasticnet正则化（sklearn库） 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 import numpy as np from sklearn.linear_model import SGDRegressor np.random.seed(20030428) # 维度 n = 4 # 数据量 m = 1000 # 设置预测值 x_pre = np.random.random(size = (3,n))*10 # 随机生成 x 序列 x = np.random.rand(m,n) * 2 # 补充 x0 列 X = np.c_[np.ones((m,1)),x] # 模拟 x 对应的 y 标签，误差呈正态分布 w = np.random.randint(0,6,size=(n+1,1)) y = X.dot(w) + np.random.randn(m,1) sgd_reg = SGDRegressor( penalty=\u0026#39;elasticnet\u0026#39;, # 正则化类型 max_iter = 100000, # 迭代次数 alpha = 0.01, # 正则化强度 λ l1_ratio=0.2, # l1正则化比例 tol = 1e-10, # 收敛阈值 # …… ) sgd_reg.fit(x,y.ravel()) print(\u0026#39;W_hat:\u0026#39;) print(np.c_[sgd_reg.intercept_,sgd_reg.coef_.reshape(1,-1)]) # 预测 y_pre = sgd_reg.predict(x_pre) print(\u0026#39;X_pre: \u0026#39;) print(x_pre) print(\u0026#39;Y_pre: \u0026#39;) print(y_pre.reshape(-1,1)) W_hat: [[5.1320958 3.93664843 3.95394899 4.913876 1.04281603]] X_pre: [[0.03016482 4.79096225 5.74410225 4.2038564 ] [0.0650456 9.53496505 3.37288732 7.77475153] [2.68648726 5.44484467 6.07293252 8.00564199]] Y_pre: [[56.80371943] [67.77050846] [75.42653888]] 六、升维方法 — 多项式回归（Polynomial Regression） 目的：解决欠拟合问题 常见手段：将已知维度进行相乘来构建新的维度，将非线性Data转换为线性Data 以二阶多项式升维为例： $$ y=w_0+w_1x_1+w_2x_2\\Rightarrow y^*=w_0+w_1x_1+w_2x_2+w_3x_1^2+w_4x_2^2+w_5x_1x_2 $$ 以下面拟合过程为例：$D_3$ 达到最佳拟合。随着维度的增加，$D_3$ 以下训练集和测试集误差均减小，属于欠拟合；$D_3$ 以上训练集误差减小，测试集误差增大，属于过拟合 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 import numpy as np import matplotlib.pyplot as plt from sklearn.preprocessing import PolynomialFeatures from sklearn.linear_model import LinearRegression from sklearn.metrics import mean_squared_error np.random.seed(20030428) plt.xlim(-21, 51) # 横轴 plt.ylim(-8000, 4000) # 纵轴 # 预设函数 def mapping(x): return 0.2*x**3 - 10*x**2 + 5*x - 1 # 维度 n = 1 x_show = np.linspace(-20,50,10000) plt.plot(x_show,mapping(x_show),color=\u0026#39;C0\u0026#39;,label=\u0026#39;scheduled model\u0026#39;) # 训练集数据量 train_size= 10000 # 测试集数据量 test_size = 10000 # 随机生成训练集 x_train = np.random.rand(m,n) * 60 - 20 y_train = mapping(x_train) + np.random.randn(m,1)*1111 plt.scatter(x_train,y_train,color=\u0026#39;C1\u0026#39;,s=5,label=\u0026#39;train set data\u0026#39;) # 随机生成测试集 x_test = np.random.rand(m,n) * 400 - 100 y_test = mapping(x_test) + np.random.randn(m,1)*1111 plt.scatter(x_test,y_test,color=\u0026#39;C2\u0026#39;,s=5,label=\u0026#39;train set data\u0026#39;) ### 升维预测 ### # dimensions = {2:\u0026#39;C3\u0026#39;} dimensions = {1:\u0026#39;C8\u0026#39;,2:\u0026#39;C3\u0026#39;,3:\u0026#39;C4\u0026#39;,4:\u0026#39;C5\u0026#39;,5:\u0026#39;C9\u0026#39;,6:\u0026#39;C7\u0026#39;,7:\u0026#39;C6\u0026#39;,8:\u0026#39;C10\u0026#39;} for dim,color in dimensions.items(): # 多项式升维 poly_features = PolynomialFeatures( degree=dim, # 设置维度 include_bias=True # 设置截距 ) # 训练集和测试集升维 x_train_poly = poly_features.fit_transform(x_train) x_test_poly = poly_features.fit_transform(x_test) # 线性回归 lin_reg = LinearRegression( fit_intercept=False # 不加偏置 ) lin_reg.fit(x_train_poly,y_train) y_train_pre = lin_reg.predict(x_train_poly) y_test_pre = lin_reg.predict(x_test_poly) train_norm = mean_squared_error(y_train,y_train_pre) test_norm = mean_squared_error(y_test,y_test_pre) lab = f\u0026#34;D:{dim}/Train:{train_norm/1e6:.5f}/Test:{test_norm/1e6:.5f}\u0026#34; x_show_poly = poly_features.fit_transform(x_show.reshape(-1,1)) plt.plot(x_show,lin_reg.predict(x_show_poly),color=dimensions[dim],label=lab) plt.legend( loc=\u0026#39;upper left\u0026#39;, # 以legend左上角为基准点 bbox_to_anchor=(1.02, 1), # 距离原点的相对位置，图像为0-1 borderaxespad=0, # 间距值 ncol=1, # 分几列 ) \u0026lt;matplotlib.legend.Legend at 0x1a768d8d150\u0026gt; 七、实践 — 保险花销预测 数据集路径： \u0026ldquo;.\\Dataset\\1_Insurance_Expense_Forecast\\insurance.csv\u0026rdquo; 1、数据提取 1 2 3 4 5 6 7 import pandas as pd import numpy as np data = pd.read_csv( \u0026#39;./Dataset/1_Insurance_Expense_Forecast/insurance.csv\u0026#39;, sep=\u0026#39;,\u0026#39;) data.head(6) age sex bmi children smoker region charges 0 19 female 27.900 0 yes southwest 16884.92400 1 18 male 33.770 1 no southeast 1725.55230 2 28 male 33.000 3 no southeast 4449.46200 3 33 male 22.705 0 no northwest 21984.47061 4 32 male 28.880 0 no northwest 3866.85520 5 31 female 25.740 0 no southeast 3756.62160 2、EDA（Exploratory Data Analysis，探索性数据分析） 本质：在把数据喂给算法之前，先用人眼和统计工具观察和清洗数据 目的：发现数据长什么样、哪里脏、哪里怪、哪里藏着有用的信号，从而为后续的特征工程、模型选择、甚至业务决策提供直觉和依据 发现charges出现右偏问题，通过特征工程中数值变换的取对数手段进行处理 1 2 3 4 # 观察数据结构信息 print(data.info()) # 结果显示数据质量较好，无缺失值 print(data.describe()) \u0026lt;class 'pandas.core.frame.DataFrame'\u0026gt; RangeIndex: 1338 entries, 0 to 1337 Data columns (total 7 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 age 1338 non-null int64 1 sex 1338 non-null object 2 bmi 1338 non-null float64 3 children 1338 non-null int64 4 smoker 1338 non-null object 5 region 1338 non-null object 6 charges 1338 non-null float64 dtypes: float64(2), int64(2), object(3) memory usage: 73.3+ KB None age bmi children charges count 1338.000000 1338.000000 1338.000000 1338.000000 mean 39.207025 30.663397 1.094918 13270.422265 std 14.049960 6.098187 1.205493 12110.011237 min 18.000000 15.960000 0.000000 1121.873900 25% 27.000000 26.296250 0.000000 4740.287150 50% 39.000000 30.400000 1.000000 9382.033000 75% 51.000000 34.693750 2.000000 16639.912515 max 64.000000 53.130000 5.000000 63770.428010 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 # 观察数据分布 import matplotlib.pyplot as plt # %matplotlib inline plt.rcParams[\u0026#39;font.family\u0026#39;] = \u0026#39;sans-serif\u0026#39; # 1. 启用 sans-serif 列表 plt.rcParams[\u0026#39;font.sans-serif\u0026#39;] = [\u0026#39;SimHei\u0026#39;] # 2. 把 SimHei 放在最前 plt.rcParams[\u0026#39;axes.unicode_minus\u0026#39;] = False # 3. 让负号正常显示（可选） plt.figure(figsize=(10, 8)) # 设置画布大小 plt.subplot(2, 2, 1) # 第一个子图 plt.hist(data[\u0026#39;age\u0026#39;], bins=20, color=\u0026#39;lightgreen\u0026#39;, edgecolor=\u0026#39;k\u0026#39;) plt.xlabel(\u0026#39;Ages\u0026#39;) plt.ylabel(\u0026#39;Count\u0026#39;) plt.title(\u0026#39;Distribution of Age\u0026#39;) plt.subplot(2, 2, 2) # 第二个子图 plt.hist(data[\u0026#39;bmi\u0026#39;], bins=25, color=\u0026#39;salmon\u0026#39;, edgecolor=\u0026#39;k\u0026#39;) plt.xlabel(\u0026#39;BMI\u0026#39;) plt.ylabel(\u0026#39;Count\u0026#39;) plt.title(\u0026#39;Distribution of BMI\u0026#39;) plt.subplot(2, 2, 3) # 第三个子图 plt.hist(data[\u0026#39;children\u0026#39;], bins=6, color=\u0026#39;gold\u0026#39;, edgecolor=\u0026#39;k\u0026#39;) plt.xlabel(\u0026#39;Children\u0026#39;) plt.ylabel(\u0026#39;Count\u0026#39;) plt.title(\u0026#39;Distribution of Children\u0026#39;) ax_charges=plt.subplot(2, 2, 4) # 第四个子图 plt.hist(data[\u0026#39;charges\u0026#39;], bins=30, color=\u0026#39;skyblue\u0026#39;, edgecolor=\u0026#39;k\u0026#39;) plt.xlabel(\u0026#39;Charges\u0026#39;) plt.ylabel(\u0026#39;Count\u0026#39;) plt.title(\u0026#39;Distribution of Charges\u0026#39;) ax_charges.text( 0.95,0.95, # 百分比坐标 \u0026#39;右偏（positively skewed / right-skewed）\\n指分布的尾部向右（值大的一侧）拖得很长\\n均值 \u0026gt; 中位数\u0026#39;, transform=ax_charges.transAxes, # 用子图的坐标系 va=\u0026#39;top\u0026#39;,ha=\u0026#39;right\u0026#39;, # 文本框基准点选取 fontsize = 10, bbox=dict( boxstyle=\u0026#39;round,pad=0.3\u0026#39;, facecolor=(0.9,0.9,0.9), alpha=0.5, ) ) plt.tight_layout() # 自动调整间距 plt.show() 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 # 压缩优化异常标签值 ax = plt.subplot(1, 1, 1) ax.text( 0.55,0.95, # 百分比坐标 \u0026#39;通过ln函数将原右偏拖尾压缩，\\n使结果更接近正态\u0026#39;, transform=ax.transAxes, # 用子图的坐标系 va=\u0026#39;top\u0026#39;,ha=\u0026#39;left\u0026#39;, # 文本框基准点选取 fontsize = 10, bbox=dict( boxstyle=\u0026#39;round,pad=0.3\u0026#39;, facecolor=(0.9,0.9,0.9), alpha=0.5, ) ) plt.hist(np.log1p(data[\u0026#39;charges\u0026#39;]),bins=20, color=\u0026#39;skyblue\u0026#39;, edgecolor=\u0026#39;k\u0026#39;) plt.title(\u0026#39;log of \\\u0026#39;charges\\\u0026#39;\u0026#39;) plt.show() # 增加标签值压缩列 data[\u0026#39;log_charges\u0026#39;]=np.log1p(data[\u0026#39;charges\u0026#39;]) data.head(6) age sex bmi children smoker region charges log_charges 0 19 female 27.900 0 yes southwest 16884.92400 9.734236 1 18 male 33.770 1 no southeast 1725.55230 7.453882 2 28 male 33.000 3 no southeast 4449.46200 8.400763 3 33 male 22.705 0 no northwest 21984.47061 9.998137 4 32 male 28.880 0 no northwest 3866.85520 8.260455 5 31 female 25.740 0 no southeast 3756.62160 8.231541 3、特征工程 类别：缺失值处理/数值变换/类别编码/高维稀疏/非线性交叉/时序\u0026amp;序列/业务先验 类别编码 — One-Hot编码：对某个特征从“分类”到“向量”的变化过程，例如： 若简单地将“男”/“女”，编码为0/1，模型会误以为“女”\u0026gt;“男”，把颜色当成连续量，学到错误的序关系 将“男”/“女”编码为二维向量 $(0,1)$ 和 $(1,0)$ 将本项目中的region特征中的southeast、southwest、northeast、northwest编码为$(1,0,0,0)，(0,1,0,0)，(0,0,1,0)，(0,0,0,1)$ 1 2 3 # 进行 One-Hot编码 data = pd.get_dummies(data,dtype=int) data.head(6) age bmi children charges log_charges sex_female sex_male smoker_no smoker_yes region_northeast region_northwest region_southeast region_southwest 0 19 27.900 0 16884.92400 9.734236 1 0 0 1 0 0 0 1 1 18 33.770 1 1725.55230 7.453882 0 1 1 0 0 0 1 0 2 28 33.000 3 4449.46200 8.400763 0 1 1 0 0 0 1 0 3 33 22.705 0 21984.47061 9.998137 0 1 1 0 0 1 0 0 4 32 28.880 0 3866.85520 8.260455 0 1 1 0 0 1 0 0 5 31 25.740 0 3756.62160 8.231541 1 0 1 0 0 0 1 0 1 2 3 4 5 6 7 8 9 # 取出样本及标签值 x = data.drop(columns=[\u0026#39;charges\u0026#39;,\u0026#39;log_charges\u0026#39;]) y = data[\u0026#39;log_charges\u0026#39;] # 填充空值 x.fillna(0,inplace=True) y.fillna(0,inplace=True) x.head() age bmi children sex_female sex_male smoker_no smoker_yes region_northeast region_northwest region_southeast region_southwest 0 19 27.900 0 1 0 0 1 0 0 0 1 1 18 33.770 1 0 1 1 0 0 0 1 0 2 28 33.000 3 0 1 1 0 0 0 1 0 3 33 22.705 0 0 1 1 0 0 1 0 0 4 32 28.880 0 0 1 1 0 0 1 0 0 1 2 3 4 5 6 # 划分训练集与测试集 from sklearn.model_selection import train_test_split x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3) print(x.shape,y.shape) print(x_train.shape,y_train.shape) print(x_test.shape,y_test.shape) (1338, 11) (1338,) (936, 11) (936,) (402, 11) (402,) 1 2 3 4 5 6 7 8 9 # 归一化 from sklearn.preprocessing import StandardScaler x_scaler = StandardScaler( copy=True, # 非原地更改 with_mean=True, # 先减去平均值 with_std=True, # 除以标准差 ).fit(x_train) # 用测试集数据训练出均值和标准差 x_train_scaled = x_scaler.transform(x_train) x_test_scaled = x_scaler.transform(x_test) 1 2 3 4 5 6 7 8 # 多项式升维以拟合非线性特征 from sklearn.preprocessing import PolynomialFeatures poly_features = PolynomialFeatures( degree = 1, include_bias=False ) x_train_scaled = poly_features.fit_transform(x_train_scaled) x_test_scaled = poly_features.fit_transform(x_test_scaled) 4、模型训练 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 from sklearn.linear_model import LinearRegression from sklearn.linear_model import Ridge from sklearn.linear_model import SGDRegressor # 线性回归 lin_reg = LinearRegression() lin_reg.fit(x_train_scaled,y_train) y_train_predict_lin = lin_reg.predict(x_train_scaled) y_test_predict_lin = lin_reg.predict(x_test_scaled) # 岭回归 rid_reg = Ridge() rid_reg.fit(x_train_scaled,y_train) y_train_predict_rid = rid_reg.predict(x_train_scaled) y_test_predict_rid = rid_reg.predict(x_test_scaled) # 随机梯度下降 sgd_reg = SGDRegressor() sgd_reg.fit(x_train_scaled,y_train) y_train_predict_sgd = sgd_reg.predict(x_train_scaled) y_test_predict_sgd = sgd_reg.predict(x_test_scaled) 5、模型评估 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 from sklearn.metrics import mean_squared_error print( \u0026#34;LinearRegression(Train / Test): \u0026#34;, np.sqrt(mean_squared_error(y_train,y_train_predict_lin)), np.sqrt(mean_squared_error(y_test,y_test_predict_lin)) ) print( \u0026#34;RidgeRegression(Train / Test): \u0026#34;, np.sqrt(mean_squared_error(y_train,y_train_predict_rid)), np.sqrt(mean_squared_error(y_test,y_test_predict_rid)) ) print( \u0026#34;SGDRegression(Train / Test): \u0026#34;, np.sqrt(mean_squared_error(y_train,y_train_predict_sgd)), np.sqrt(mean_squared_error(y_test,y_test_predict_sgd)) ) LinearRegression(Train / Test): 0.44905273756975034 0.43211955383967054 RidgeRegression(Train / Test): 0.44905315842354915 0.4321996372029468 SGDRegression(Train / Test): 0.449286159412961 0.43308751362633735 6、进阶 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns from sklearn.preprocessing import PolynomialFeatures from sklearn.model_selection import train_test_split from sklearn.linear_model import LinearRegression from sklearn.linear_model import Ridge from sklearn.ensemble import GradientBoostingRegressor from sklearn.metrics import mean_squared_error from sklearn.preprocessing import StandardScaler # 数据读取 data = pd.read_csv(\u0026#39;./Dataset/1_Insurance_Expense_Forecast/insurance.csv\u0026#39;) # EDA # 如果对于某个特征对预测值几乎无影响，则可忽略（降噪） data[\u0026#39;charges\u0026#39;] = np.log1p(data[\u0026#39;charges\u0026#39;]) print(data.head()) print(data.info()) print(data.describe()) ax = plt.subplot(2,2,1) plt.title(\u0026#39;sex\u0026#39;) sns.kdeplot(data.loc[data.sex==\u0026#39;male\u0026#39;,\u0026#39;charges\u0026#39;], fill=True, label=\u0026#39;male\u0026#39;, ax=ax) sns.kdeplot(data.loc[data.sex==\u0026#39;female\u0026#39;,\u0026#39;charges\u0026#39;], fill=True, label=\u0026#39;female\u0026#39;, ax=ax) plt.legend() ax = plt.subplot(2,2,2) plt.title(\u0026#39;smoke\u0026#39;) sns.kdeplot(data.loc[data.smoker==\u0026#39;yes\u0026#39;,\u0026#39;charges\u0026#39;], fill=True, label=\u0026#39;smoker_yes\u0026#39;, ax=ax) sns.kdeplot(data.loc[data.smoker==\u0026#39;no\u0026#39;,\u0026#39;charges\u0026#39;], fill=True, label=\u0026#39;smoker_no\u0026#39;, ax=ax) plt.legend() ax = plt.subplot(2,2,3) plt.title(\u0026#39;region\u0026#39;) sns.kdeplot(data.loc[data.region==\u0026#39;southeast\u0026#39;,\u0026#39;charges\u0026#39;], fill=True,label=\u0026#39;southeast\u0026#39;,ax=ax) sns.kdeplot(data.loc[data.region==\u0026#39;southwest\u0026#39;,\u0026#39;charges\u0026#39;], fill=True,label=\u0026#39;southwest\u0026#39;,ax=ax) sns.kdeplot(data.loc[data.region==\u0026#39;northeast\u0026#39;,\u0026#39;charges\u0026#39;], fill=True,label=\u0026#39;northeast\u0026#39;,ax=ax) sns.kdeplot(data.loc[data.region==\u0026#39;northwest\u0026#39;,\u0026#39;charges\u0026#39;], fill=True,label=\u0026#39;northwest\u0026#39;,ax=ax) plt.legend() ax = plt.subplot(2,2,4) plt.title(\u0026#39;children\u0026#39;) sns.kdeplot(data.loc[data.children==0,\u0026#39;charges\u0026#39;], fill=True, label=\u0026#39;children_0\u0026#39;, ax=ax) sns.kdeplot(data.loc[data.children==1,\u0026#39;charges\u0026#39;], fill=True, label=\u0026#39;children_1\u0026#39;, ax=ax) sns.kdeplot(data.loc[data.children==2,\u0026#39;charges\u0026#39;], fill=True, label=\u0026#39;children_2\u0026#39;, ax=ax) sns.kdeplot(data.loc[data.children==3,\u0026#39;charges\u0026#39;], fill=True, label=\u0026#39;children_3\u0026#39;, ax=ax) sns.kdeplot(data.loc[data.children==4,\u0026#39;charges\u0026#39;], fill=True, label=\u0026#39;children_4\u0026#39;, ax=ax) sns.kdeplot(data.loc[data.children==5,\u0026#39;charges\u0026#39;], fill=True, label=\u0026#39;children_5\u0026#39;, ax=ax) plt.legend() plt.tight_layout() plt.show() # 特征工程 data = data.drop([\u0026#39;region\u0026#39;,\u0026#39;sex\u0026#39;],axis=1) # 删除无用特征 # 特征离散化 def discretization(df,bmi=30,child=0): df[\u0026#39;bmi\u0026#39;]=\u0026#39;over\u0026#39; if df[\u0026#39;bmi\u0026#39;]\u0026gt;=bmi else \u0026#39;under\u0026#39; df[\u0026#39;children\u0026#39;] = \u0026#39;no\u0026#39; if df[\u0026#39;children\u0026#39;] == child else \u0026#39;yes\u0026#39; return df data = data.apply(discretization,axis=1,args=(30,0)) print(data.head()) # one-hot 编码 data = pd.get_dummies(data) # 解决右偏问题 print(data.head()) # 样本获取与空值填充 x = data.drop(\u0026#39;charges\u0026#39;,axis=1) y = data[\u0026#39;charges\u0026#39;] x.fillna(0,inplace=True) y.fillna(0,inplace=True) print(x.head()) print(y.head()) # 模型训练 # 划分 x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3) # # 归一化 # x_scaler = StandardScaler().fit(x_train) # x_train = x_scaler.transform(x_train) # 升维 poly_featrues = PolynomialFeatures(degree=2,include_bias=False) x_train_poly = poly_features.fit_transform(x_train) # 训练 reg_lin = LinearRegression() reg_lin.fit(x_train_poly,y_train) reg_rid = Ridge() reg_rid.fit(x_train_poly,y_train) reg_gra = GradientBoostingRegressor() reg_gra.fit(x_train_poly,y_train) # 测试集预测 # # 归一化 # x_test = x_scaler.transform(x_test) # 升维 x_test_poly = poly_features.fit_transform(x_test) # 预测 y_test_pre_lin = reg_lin.predict(x_test_poly) y_test_pre_rid = reg_rid.predict(x_test_poly) y_test_pre_gra = reg_gra.predict(x_test_poly) # 模型评估 print( \u0026#34;LinearRegression: \u0026#34;, np.sqrt(mean_squared_error(y_test,y_test_pre_lin)) ) print( \u0026#34;RidgeRegression: \u0026#34;, np.sqrt(mean_squared_error(y_test,y_test_pre_rid)) ) print( \u0026#34;GradientBoostingRegressor: \u0026#34;, np.sqrt(mean_squared_error(y_test,y_test_pre_gra)) ) age sex bmi children smoker region charges 0 19 female 27.900 0 yes southwest 9.734236 1 18 male 33.770 1 no southeast 7.453882 2 28 male 33.000 3 no southeast 8.400763 3 33 male 22.705 0 no northwest 9.998137 4 32 male 28.880 0 no northwest 8.260455 \u0026lt;class 'pandas.core.frame.DataFrame'\u0026gt; RangeIndex: 1338 entries, 0 to 1337 Data columns (total 7 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 age 1338 non-null int64 1 sex 1338 non-null object 2 bmi 1338 non-null float64 3 children 1338 non-null int64 4 smoker 1338 non-null object 5 region 1338 non-null object 6 charges 1338 non-null float64 dtypes: float64(2), int64(2), object(3) memory usage: 73.3+ KB None age bmi children charges count 1338.000000 1338.000000 1338.000000 1338.000000 mean 39.207025 30.663397 1.094918 9.098828 std 14.049960 6.098187 1.205493 0.919379 min 18.000000 15.960000 0.000000 7.023647 25% 27.000000 26.296250 0.000000 8.464064 50% 39.000000 30.400000 1.000000 9.146658 75% 51.000000 34.693750 2.000000 9.719618 max 64.000000 53.130000 5.000000 11.063061 age bmi children smoker charges 0 19 under no yes 9.734236 1 18 over yes no 7.453882 2 28 over yes no 8.400763 3 33 under no no 9.998137 4 32 under no no 8.260455 age charges bmi_over bmi_under children_no children_yes smoker_no \\ 0 19 9.734236 False True True False False 1 18 7.453882 True False False True True 2 28 8.400763 True False False True True 3 33 9.998137 False True True False True 4 32 8.260455 False True True False True smoker_yes 0 True 1 False 2 False 3 False 4 False age bmi_over bmi_under children_no children_yes smoker_no smoker_yes 0 19 False True True False False True 1 18 True False False True True False 2 28 True False False True True False 3 33 False True True False True False 4 32 False True True False True False 0 9.734236 1 7.453882 2 8.400763 3 9.998137 4 8.260455 Name: charges, dtype: float64 LinearRegression: 0.41171540130927264 RidgeRegression: 0.4119283802418486 GradientBoostingRegressor: 0.3163566171772987 ","date":"2025-12-02T13:16:53+08:00","image":"http://localhost:1313/p/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/output_45_1_hu_59b9da1fd30f562f.jpg","permalink":"http://localhost:1313/p/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/","title":"机器学习-线性回归"}]