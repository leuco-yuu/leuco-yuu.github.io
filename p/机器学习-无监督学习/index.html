<!doctype html><html lang=zh_CN dir=ltr><head><meta charset=utf-8><meta name=viewport content='width=device-width,initial-scale=1'><meta name=description content="【3】聚类是一种无监督的机器学习任务，它可以自动将数据划分成类（Cluster）"><meta name=keywords content="聚类,K-Means,层次聚类,密度聚类,高斯混合模型,GMM,GM,EM,PCA,降维"><title>机器学习-无监督学习</title><link rel=canonical href=http://localhost:1313/p/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/><link rel=stylesheet href=/scss/style.min.85f75f3209ee69adac3c94c489274ecc9d07b319e20cd3c86717f7734ee67869.css><meta property='og:title' content="机器学习-无监督学习"><meta property='og:description' content="【3】聚类是一种无监督的机器学习任务，它可以自动将数据划分成类（Cluster）"><meta property='og:url' content='http://localhost:1313/p/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/'><meta property='og:site_name' content="Leuco's Blog"><meta property='og:type' content='article'><meta property='article:section' content='Post'><meta property='article:tag' content='聚类'><meta property='article:tag' content='K-Means'><meta property='article:tag' content='层次聚类'><meta property='article:tag' content='密度聚类'><meta property='article:tag' content='高斯混合模型'><meta property='article:tag' content='GMM'><meta property='article:tag' content='GM'><meta property='article:tag' content='EM'><meta property='article:tag' content='PCA'><meta property='article:tag' content='降维'><meta property='article:published_time' content='2025-12-02T19:45:17+08:00'><meta property='article:modified_time' content='2025-12-02T19:45:17+08:00'><meta property='og:image' content='http://localhost:1313/p/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/cover.png'><meta name=twitter:title content="机器学习-无监督学习"><meta name=twitter:description content="【3】聚类是一种无监督的机器学习任务，它可以自动将数据划分成类（Cluster）"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content='http://localhost:1313/p/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/cover.png'></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label=切换菜单>
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/><img src=/img/avatar_hu_8cf2f27a1af30ef.png width=300 height=300 class=site-logo loading=lazy alt=Avatar></a></figure><div class=site-meta><h1 class=site-name><a href=/>Leuco's Blog</a></h1><h2 class=site-description></h2></div></header><ol class=menu-social><li><a href=https://github.com/leuco-yuu/ target=_blank title=GitHub rel=me><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2.5" stroke-linecap="round" stroke-linejoin="round"><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37.0 00-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44.0 0020 4.77 5.07 5.07.0 0019.91 1S18.73.65 16 2.48a13.38 13.38.0 00-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07.0 005 4.77 5.44 5.44.0 003.5 8.55c0 5.42 3.3 6.61 6.44 7A3.37 3.37.0 009 18.13V22"/></svg></a></li><li><a href='https://mail.google.com/mail/u/0/?tf=cm&amp;to=leucoyuu@gmail.com&amp;fs=1' target=_blank title=Gmail rel=me><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><rect x="2" y="4" width="20" height="16" rx="2"/><polyline points="3,5 12,14 21,5"/></svg></a></li><li><a href='https://wpa.qq.com/msgrd?v=3&amp;uin=1286938728&amp;site=qq&amp;menu=yes/' target=_blank title=QQ rel=me><svg height="32" viewBox="0 0 32 32" width="32"><g fill="none" fill-rule="evenodd"><path d="m15.9998867.0-.3175962.00373031C8.84822.16478688 5.08430177 5.52571032 5.05900841 13.1032973L5.06 13.376l-.86256398 2.1053645c-.34339269.8459887-.59479858 1.4918602-.8236549 2.1249911-.16246446.4494579-.31092035.8862129-.4515632 1.3302912-1.41043336 4.4529342-1.46997142 7.7313526 1.14979123 8.0424546l.19586827.0182568c.69853306.0462522 1.18775172-.1357889 1.69345756-.5695745L6.008 26.385l.10703497.1859114.16295747.2641667L6.368 26.972l-.10203891.0725127c-1.18457469.8599516-1.78515849 2.0422997-.8749963 3.5478041.72014893 1.1919865 1.68579016 1.3851809 4.67654251 1.4059241h1.2781117l.7210113-.0101194c1.4578857-.0287327 2.9764342-.1065736 3.9343697-.2001215l.2555297.0243167c1.1291243.09698 2.8606159.1735357 4.3993902.1859242h1.2781118c2.9907546-.0207471 3.95642-.2139816 4.6768437-1.4064227l.0983227-.1723424c.764361-1.4241985.1648309-2.548721-.9747412-3.3755348L25.631 26.971l.0912293-.1362605.1629318-.2641511L25.992 26.385l.0466679.0427821c.5516504.4732059 1.0836451.6468525 1.8890001.5513616 2.6200145-.3116105 2.5604594-3.5895097 1.1502083-8.0424822-.136678-.4315245-.2810058-.8567817-.4385987-1.2939849l-.1781698-.4846461c-.0923296-.2464665-.1903348-.5013359-.2976536-.7744519l-.3954267-.9868956L26.939 13.376l9399e-7-.279262c-.038761-7.60576089-3.8169423-12.93053326-10.6187985-13.09291824zm1023e-7 2c6.339972 32813e-8 9.0947051 5.25726658 8.9334981 11.7557738l.8516442 2.0748544c.5606846 1.3708665.9939584 2.4718584 1.3860782 3.7098724 1.2134003 3.8314013.8203049 5.4169539.5209683 5.4525554-.6424867.0761796-2.5006035-2.8842697-2.5006035-2.8842697.0 1.7141775-.8994592 3.9510027-2.8457046 5.5664147l.3377045.1073023c1.0249215.341207 2.6653848 1.0302704 2.2154641 1.7750936-.4078113.675007-6.9962271.4309917-8.8982689.2207732l-.4753503.0452169c-2.33636.195172-8.04423656.3608021-8.42291849-.2659901-.50418989-.8339832 1.61195836-1.5976385 2.55205373-1.8820678-1.94652419-1.615412-2.84615056-3.852456-2.84615056-5.5667428l-.32111727.4924567c-.56063792.838483-1.70355568 2.4482423-2.17943051 2.391813-.29933663-.0355468-.69254342-1.6211541.52102408-5.4525554l.21887662-.6654718c.52220361-1.5311231 1.13363576-2.9515889 2.01884579-5.1193097C6.8963095 7.36281332 9.58861109 2.00032813 15.999989 2z" fill="currentColor" fill-rule="nonzero"/></g></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/><svg class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg>
<span>主页</span></a></li><li><a href=/%E5%8D%9A%E4%B8%BB/><svg class="icon icon-tabler icon-tabler-user" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="7" r="4"/><path d="M6 21v-2a4 4 0 014-4h4a4 4 0 014 4v2"/></svg>
<span>博主</span></a></li><li><a href=/archives/><svg class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg>
<span>博文</span></a></li><li><a href=/search/><svg class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg>
<span>搜索</span></a></li><li><a href=/%E9%93%BE%E6%8E%A5/><svg class="icon icon-tabler icon-tabler-link" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M10 14a3.5 3.5.0 005 0l4-4a3.5 3.5.0 00-5-5l-.5.5"/><path d="M14 10a3.5 3.5.0 00-5 0l-4 4a3.5 3.5.0 005 5l.5-.5"/></svg>
<span>链接</span></a></li><li class=menu-bottom-section><ol class=menu><li id=dark-mode-toggle><svg class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<svg class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<span>暗色模式</span></li></ol></li></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">目录</h2><div class=widget--toc><nav id=TableOfContents><ul><li><a href=#k均值聚类k-means-clustering>K均值聚类（K-Means Clustering）</a></li><li><a href=#层次聚类hierarchical-clustering>层次聚类（Hierarchical Clustering）</a></li><li><a href=#密度聚类>密度聚类</a></li><li><a href=#高斯混合模型gmm模型>高斯混合模型（GMM模型）</a><ul><li><a href=#单个高斯分布gm>单个高斯分布（GM）</a></li><li><a href=#混合高斯模型gmm>混合高斯模型（GMM）</a></li><li><a href=#em算法>EM算法</a></li><li><a href=#gmm流程>GMM流程</a></li></ul></li><li><a href=#降维>降维</a><ul><li><a href=#降维方法>降维方法：</a></li><li><a href=#pca降维>PCA降维</a></li></ul></li></ul></nav></div></section></aside><main class="main full-width"><article class="has-image main-article"><header class=article-header><div class=article-image><a href=/p/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/><img src=/p/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/cover_hu_47f235506752e75f.png srcset="/p/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/cover_hu_47f235506752e75f.png 800w, /p/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/cover_hu_702e3bec99624746.png 1600w" width=800 height=240 loading=lazy alt="Featured image of post 机器学习-无监督学习"></a></div><div class=article-details><header class=article-category><a href=/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/ style=background-color:#2a9d8f;color:#fff>机器学习
</a><a href=/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/ style=background-color:#2a9d8f;color:#fff>学习笔记</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/p/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/>机器学习-无监督学习</a></h2><h3 class=article-subtitle>【3】聚类是一种无监督的机器学习任务，它可以自动将数据划分成类（Cluster）</h3></div><footer class=article-time><div><svg class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg>
<time class=article-time--published datetime=2025-12-02T19:45:17+08:00>Dec 02, 2025</time></div></footer></div></header><section class=article-content><div class=custom-toc><h3>目录</h3><nav id=TableOfContents><ul><li><a href=#k均值聚类k-means-clustering>K均值聚类（K-Means Clustering）</a></li><li><a href=#层次聚类hierarchical-clustering>层次聚类（Hierarchical Clustering）</a></li><li><a href=#密度聚类>密度聚类</a></li><li><a href=#高斯混合模型gmm模型>高斯混合模型（GMM模型）</a><ul><li><a href=#单个高斯分布gm>单个高斯分布（GM）</a></li><li><a href=#混合高斯模型gmm>混合高斯模型（GMM）</a></li><li><a href=#em算法>EM算法</a></li><li><a href=#gmm流程>GMM流程</a></li></ul></li><li><a href=#降维>降维</a><ul><li><a href=#降维方法>降维方法：</a></li><li><a href=#pca降维>PCA降维</a></li></ul></li></ul></nav></div><h1 id=无监督学习unsupervised-learning>无监督学习（Unsupervised Learning）</h1><ul><li>聚类是一种无监督的机器学习任务，它可以自动将数据划分成类（Cluster）</li></ul><h2 id=k均值聚类k-means-clustering>K均值聚类（K-Means Clustering）</h2><ul><li>对于没有标签的数据样本 $X$ ，根据 $X$ 的相似度划分为 $k$ 类</li><li>K-Means一般过程：<ul><li>初始化：指定中心数（簇数） $k$ 以及 $k$ 个簇中心（ $ \mu_1,\mu_2\dots \mu_k $ ，经典K-Means中随机选择中心）</li><li>迭代：<ul><li>分配步：对每个样本 $$ x_i,i \in \{ 1,2\dots m \},$$ 计算 $ x_i $与 $ \mu_1,\mu_2\dots \mu_k $ 的距离，并将 $ x_i $ 分配给簇中心距离其最近的簇：$$\displaystyle{x_i \rightarrow Cluster_{ \underset{j}{argmin}||x_i-\mu_j||^2}}$$</li><li>更新步：更新簇中心：$$\displaystyle{\mu_j=\frac{1}{|N_j|}\sum_{x_i\in Cluster_j}x_i}$$</li></ul></li><li>迭代至：<ul><li>簇中心不变或变化幅度小于阈值</li><li>样本点的归属不再发生变化</li><li>迭代次数达到最大</li></ul></li></ul></li><li>K-Means损失函数MSE：每个点到中心点的距离 $$\displaystyle{J(\mu_1,\mu_2 \dots \mu_k) = \frac{1}{2} \sum^k_{j=1}\sum^{N_j}_{i=1}(x_i-\mu_j)^2}$$</li><li>$k$ 的选取——肘部法<ul><li>依次选取 $k\in{1,2\dots m}$，计算整体的损失</li><li>最终选取 $k_m$ 使得收益： $$Gain=|Loss(k_m-1)-Loss(k_m)|-|Loss(k_m)-Loss(k_m+1)|$$ 取得最大值。即$$\displaystyle{k_m = \underset{k}{\operatorname{argmax}} |Loss(k-1)-Loss(k)|-|Loss(k)-Loss(k+1)|}$$</li></ul></li><li>相似度——以距离函数衡量<ul><li>闵可夫斯基距离：$$\displaystyle{d_p(x,y)=\sqrt[p]{\sum_{i=1}^n|x_n-y_n|^p}}$$</li><li>曼哈顿距离(p=1)：$$\displaystyle{d_1(x,y)=\sum_{i=1}^n|x_n-y_n|}$$</li><li>欧式距离(p=2)：$$\displaystyle{d_2(x,y)=\sqrt{(x-y)^T(x-y)}}$$</li><li>切比雪夫距离(p=$\infty$)：$$\displaystyle{d_\infty(x,y)=\max\limits_{i\in\{1,2\dots n\}}|x_i-y_i|}$$</li><li>余弦距离：$$\displaystyle{d_{\cos}(x,y)=\frac{x^Ty}{||x||·||y||}} \in [-1,1]$$</li><li>Jaccard相似系数：$$\displaystyle{J(A,B) = \frac{|A\cap B|}{|A \cup B|} = \frac{|A\cap B|}{|A|+|B|-|A\cap B|}}$$</li></ul></li><li>K-Means变种<ul><li>K-Mediods：计算新的簇中心的时候不再选择均值，而选择中位数。抗噪能力得到加强</li><li>二分K-Means：合并簇中心点比较近，MSE很小的簇；切分簇中心点比较远，MSE比较大的簇。重新进行K-Means聚类</li><li>K-Means++：使初始化簇中心稍微远一点。随机选择第一个中心点，计算MSE，将MSE转化为概率进行概率化选择初始簇中心点</li></ul></li><li>Canopy聚类： 一次迭代，给出k的值以及k个初始中心点（然后进行K-Means算法）</li></ul><h2 id=层次聚类hierarchical-clustering>层次聚类（Hierarchical Clustering）</h2><ul><li>分裂法<ul><li>思想：将所有样本归为一个簇。每次迭代在一个簇中找到距离最远的两个样本点，将该簇划分为两个子簇。依次类推直到划分为k个簇</li></ul></li><li>凝聚法<ul><li>思想：将所有样本点看作一个独立的簇。每次迭代找到距离最小的两个簇进行合并。</li></ul></li></ul><h2 id=密度聚类>密度聚类</h2><ul><li>DB-SCAN聚类（Density_Based Spatial Clustering of Applications with Noise）<ul><li>与层次聚类不同，它将簇定义为密度相连的点的最大集合，能够把具有高密度的区域划分为簇，并可有效地对抗噪声</li><li>密度相连<ul><li>直接密度可达（若对象q的e邻域内至少有m个对象，m指定。则q为核心对象）<ul><li>若给定一个对象集合D，如果p在q的e邻域内，而q是一个核心对象，则p从q出发是直接密度可达的</li><li>若q直接密度可达r，r直接密度可达p，则q直接密度可达p</li></ul></li><li>密度可达：如果存在一个对象链使得相邻两对象直接密度可达，则称对象链首尾密度可达</li><li>密度相连：若o密度可达p，o密度可达q，则p，q密度相连</li></ul></li><li>DB-SCAN通过检查数据集中每个对象的e邻域来寻找聚类<ul><li>如果一个点p是核心对象则以p为中心创建新簇。依据p来反复寻找密度相连的集合（可能合并原有簇），当没有新点时寻找结束</li></ul></li></ul></li><li>密度最大值聚类<ul><li>局部密度：
$$
\displaystyle{\rho_i = \sum_j\chi(d_{ij}-d_c), 其中 \chi(x) = }
\begin{cases}
1 &, x < 0 \\
0 &, otherwise
\end{cases}
$$</li><li>$D_c$ 是一个截断距离，$\rho_i$ 即到对象 $i$ 的距离小于 $D_c$ 的对象的个数，即： $\rho_i$ = 任何一个点以 $D_c$ 为半径的圆内的样本点的数量， $D_c$ 的设定经验是使每个点的邻居数目是所有点的 1% ~ 2%</li></ul></li></ul><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>  1
</span><span class=lnt>  2
</span><span class=lnt>  3
</span><span class=lnt>  4
</span><span class=lnt>  5
</span><span class=lnt>  6
</span><span class=lnt>  7
</span><span class=lnt>  8
</span><span class=lnt>  9
</span><span class=lnt> 10
</span><span class=lnt> 11
</span><span class=lnt> 12
</span><span class=lnt> 13
</span><span class=lnt> 14
</span><span class=lnt> 15
</span><span class=lnt> 16
</span><span class=lnt> 17
</span><span class=lnt> 18
</span><span class=lnt> 19
</span><span class=lnt> 20
</span><span class=lnt> 21
</span><span class=lnt> 22
</span><span class=lnt> 23
</span><span class=lnt> 24
</span><span class=lnt> 25
</span><span class=lnt> 26
</span><span class=lnt> 27
</span><span class=lnt> 28
</span><span class=lnt> 29
</span><span class=lnt> 30
</span><span class=lnt> 31
</span><span class=lnt> 32
</span><span class=lnt> 33
</span><span class=lnt> 34
</span><span class=lnt> 35
</span><span class=lnt> 36
</span><span class=lnt> 37
</span><span class=lnt> 38
</span><span class=lnt> 39
</span><span class=lnt> 40
</span><span class=lnt> 41
</span><span class=lnt> 42
</span><span class=lnt> 43
</span><span class=lnt> 44
</span><span class=lnt> 45
</span><span class=lnt> 46
</span><span class=lnt> 47
</span><span class=lnt> 48
</span><span class=lnt> 49
</span><span class=lnt> 50
</span><span class=lnt> 51
</span><span class=lnt> 52
</span><span class=lnt> 53
</span><span class=lnt> 54
</span><span class=lnt> 55
</span><span class=lnt> 56
</span><span class=lnt> 57
</span><span class=lnt> 58
</span><span class=lnt> 59
</span><span class=lnt> 60
</span><span class=lnt> 61
</span><span class=lnt> 62
</span><span class=lnt> 63
</span><span class=lnt> 64
</span><span class=lnt> 65
</span><span class=lnt> 66
</span><span class=lnt> 67
</span><span class=lnt> 68
</span><span class=lnt> 69
</span><span class=lnt> 70
</span><span class=lnt> 71
</span><span class=lnt> 72
</span><span class=lnt> 73
</span><span class=lnt> 74
</span><span class=lnt> 75
</span><span class=lnt> 76
</span><span class=lnt> 77
</span><span class=lnt> 78
</span><span class=lnt> 79
</span><span class=lnt> 80
</span><span class=lnt> 81
</span><span class=lnt> 82
</span><span class=lnt> 83
</span><span class=lnt> 84
</span><span class=lnt> 85
</span><span class=lnt> 86
</span><span class=lnt> 87
</span><span class=lnt> 88
</span><span class=lnt> 89
</span><span class=lnt> 90
</span><span class=lnt> 91
</span><span class=lnt> 92
</span><span class=lnt> 93
</span><span class=lnt> 94
</span><span class=lnt> 95
</span><span class=lnt> 96
</span><span class=lnt> 97
</span><span class=lnt> 98
</span><span class=lnt> 99
</span><span class=lnt>100
</span><span class=lnt>101
</span><span class=lnt>102
</span><span class=lnt>103
</span><span class=lnt>104
</span><span class=lnt>105
</span><span class=lnt>106
</span><span class=lnt>107
</span><span class=lnt>108
</span><span class=lnt>109
</span><span class=lnt>110
</span><span class=lnt>111
</span><span class=lnt>112
</span><span class=lnt>113
</span><span class=lnt>114
</span><span class=lnt>115
</span><span class=lnt>116
</span><span class=lnt>117
</span><span class=lnt>118
</span><span class=lnt>119
</span><span class=lnt>120
</span><span class=lnt>121
</span><span class=lnt>122
</span><span class=lnt>123
</span><span class=lnt>124
</span><span class=lnt>125
</span><span class=lnt>126
</span><span class=lnt>127
</span><span class=lnt>128
</span><span class=lnt>129
</span><span class=lnt>130
</span><span class=lnt>131
</span><span class=lnt>132
</span><span class=lnt>133
</span><span class=lnt>134
</span><span class=lnt>135
</span><span class=lnt>136
</span><span class=lnt>137
</span><span class=lnt>138
</span><span class=lnt>139
</span><span class=lnt>140
</span><span class=lnt>141
</span><span class=lnt>142
</span><span class=lnt>143
</span><span class=lnt>144
</span><span class=lnt>145
</span><span class=lnt>146
</span><span class=lnt>147
</span><span class=lnt>148
</span><span class=lnt>149
</span><span class=lnt>150
</span><span class=lnt>151
</span><span class=lnt>152
</span><span class=lnt>153
</span><span class=lnt>154
</span><span class=lnt>155
</span><span class=lnt>156
</span><span class=lnt>157
</span><span class=lnt>158
</span><span class=lnt>159
</span><span class=lnt>160
</span><span class=lnt>161
</span><span class=lnt>162
</span><span class=lnt>163
</span><span class=lnt>164
</span><span class=lnt>165
</span><span class=lnt>166
</span><span class=lnt>167
</span><span class=lnt>168
</span><span class=lnt>169
</span><span class=lnt>170
</span><span class=lnt>171
</span><span class=lnt>172
</span><span class=lnt>173
</span><span class=lnt>174
</span><span class=lnt>175
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>time</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>matplotlib.pyplot</span> <span class=k>as</span> <span class=nn>plt</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.datasets</span> <span class=kn>import</span> <span class=n>make_blobs</span> <span class=c1># 生成高斯blob数据</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.cluster</span> <span class=kn>import</span> <span class=n>KMeans</span><span class=p>,</span><span class=n>AgglomerativeClustering</span><span class=p>,</span><span class=n>DBSCAN</span> <span class=c1># KMeans、层次、密度聚类</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.metrics</span> <span class=kn>import</span> <span class=n>adjusted_rand_score</span><span class=p>,</span><span class=n>silhouette_score</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.preprocessing</span> <span class=kn>import</span> <span class=n>StandardScaler</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>titles</span> <span class=o>=</span> <span class=p>[</span><span class=s1>&#39;Ground Truth&#39;</span><span class=p>,</span><span class=s1>&#39;K-Means(random)&#39;</span><span class=p>,</span><span class=s1>&#39;K-Means++&#39;</span><span class=p>,</span><span class=s1>&#39;K-Means(Canopy)&#39;</span><span class=p>,</span><span class=s1>&#39;Hierarchical&#39;</span><span class=p>,</span><span class=s1>&#39;DBSCAN&#39;</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=c1># colors = [</span>
</span></span><span class=line><span class=cl><span class=c1>#     &#39;#e41a1c&#39;, &#39;#377eb8&#39;, &#39;#4daf4a&#39;, &#39;#984ea3&#39;,  # 红 蓝 绿 紫</span>
</span></span><span class=line><span class=cl><span class=c1>#     &#39;#ff7f00&#39;, &#39;#ffff33&#39;,                         # 橙 黄</span>
</span></span><span class=line><span class=cl><span class=c1>#     &#39;#1b9e77&#39;, &#39;#d95f02&#39;, &#39;#7570b3&#39;, &#39;#e7298a&#39;,   # 青 棕 靛 洋红</span>
</span></span><span class=line><span class=cl><span class=c1>#     &#39;#66a61e&#39;, &#39;#666666&#39;                          # 草绿 深灰</span>
</span></span><span class=line><span class=cl><span class=c1># ]</span>
</span></span><span class=line><span class=cl><span class=c1># markers = [</span>
</span></span><span class=line><span class=cl><span class=c1>#     &#39;o&#39;,          # 圆</span>
</span></span><span class=line><span class=cl><span class=c1>#     &#39;s&#39;,          # 方</span>
</span></span><span class=line><span class=cl><span class=c1>#     &#39;^&#39;,          # 上三角</span>
</span></span><span class=line><span class=cl><span class=c1>#     &#39;d&#39;,          # 钻石</span>
</span></span><span class=line><span class=cl><span class=c1>#     &#39;v&#39;,          # 下三角</span>
</span></span><span class=line><span class=cl><span class=c1>#     &#39;p&#39;,          # 五边形</span>
</span></span><span class=line><span class=cl><span class=c1>#     &#39;*&#39;,          # 星号</span>
</span></span><span class=line><span class=cl><span class=c1>#     &#39;h&#39;,          # 六边形1</span>
</span></span><span class=line><span class=cl><span class=c1>#     &#39;H&#39;,          # 六边形2</span>
</span></span><span class=line><span class=cl><span class=c1>#     &#39;8&#39;,          # 八边形</span>
</span></span><span class=line><span class=cl><span class=c1>#     &#39;&gt;&#39;,          # 右三角</span>
</span></span><span class=line><span class=cl><span class=c1>#     &#39;&lt;&#39;           # 左三角</span>
</span></span><span class=line><span class=cl><span class=c1># ]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 数据可视化</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>data_visualization_only</span><span class=p>(</span><span class=n>x</span><span class=p>,</span><span class=n>y</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>plt</span><span class=o>.</span><span class=n>scatter</span><span class=p>(</span><span class=n>x</span><span class=p>[:,</span><span class=mi>0</span><span class=p>],</span><span class=n>x</span><span class=p>[:,</span><span class=mi>1</span><span class=p>],</span><span class=n>s</span><span class=o>=</span><span class=mi>12</span><span class=p>,</span><span class=n>c</span><span class=o>=</span><span class=n>y</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>data_visualization_all</span><span class=p>(</span><span class=n>labels</span><span class=p>,</span><span class=n>times</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>fix</span><span class=p>,</span><span class=n>axes</span> <span class=o>=</span> <span class=n>plt</span><span class=o>.</span><span class=n>subplots</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span><span class=mi>3</span><span class=p>,</span><span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>20</span><span class=p>,</span><span class=mi>12</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    <span class=n>axes</span> <span class=o>=</span> <span class=n>axes</span><span class=o>.</span><span class=n>flatten</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>index</span><span class=p>,(</span><span class=n>ax</span><span class=p>,</span><span class=n>lab</span><span class=p>,</span><span class=n>tit</span><span class=p>,</span><span class=n>t</span><span class=p>)</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=nb>zip</span><span class=p>(</span><span class=n>axes</span><span class=p>,</span><span class=n>labels</span><span class=p>,</span><span class=n>titles</span><span class=p>,</span><span class=n>times</span><span class=p>)):</span>
</span></span><span class=line><span class=cl>        <span class=n>ax</span><span class=o>.</span><span class=n>scatter</span><span class=p>(</span><span class=n>X</span><span class=p>[:,</span><span class=mi>0</span><span class=p>],</span><span class=n>X</span><span class=p>[:,</span><span class=mi>1</span><span class=p>],</span><span class=n>c</span><span class=o>=</span><span class=n>lab</span><span class=p>,</span><span class=n>s</span><span class=o>=</span><span class=mi>12</span><span class=p>,</span><span class=n>cmap</span><span class=o>=</span><span class=s1>&#39;tab10&#39;</span><span class=p>,)</span>
</span></span><span class=line><span class=cl>        <span class=n>ax</span><span class=o>.</span><span class=n>set_title</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;</span><span class=si>{</span><span class=n>tit</span><span class=si>}</span><span class=s2>(</span><span class=si>{</span><span class=n>t</span><span class=si>:</span><span class=s2>.4f</span><span class=si>}</span><span class=s2>s)&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=c1># ax.set_title(tit+&#34;(&#34;++&#34;s)&#34;)</span>
</span></span><span class=line><span class=cl>        <span class=n>ax</span><span class=o>.</span><span class=n>set_xticks</span><span class=p>([])</span>
</span></span><span class=line><span class=cl>        <span class=n>ax</span><span class=o>.</span><span class=n>set_yticks</span><span class=p>([])</span>
</span></span><span class=line><span class=cl>    <span class=n>plt</span><span class=o>.</span><span class=n>tight_layout</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl><span class=c1># 一 构造模拟数据</span>
</span></span><span class=line><span class=cl><span class=n>centers</span> <span class=o>=</span> <span class=p>[(</span><span class=mi>0</span><span class=p>,</span><span class=mi>0</span><span class=p>),(</span><span class=mi>8</span><span class=p>,</span><span class=mi>8</span><span class=p>),(</span><span class=o>-</span><span class=mi>5</span><span class=p>,</span><span class=mi>10</span><span class=p>),(</span><span class=mi>10</span><span class=p>,</span><span class=o>-</span><span class=mi>4</span><span class=p>)]</span> <span class=c1># 样本中心</span>
</span></span><span class=line><span class=cl><span class=n>cluster_std</span> <span class=o>=</span> <span class=p>[</span><span class=mf>1.2</span><span class=p>,</span> <span class=mf>0.8</span><span class=p>,</span> <span class=mf>2.0</span><span class=p>,</span> <span class=mf>0.5</span><span class=p>]</span> <span class=c1># 标准差</span>
</span></span><span class=line><span class=cl><span class=n>n_samples</span> <span class=o>=</span> <span class=p>[</span><span class=mi>800</span><span class=p>,</span><span class=mi>600</span><span class=p>,</span><span class=mi>400</span><span class=p>,</span><span class=mi>200</span><span class=p>]</span> <span class=c1># 样本数量</span>
</span></span><span class=line><span class=cl><span class=n>X</span><span class=p>,</span><span class=n>y_true</span> <span class=o>=</span> <span class=n>make_blobs</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>n_samples</span><span class=o>=</span><span class=n>n_samples</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>centers</span><span class=o>=</span><span class=n>centers</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>cluster_std</span><span class=o>=</span><span class=n>cluster_std</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>random_state</span><span class=o>=</span><span class=mi>42</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=c1># data_visualization_only(X,y_true)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 二 算法实现</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>kmeans_random</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>k</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s1>&#39;&#39;&#39;
</span></span></span><span class=line><span class=cl><span class=s1>    标准K-Means算法，随机选择初始中心
</span></span></span><span class=line><span class=cl><span class=s1>    &#39;&#39;&#39;</span>
</span></span><span class=line><span class=cl>    <span class=n>start</span> <span class=o>=</span> <span class=n>time</span><span class=o>.</span><span class=n>time</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=n>model</span> <span class=o>=</span> <span class=n>KMeans</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=n>n_clusters</span><span class=o>=</span><span class=n>k</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>init</span><span class=o>=</span><span class=s1>&#39;random&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>n_init</span><span class=o>=</span><span class=mi>10</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>random_state</span><span class=o>=</span><span class=mi>42</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>labels</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>fit_predict</span><span class=p>(</span><span class=n>X</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>labels</span><span class=p>,</span><span class=n>time</span><span class=o>.</span><span class=n>time</span><span class=p>()</span><span class=o>-</span><span class=n>start</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>kmeans_pp</span><span class=p>(</span><span class=n>X</span><span class=p>,</span><span class=n>k</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s1>&#39;&#39;&#39;
</span></span></span><span class=line><span class=cl><span class=s1>    K-Means++:智能初始化，远离地选择中心
</span></span></span><span class=line><span class=cl><span class=s1>    &#39;&#39;&#39;</span>
</span></span><span class=line><span class=cl>    <span class=n>start</span><span class=o>=</span><span class=n>time</span><span class=o>.</span><span class=n>time</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=n>model</span> <span class=o>=</span> <span class=n>KMeans</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=n>n_clusters</span><span class=o>=</span><span class=n>k</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>init</span><span class=o>=</span><span class=s1>&#39;k-means++&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>n_init</span><span class=o>=</span><span class=mi>10</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>random_state</span><span class=o>=</span><span class=mi>42</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>labels</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>fit_predict</span><span class=p>(</span><span class=n>X</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>labels</span><span class=p>,</span><span class=n>time</span><span class=o>.</span><span class=n>time</span><span class=p>()</span><span class=o>-</span><span class=n>start</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>kmeans_canopy</span><span class=p>(</span><span class=n>X</span><span class=p>,</span><span class=n>t1</span><span class=o>=</span><span class=mf>4.0</span><span class=p>,</span><span class=n>t2</span><span class=o>=</span><span class=mf>2.0</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s1>&#39;&#39;&#39;
</span></span></span><span class=line><span class=cl><span class=s1>    K-Means+Canopy。智能选择k和初始化中心
</span></span></span><span class=line><span class=cl><span class=s1>    &#39;&#39;&#39;</span>
</span></span><span class=line><span class=cl>    <span class=n>start</span> <span class=o>=</span> <span class=n>time</span><span class=o>.</span><span class=n>time</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=n>canopies</span><span class=o>=</span><span class=p>[]</span>
</span></span><span class=line><span class=cl>    <span class=n>X_copy</span> <span class=o>=</span> <span class=n>X</span><span class=o>.</span><span class=n>copy</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>shuffle</span><span class=p>(</span><span class=n>X_copy</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>pt</span> <span class=ow>in</span> <span class=n>X_copy</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=nb>any</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>linalg</span><span class=o>.</span><span class=n>norm</span><span class=p>(</span><span class=n>pt</span><span class=o>-</span><span class=n>c</span><span class=p>)</span> <span class=o>&lt;</span> <span class=n>t2</span> <span class=k>for</span> <span class=n>c</span> <span class=ow>in</span> <span class=n>canopies</span><span class=p>):</span>
</span></span><span class=line><span class=cl>            <span class=k>continue</span>
</span></span><span class=line><span class=cl>        <span class=n>canopies</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>pt</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>k</span> <span class=o>=</span> <span class=nb>len</span><span class=p>(</span><span class=n>canopies</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>model</span> <span class=o>=</span> <span class=n>KMeans</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=n>n_clusters</span><span class=o>=</span><span class=n>k</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>init</span><span class=o>=</span><span class=n>np</span><span class=o>.</span><span class=n>vstack</span><span class=p>(</span><span class=n>canopies</span><span class=p>),</span>
</span></span><span class=line><span class=cl>        <span class=n>n_init</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>random_state</span><span class=o>=</span><span class=mi>42</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>labels</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>fit_predict</span><span class=p>(</span><span class=n>X</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>labels</span><span class=p>,</span><span class=n>time</span><span class=o>.</span><span class=n>time</span><span class=p>()</span><span class=o>-</span><span class=n>start</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>hierarchical</span><span class=p>(</span><span class=n>X</span><span class=p>,</span><span class=n>k</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s1>&#39;&#39;&#39;
</span></span></span><span class=line><span class=cl><span class=s1>    凝聚层次聚类，使用Ward方差最小化准则
</span></span></span><span class=line><span class=cl><span class=s1>    &#39;&#39;&#39;</span>
</span></span><span class=line><span class=cl>    <span class=n>start</span> <span class=o>=</span> <span class=n>time</span><span class=o>.</span><span class=n>time</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=n>model</span> <span class=o>=</span> <span class=n>AgglomerativeClustering</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=n>n_clusters</span><span class=o>=</span><span class=n>k</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>linkage</span><span class=o>=</span><span class=s1>&#39;ward&#39;</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>labels</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>fit_predict</span><span class=p>(</span><span class=n>X</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>labels</span><span class=p>,</span><span class=n>time</span><span class=o>.</span><span class=n>time</span><span class=p>()</span><span class=o>-</span><span class=n>start</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>dbscan</span><span class=p>(</span><span class=n>X</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s1>&#39;&#39;&#39;
</span></span></span><span class=line><span class=cl><span class=s1>    密度聚类DBSCAN
</span></span></span><span class=line><span class=cl><span class=s1>    &#39;&#39;&#39;</span>
</span></span><span class=line><span class=cl>    <span class=n>start</span> <span class=o>=</span> <span class=n>time</span><span class=o>.</span><span class=n>time</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=n>X_stand</span> <span class=o>=</span> <span class=n>StandardScaler</span><span class=p>()</span><span class=o>.</span><span class=n>fit_transform</span><span class=p>(</span><span class=n>X</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>model</span> <span class=o>=</span> <span class=n>DBSCAN</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=n>eps</span> <span class=o>=</span> <span class=mf>0.35</span><span class=p>,</span> <span class=c1># 指定e邻域</span>
</span></span><span class=line><span class=cl>        <span class=n>min_samples</span><span class=o>=</span><span class=mi>5</span> <span class=c1># 指定m</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>labels</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>fit_predict</span><span class=p>(</span><span class=n>X_stand</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>labels</span><span class=p>,</span><span class=n>time</span><span class=o>.</span><span class=n>time</span><span class=p>()</span><span class=o>-</span><span class=n>start</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 三 数据训练</span>
</span></span><span class=line><span class=cl><span class=n>k_true</span> <span class=o>=</span> <span class=nb>len</span><span class=p>(</span><span class=nb>set</span><span class=p>(</span><span class=n>y_true</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>labels_random</span> <span class=p>,</span><span class=n>t_random</span> <span class=o>=</span> <span class=n>kmeans_random</span><span class=p>(</span><span class=n>X</span><span class=p>,</span><span class=n>k_true</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>labels_pp</span> <span class=p>,</span><span class=n>t_pp</span> <span class=o>=</span> <span class=n>kmeans_pp</span><span class=p>(</span><span class=n>X</span><span class=p>,</span><span class=n>k_true</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>labels_canopy</span> <span class=p>,</span><span class=n>t_canopy</span> <span class=o>=</span> <span class=n>kmeans_canopy</span><span class=p>(</span><span class=n>X</span><span class=p>,</span><span class=mi>30</span><span class=p>,</span><span class=mi>15</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>labels_hier</span> <span class=p>,</span><span class=n>t_hier</span> <span class=o>=</span> <span class=n>hierarchical</span><span class=p>(</span><span class=n>X</span><span class=p>,</span><span class=n>k_true</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>labels_dbscan</span> <span class=p>,</span><span class=n>t_dbscan</span> <span class=o>=</span> <span class=n>dbscan</span><span class=p>(</span><span class=n>X</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>labels</span> <span class=o>=</span> <span class=p>[</span> 
</span></span><span class=line><span class=cl>    <span class=n>y_true</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>labels_random</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>labels_pp</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>labels_canopy</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>labels_hier</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>labels_dbscan</span>
</span></span><span class=line><span class=cl><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>times</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=mi>0</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>t_random</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>t_pp</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>t_canopy</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>t_hier</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>t_dbscan</span>
</span></span><span class=line><span class=cl><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 四 结果评估</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>evaluate</span><span class=p>(</span><span class=n>name</span><span class=p>,</span><span class=n>labels</span><span class=p>,</span><span class=n>time</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>ari</span> <span class=o>=</span> <span class=n>adjusted_rand_score</span><span class=p>(</span><span class=n>y_true</span><span class=p>,</span><span class=n>labels</span><span class=p>)</span> <span class=c1># 任意一对样本，看“预测是否同簇”与“真实是否同类”一致的比例</span>
</span></span><span class=line><span class=cl>    <span class=c1># 轮廓系数：a为样本与同簇其他点的平均距离；b是样本与最近邻簇所有点的平均距离</span>
</span></span><span class=line><span class=cl>    <span class=c1># 轮廓系数 si_i = (b-a)/max(a,b)。轮廓系数约接近1，簇间距离越远，簇内距离约近，聚类越理想</span>
</span></span><span class=line><span class=cl>    <span class=n>sil</span> <span class=o>=</span> <span class=n>silhouette_score</span><span class=p>(</span><span class=n>X</span><span class=p>,</span><span class=n>labels</span><span class=p>)</span> <span class=k>if</span> <span class=nb>len</span><span class=p>(</span><span class=nb>set</span><span class=p>(</span><span class=n>labels</span><span class=p>))</span><span class=o>&gt;</span><span class=mi>1</span> <span class=k>else</span> <span class=n>np</span><span class=o>.</span><span class=n>nan</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;</span><span class=si>{</span><span class=n>name</span><span class=si>:</span><span class=s2>15s</span><span class=si>}</span><span class=s2> | ARI=</span><span class=si>{</span><span class=n>ari</span><span class=si>:</span><span class=s2>5.3f</span><span class=si>}</span><span class=s2> | Sil=</span><span class=si>{</span><span class=n>sil</span><span class=si>:</span><span class=s2>5.3f</span><span class=si>}</span><span class=s2> | Time=</span><span class=si>{</span><span class=n>time</span><span class=si>:</span><span class=s2>.3f</span><span class=si>}</span><span class=s2>s&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>index</span><span class=p>,(</span><span class=n>label</span><span class=p>,</span><span class=n>time</span><span class=p>)</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=nb>zip</span><span class=p>(</span><span class=n>labels</span><span class=p>,</span><span class=n>times</span><span class=p>)):</span>
</span></span><span class=line><span class=cl>    <span class=n>evaluate</span><span class=p>(</span><span class=n>titles</span><span class=p>[</span><span class=n>index</span><span class=p>],</span><span class=n>label</span><span class=p>,</span><span class=n>time</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 五 可视化</span>
</span></span><span class=line><span class=cl><span class=n>data_visualization_all</span><span class=p>(</span><span class=n>labels</span><span class=p>,</span><span class=n>times</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><pre><code>Ground Truth    | ARI=1.000 | Sil=0.801 | Time=0.000s
K-Means(random) | ARI=0.999 | Sil=0.801 | Time=0.012s
K-Means++       | ARI=0.999 | Sil=0.801 | Time=0.014s
K-Means(Canopy) | ARI=0.823 | Sil=0.687 | Time=0.013s
Hierarchical    | ARI=1.000 | Sil=0.801 | Time=0.068s
DBSCAN          | ARI=0.999 | Sil=0.769 | Time=0.025s
</code></pre><p><img src=/p/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/output_4_1.png width=1990 height=1190 srcset="/p/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/output_4_1_hu_fc97088d0676b5b4.png 480w, /p/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/output_4_1_hu_f42eba1c20978d64.png 1024w" loading=lazy alt=png class=gallery-image data-flex-grow=167 data-flex-basis=401px></p><h2 id=高斯混合模型gmm模型>高斯混合模型（GMM模型）</h2><h3 id=单个高斯分布gm>单个高斯分布（GM）</h3><ul><li>高斯分布：$N(\mu,\sigma)$</li><li>通过MLE思想估计 $\mu$ 和 $\sigma$ :<ul><li>$$\displaystyle{\hat \mu=\frac{1}{n}\sum_{i \in \{1,2 \dots n\}} x_i=\bar x}$$</li><li>$$\displaystyle{\hat \sigma^2=\frac{1}{n}\sum_{i \in \{1,2 \dots n\}}(x_i-\hat\mu)^2$$</li></ul></li></ul><h3 id=混合高斯模型gmm>混合高斯模型（GMM）</h3><ul><li>GMM：假设随机变量 $X$ 是由 $k$ 个高斯分布混合而来，取到各个高斯分布的概率为 $\pi_1,\pi_2 \dots \pi_k$ ，第 $i$ 个高斯分布的均值为 $\mu_i$ ，标准差为 $\sigma_i$ 。当前观测到一系列样本 $X_1,X_2\dots X_n$ ，估计参数向量： $\pi，\mu，\sigma$<ul><li>$$\displaystyle{L_{\pi,\mu,\sigma}(X) = \sum^N_{i=1}\log P(X_i)\overset{\text{全概率公式}}{=}\sum^N_{i=1}\log(\sum^K_{k=1}\pi_kN(x_i|\mu_k,\sigma_k))}$$</li><li>EM算法求解估计 $\pi，\mu，\sigma$</li></ul></li></ul><h3 id=em算法>EM算法</h3><ul><li>Jensen不等式：若f是凸函数，x是随机变量。有： $$f(\mathcal{E}x) \le \mathcal{E}f(x),\displaystyle{\sum_i \mathcal{E}_i=1}$$</li><li>E-Step（求责任度）与M-Step（更新参数）</li></ul><h3 id=gmm流程>GMM流程</h3><ul><li>变量介绍：<ul><li>$ N $ 为数据集中样本总数量</li><li>$ N_k $ 为第 $ k $ 个高斯分布的有效数据点数量</li><li>$ K $ 为聚类数（即预设的高斯分布数量）</li><li>$ X $ 为整个数据集（ $ X_i $ 为第 $ i $ 个数据向量）</li><li>$ \pi_k $ 为第 $ k $ 个高斯分布的混合系数/权重</li><li>$ \mu_k $ 为第 $ k $ 个高斯分布的均值向量</li><li>$ \sigma_k $ 为第 $ k $ 个高斯分布的协方差</li><li>$ \gamma(i,k) $ 为责任度或响应度（即E步计算核心）</li></ul></li><li>第零步：随机找 $m$ 个数据（中心点）<ul><li>确定初始 $\pi,\mu,\sigma$ ，例如随机等</li></ul></li><li>第一步：估计数据来源于哪个分布(E步——责任度)<ul><li>代入：$$\displaystyle{\gamma(i,k) = P(X \in Z_k | X=X_i) \overset{\text{贝叶斯公式}}{=} \frac{\pi_kN(X_i|\mu_k,\sigma_k)}{\displaystyle{\sum^K_{j=1}\pi_jN(X_i|\mu_j,\sigma_j)}}}$$</li><li>根据 $\gamma$ 划分类别</li></ul></li><li>第二步：更新参数（$\pi，\mu，\sigma$）（M步——参数更新）<ul><li>$$\displaystyle{N_k=\sum^N_{i=1}\gamma(i,k)}$$</li><li>$$\displaystyle{\mu_k = \frac{1}{N_k}\displaystyle{\sum_{i}^N\gamma(i,k)X_i}}$$</li><li>$$\displaystyle{\sigma_k = \frac{1}{N_k}\sum^N_{i=1}\gamma(i,k)(x_i-\mu_k)(x_i-\mu_k)^T}$$</li><li>$$\displaystyle{\pi_k = \frac{N_k}{N}=\frac{1}{N}\sum_{i=1}^N\gamma(i,k)}$$</li></ul></li><li>第三步：迭代，直到 $\pi,\mu,\sigma$ 不再变化</li></ul><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span><span class=lnt>38
</span><span class=lnt>39
</span><span class=lnt>40
</span><span class=lnt>41
</span><span class=lnt>42
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>time</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>matplotlib.pyplot</span> <span class=k>as</span> <span class=nn>plt</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.datasets</span> <span class=kn>import</span> <span class=n>make_blobs</span> <span class=c1># 生成高斯blob数据</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.mixture</span> <span class=kn>import</span> <span class=n>GaussianMixture</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.metrics</span> <span class=kn>import</span> <span class=n>adjusted_rand_score</span><span class=p>,</span><span class=n>adjusted_mutual_info_score</span><span class=p>,</span><span class=n>silhouette_score</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.preprocessing</span> <span class=kn>import</span> <span class=n>StandardScaler</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 数据可视化</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>data_visualization_only</span><span class=p>(</span><span class=n>x</span><span class=p>,</span><span class=n>y</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>plt</span><span class=o>.</span><span class=n>scatter</span><span class=p>(</span><span class=n>x</span><span class=p>[:,</span><span class=mi>0</span><span class=p>],</span><span class=n>x</span><span class=p>[:,</span><span class=mi>1</span><span class=p>],</span><span class=n>s</span><span class=o>=</span><span class=mi>12</span><span class=p>,</span><span class=n>c</span><span class=o>=</span><span class=n>y</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl><span class=c1># 一 构造模拟数据</span>
</span></span><span class=line><span class=cl><span class=n>centers</span> <span class=o>=</span> <span class=p>[(</span><span class=mi>0</span><span class=p>,</span><span class=mi>0</span><span class=p>),(</span><span class=mi>8</span><span class=p>,</span><span class=mi>8</span><span class=p>),(</span><span class=o>-</span><span class=mi>5</span><span class=p>,</span><span class=mi>10</span><span class=p>),(</span><span class=mi>10</span><span class=p>,</span><span class=o>-</span><span class=mi>4</span><span class=p>)]</span> <span class=c1># 样本中心</span>
</span></span><span class=line><span class=cl><span class=n>cluster_std</span> <span class=o>=</span> <span class=p>[</span><span class=mf>1.2</span><span class=p>,</span> <span class=mf>0.8</span><span class=p>,</span> <span class=mf>2.0</span><span class=p>,</span> <span class=mf>0.5</span><span class=p>]</span> <span class=c1># 标准差</span>
</span></span><span class=line><span class=cl><span class=n>n_samples</span> <span class=o>=</span> <span class=p>[</span><span class=mi>800</span><span class=p>,</span><span class=mi>600</span><span class=p>,</span><span class=mi>400</span><span class=p>,</span><span class=mi>200</span><span class=p>]</span> <span class=c1># 样本数量</span>
</span></span><span class=line><span class=cl><span class=n>X</span><span class=p>,</span><span class=n>y_true</span> <span class=o>=</span> <span class=n>make_blobs</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>n_samples</span><span class=o>=</span><span class=n>n_samples</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>centers</span><span class=o>=</span><span class=n>centers</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>cluster_std</span><span class=o>=</span><span class=n>cluster_std</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>random_state</span><span class=o>=</span><span class=mi>42</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 二 用BIC在2~8个成分里面选最佳GMM(看看几个候选成分时解释力最好)</span>
</span></span><span class=line><span class=cl><span class=n>n_componenrs_range</span> <span class=o>=</span> <span class=nb>range</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span><span class=mi>9</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>gmms</span><span class=o>=</span><span class=p>[</span><span class=n>GaussianMixture</span><span class=p>(</span><span class=n>n</span><span class=p>,</span><span class=n>random_state</span><span class=o>=</span><span class=mi>42</span><span class=p>)</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X</span><span class=p>)</span> <span class=k>for</span> <span class=n>n</span> <span class=ow>in</span> <span class=n>n_componenrs_range</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>bics</span> <span class=o>=</span> <span class=p>[</span><span class=n>g</span><span class=o>.</span><span class=n>bic</span><span class=p>(</span><span class=n>X</span><span class=p>)</span> <span class=k>for</span> <span class=n>g</span> <span class=ow>in</span> <span class=n>gmms</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>best_gmm</span> <span class=o>=</span> <span class=n>gmms</span><span class=p>[</span><span class=n>np</span><span class=o>.</span><span class=n>argmin</span><span class=p>(</span><span class=n>bics</span><span class=p>)]</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>gmms</span><span class=o>.</span><span class=n>index</span><span class=p>(</span><span class=n>best_gmm</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 三 聚类与评估</span>
</span></span><span class=line><span class=cl><span class=n>y_pred</span> <span class=o>=</span> <span class=n>best_gmm</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>X</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>ari</span> <span class=o>=</span> <span class=n>adjusted_rand_score</span><span class=p>(</span><span class=n>y_true</span><span class=p>,</span><span class=n>y_pred</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>ami</span> <span class=o>=</span> <span class=n>adjusted_mutual_info_score</span><span class=p>(</span><span class=n>y_true</span><span class=p>,</span><span class=n>y_pred</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>sil</span> <span class=o>=</span> <span class=n>silhouette_score</span><span class=p>(</span><span class=n>X</span><span class=p>,</span><span class=n>y_pred</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;ARI: </span><span class=si>{</span><span class=n>ari</span><span class=si>:</span><span class=s2>.3f</span><span class=si>}</span><span class=s2>   AMI: </span><span class=si>{</span><span class=n>ami</span><span class=si>:</span><span class=s2>.3f</span><span class=si>}</span><span class=s2>   Silhouette: </span><span class=si>{</span><span class=n>sil</span><span class=si>:</span><span class=s2>.3f</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># data_visualization_only(X,y_true)</span>
</span></span><span class=line><span class=cl><span class=n>data_visualization_only</span><span class=p>(</span><span class=n>X</span><span class=p>,</span><span class=n>y_pred</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><pre><code>2
ARI: 1.000   AMI: 1.000   Silhouette: 0.801
</code></pre><p><img src=/p/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/output_6_1.png width=546 height=413 srcset="/p/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/output_6_1_hu_1d6fd7060530322b.png 480w, /p/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/output_6_1_hu_ed735af62706903.png 1024w" loading=lazy alt=png class=gallery-image data-flex-grow=132 data-flex-basis=317px></p><h2 id=降维>降维</h2><h3 id=降维方法>降维方法：</h3><ul><li>特征提取：特征映射。把高维空间的数据映射到低维空间。比如PCA和基于神经网络的降维等</li><li>特征选择：<ul><li>过滤式（打分机制）：通过某个阈值进行过滤。比如根据方差、信息增益、互信息过滤经常会看到但可能不会去用的信息<ul><li>独立于任何机器学习算法。它基于数据本身的统计特征（如相关性、互信息）对特征进行评分和筛选，就像用一个“过滤器”把不好的特征过滤掉。</li><li>优点：快，计算开销小，不依赖具体模型。</li><li>缺点：可能忽略特征与模型的协同作用，选出的特征单独看很强，但组合起来对特定模型可能不是最优的。</li><li>互信息：$$\displaystyle{I(X;Y)=\sum_{x\in X}\sum_{y \in Y}P(X=x,Y=y)\log_2\frac{P(X=x,Y=y)}{P(X=x)P(Y=y)}}$$</li></ul></li><li>包裹式：每次迭代产生一个特征子集，评分<ul><li>将模型性能作为评价标准。它会尝试不同的特征子集，并用一个特定的机器学习模型去评估每个子集的性能（比如准确率）。</li><li>优点：针对性强，选出的特征子集对该模型通常性能最优。</li><li>缺点：非常慢，计算开销大，容易过拟合。</li></ul></li><li>嵌入式：先通过机器学习模型训练来对每个特征提到一个权值。接下来和过滤式类似，通过设定某个阈值来筛选特征<ul><li>特征选择的过程嵌入在模型训练过程之中。模型在训练的同时会自动进行特征选择。</li><li>优点：平衡了效率和效果，比过滤式更针对模型，比包裹式快很多。</li><li>缺点：依赖于具有内置特征选择机制的模型。</li></ul></li></ul></li></ul><h3 id=pca降维>PCA降维</h3><ul><li>算法过程（对于样本矩阵 $X_{m×n}$，从 $n$ 维到 $k$ 维）<ul><li>1、数据标准化（中心化）：计算每个特征的均值 $ X_{mean} $ ，另 $ X_c=X-X_{mean} $</li><li>2、计算协方差矩阵：$$\displaystyle{\Sigma = \frac{1}{m-1}X_c^TX_c}$$</li><li>3、特征值分解（正交对角化）：
$$
\begin{aligned}
&\Sigma=\chi\Lambda \chi^T =
\begin{bmatrix}
\xi_1,\xi_2 \dots \xi_n
\end{bmatrix}
\begin{bmatrix}
\lambda_1 \\
& \lambda_2 \\
&& \ddots \\
&&& \lambda_n
\end{bmatrix}
\begin{bmatrix}
\xi_1,\xi_2 \dots \xi_n
\end{bmatrix}^{T}\\
& \lambda_1 \ge \lambda_2 \ge \dots \ge \lambda_n
\end{aligned}
$$</li><li>4、选择主成分（$\lambda_1,\lambda_2\dots \lambda_k,k&lt;n$）<ul><li>保留信息量（方差贡献量）：$$\displaystyle{\eta_{k/n} = \frac{\displaystyle{\sum_{i=1}^k\lambda_i}}{\displaystyle{\sum_{i=1}^n\lambda_i}}}$$</li><li>截取投影矩阵：
$$
\chi_k =
\begin{bmatrix}
\xi_1,\xi_2 \dots \xi_k
\end{bmatrix}
$$</li></ul></li><li>5、投影（降维）： $Z=X_c·\chi_k$</li></ul></li><li>Kernel PCA<ul><li>KernelPCA首先将 $X_{m×n}$ 升维映射到 $K_{n×n}$</li><li>$K_{i,j} = F_k(X_i,X_j)$ ， $F_k$ 为核函数</li></ul></li><li>特征值特征向量的求解：SVD分解<ul><li>直接对 $X_c$ 进行SVD分解：$$X_c = U·\Sigma·V^T$$<ul><li>$ U $ 是一个 $ m×m $ 的正交矩阵，左奇异向量</li><li>$ \Sigma $ 是一个 $ m×n $ 的对角矩阵，对角线上的值 $ \sigma_i $ 是奇异值。 $$\displaystyle{\lambda_i=\frac{\sigma_i^2}{m-1}}$$</li><li>$ V $ 是一 $ n×n $ 的正交矩阵，右奇异向量。 $$V = \chi $$</li></ul></li></ul></li></ul></section><footer class=article-footer><section class=article-tags><a href=/tags/%E8%81%9A%E7%B1%BB/>聚类</a>
<a href=/tags/k-means/>K-Means</a>
<a href=/tags/%E5%B1%82%E6%AC%A1%E8%81%9A%E7%B1%BB/>层次聚类</a>
<a href=/tags/%E5%AF%86%E5%BA%A6%E8%81%9A%E7%B1%BB/>密度聚类</a>
<a href=/tags/%E9%AB%98%E6%96%AF%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B/>高斯混合模型</a>
<a href=/tags/gmm/>GMM</a>
<a href=/tags/gm/>GM</a>
<a href=/tags/em/>EM</a>
<a href=/tags/pca/>PCA</a>
<a href=/tags/%E9%99%8D%E7%BB%B4/>降维</a></section></footer><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><script src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous defer></script><script>window.addEventListener("DOMContentLoaded",()=>{const e=[".main-article",".widget--toc"];e.forEach(e=>{const t=document.querySelector(e);t&&renderMathInElement(t,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],ignoredClasses:["gist"]})})})</script></article><aside class=related-content--wrapper><h2 class=section-title>相关文章</h2><div class=related-content><div class="flex article-list--tile"><article class=has-image><a href=/p/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/><div class=article-image><img src=/p/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/60aa0004-c259-49eb-b742-6515691c6d92.a2bc150b77d997c7aa74a554d4c80408_hu_271026cfc48cbafd.png width=250 height=150 loading=lazy alt="Featured image of post 机器学习-神经网络" data-hash="md5-orwVC3fZl8eqdKVU1MgECA=="></div><div class=article-details><h2 class=article-title>机器学习-神经网络</h2></div></a></article><article class=has-image><a href=/p/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/><div class=article-image><img src=/p/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/output_3_1.97aa252439207abddc3143a10ae1f4f6_hu_d028bd9bd3a40326.png width=250 height=150 loading=lazy alt="Featured image of post 机器学习-集成学习" data-hash="md5-l6olJDkger3cMUOhCuH09g=="></div><div class=article-details><h2 class=article-title>机器学习-集成学习</h2></div></a></article><article class=has-image><a href=/p/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB/><div class=article-image><img src=/p/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB/SVM.b96e9ba18f9f55522ce3f1ae5994872a_hu_3db6860e7015ff5e.jpg width=250 height=150 loading=lazy alt="Featured image of post 机器学习-线性分类" data-hash="md5-uW6boY+fVVIs4/GuWZSHKg=="></div><div class=article-details><h2 class=article-title>机器学习-线性分类</h2></div></a></article><article class=has-image><a href=/p/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/><div class=article-image><img src=/p/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/output_45_1.ac66b82ba1a91437f422db6d948e641f_hu_2e3482a2dbd8838a.jpg width=250 height=150 loading=lazy alt="Featured image of post 机器学习-线性回归" data-hash="md5-rGa4K6GpFDf0ItttlI5kHw=="></div><div class=article-details><h2 class=article-title>机器学习-线性回归</h2></div></a></article><article><a href=/p/%E6%B8%97%E9%80%8F%E6%B5%8B%E8%AF%95%E5%9F%BA%E7%A1%80/><div class=article-details><h2 class=article-title>渗透测试基础</h2></div></a></article></div></div></aside><script src=https://utteranc.es/client.js repo=leuco-yuu/blog-comments issue-term=pathname crossorigin=anonymous async></script><style>.utterances{max-width:unset}</style><script>let utterancesLoaded=!1;function setUtterancesTheme(e){let t=document.querySelector(".utterances iframe");t&&t.contentWindow.postMessage({type:"set-theme",theme:`github-${e}`},"https://utteranc.es")}addEventListener("message",e=>{if(e.origin!=="https://utteranc.es")return;utterancesLoaded=!0,setUtterancesTheme(document.documentElement.dataset.scheme)}),window.addEventListener("onColorSchemeChange",e=>{if(!utterancesLoaded)return;setUtterancesTheme(e.detail)})</script><footer class=site-footer><section class=copyright>&copy;
2025 -
2026 Leuco(leucoyuu@163.com)</section><section class=powerby>Embrace Bit Art<br>使用 <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a> 构建<br>主题 <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.32.0>Stack</a></b> 由 <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a> 设计</section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/ts/main.29c987cab5419174d4dc85c2a56544a08027cc7f77efb5749447a3e80f85a03d.js defer></script><script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>